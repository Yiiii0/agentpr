{"type":"thread.started","thread_id":"019c9294-f472-74b1-9cd5-3b5289520b67"}
{"type":"turn.started"}
{"type":"item.completed","item":{"id":"item_0","type":"reasoning","text":"**Clarifying skill file access conflict**"}}
{"type":"item.completed","item":{"id":"item_1","type":"agent_message","text":"I‚Äôll run the preflight discovery contract now: first I‚Äôll scan governance files (`AGENTS.md`, `CONTRIBUTING*`, PR templates), then CI workflows, then map the minimal Forge integration touchpoints without editing code."}}
{"type":"item.completed","item":{"id":"item_2","type":"reasoning","text":"**Listing repository root contents**"}}
{"type":"item.started","item":{"id":"item_3","type":"command_execution","command":"/bin/zsh -lc 'pwd && ls -la'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_4","type":"command_execution","command":"/bin/zsh -lc 'rg --files | head -n 200'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_3","type":"command_execution","command":"/bin/zsh -lc 'pwd && ls -la'","aggregated_output":"/Users/yi/Documents/Career/TensorBlcok/agentpr/workspaces/mem0\ntotal 1488\ndrwxr-xr-x@ 28 yi  staff     896 Feb 24 21:16 .\ndrwxr-xr-x@  7 yi  staff     224 Feb 24 20:50 ..\ndrwxr-xr-x@ 13 yi  staff     416 Feb 24 21:16 .agentpr_runtime\ndrwxr-xr-x@ 15 yi  staff     480 Feb 24 21:16 .git\ndrwxr-xr-x@  5 yi  staff     160 Feb 24 17:22 .github\n-rw-r--r--@  1 yi  staff    3338 Feb 24 17:22 .gitignore\n-rw-r--r--@  1 yi  staff     314 Feb 24 17:22 .pre-commit-config.yaml\ndrwxr-xr-x@  6 yi  staff     192 Feb 24 17:25 .pytest_cache\n-rw-r--r--@  1 yi  staff    1958 Feb 24 17:22 CONTRIBUTING.md\n-rw-r--r--@  1 yi  staff   11349 Feb 24 17:22 LICENSE\n-rw-r--r--@  1 yi  staff   37202 Feb 24 17:22 LLM.md\n-rw-r--r--@  1 yi  staff    5057 Feb 24 17:22 MIGRATION_GUIDE_v1.0.md\n-rw-r--r--@  1 yi  staff    1031 Feb 24 17:22 Makefile\n-rw-r--r--@  1 yi  staff    6813 Feb 24 17:22 README.md\ndrwxr-xr-x@  5 yi  staff     160 Feb 24 17:22 cookbooks\ndrwxr-xr-x@ 25 yi  staff     800 Feb 24 20:50 docs\ndrwxr-xr-x@ 16 yi  staff     512 Feb 24 17:22 embedchain\ndrwxr-xr-x@ 10 yi  staff     320 Feb 24 17:22 evaluation\ndrwxr-xr-x@ 10 yi  staff     320 Feb 24 17:22 examples\ndrwxr-xr-x@ 14 yi  staff     448 Feb 24 17:22 mem0\ndrwxr-xr-x@ 11 yi  staff     352 Feb 24 17:22 mem0-ts\ndrwxr-xr-x@  7 yi  staff     224 Feb 24 17:22 openclaw\ndrwxr-xr-x@ 12 yi  staff     384 Feb 24 17:22 openmemory\n-rw-r--r--@  1 yi  staff  671442 Feb 24 17:22 poetry.lock\n-rw-r--r--@  1 yi  staff    3019 Feb 24 17:22 pyproject.toml\ndrwxr-xr-x@ 10 yi  staff     320 Feb 24 17:22 server\ndrwxr-xr-x@ 13 yi  staff     416 Feb 24 17:22 tests\ndrwxr-xr-x@ 15 yi  staff     480 Feb 24 17:22 vercel-ai-sdk\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_4","type":"command_execution","command":"/bin/zsh -lc 'rg --files | head -n 200'","aggregated_output":"MIGRATION_GUIDE_v1.0.md\ncookbooks/customer-support-chatbot.ipynb\ncookbooks/helper/__init__.py\ncookbooks/helper/mem0_teachability.py\ncookbooks/mem0-autogen.ipynb\nCONTRIBUTING.md\nLLM.md\npoetry.lock\nMakefile\nREADME.md\nLICENSE\nopenmemory/docker-compose.yml\nmem0/client/project.py\nmem0/client/main.py\nmem0/client/utils.py\nmem0/client/__init__.py\nevaluation/src/openai/predict.py\ndocs/favicon.svg\nexamples/openai-inbuilt-tools/package.json\nexamples/openai-inbuilt-tools/index.js\ntests/test_main.py\ntests/test_telemetry.py\nopenclaw/index.ts\nopenclaw/package.json\nopenclaw/README.md\nopenclaw/openclaw.plugin.json\nevaluation/src/memzero/search.py\nevaluation/src/memzero/add.py\nevaluation/src/rag.py\nevaluation/src/utils.py\nevaluation/src/langmem.py\nopenmemory/ui/store/appsSlice.ts\nopenmemory/ui/store/store.ts\nopenmemory/ui/store/memoriesSlice.ts\nopenmemory/ui/store/configSlice.ts\nopenmemory/ui/store/uiSlice.ts\nopenmemory/ui/store/profileSlice.ts\nopenmemory/ui/store/filtersSlice.ts\nopenmemory/ui/next.config.dev.mjs\nopenmemory/ui/entrypoint.sh\nopenmemory/ui/.env.example\nopenmemory/ui/tsconfig.json\nopenmemory/ui/components.json\nevaluation/src/zep/search.py\nevaluation/src/zep/add.py\nevaluation/run_experiments.py\nevaluation/generate_scores.py\nevaluation/README.md\nevaluation/prompts.py\nevaluation/evals.py\nevaluation/Makefile\ntests/llms/test_gemini.py\ntests/llms/test_deepseek.py\ntests/llms/test_groq.py\ntests/llms/test_lm_studio.py\ntests/llms/test_litellm.py\ntests/llms/test_together.py\ntests/llms/test_langchain.py\ntests/llms/test_ollama.py\ntests/llms/test_azure_openai.py\ntests/llms/test_azure_openai_structured.py\ntests/llms/test_vllm.py\ntests/llms/test_openai.py\ntests/test_proxy.py\ntests/test_memory.py\nopenmemory/api/main.py\nopenmemory/api/.env.example\nvercel-ai-sdk/src/mem0-utils.ts\nvercel-ai-sdk/src/mem0-generic-language-model.ts\nvercel-ai-sdk/src/stream-utils.ts\nvercel-ai-sdk/src/index.ts\nvercel-ai-sdk/src/mem0-provider.ts\nvercel-ai-sdk/src/mem0-provider-selector.ts\nvercel-ai-sdk/src/provider-response-provider.ts\nvercel-ai-sdk/src/mem0-facade.ts\nvercel-ai-sdk/src/mem0-types.ts\nvercel-ai-sdk/teardown.ts\nvercel-ai-sdk/tsconfig.json\nvercel-ai-sdk/tsup.config.ts\nvercel-ai-sdk/package.json\nvercel-ai-sdk/README.md\nopenmemory/api/alembic/README\nmem0/llms/base.py\nmem0/llms/ollama.py\nmem0/llms/openai_structured.py\nmem0/llms/langchain.py\nmem0/llms/together.py\nmem0/llms/xai.py\nmem0/llms/anthropic.py\nmem0/llms/sarvam.py\nmem0/llms/deepseek.py\nmem0/llms/azure_openai.py\nmem0/llms/openai.py\nmem0/llms/groq.py\nmem0/llms/litellm.py\nmem0/llms/aws_bedrock.py\nmem0/llms/gemini.py\nmem0/llms/__init__.py\nmem0/llms/azure_openai_structured.py\nmem0/llms/lmstudio.py\nmem0/llms/vllm.py\nmem0/llms/configs.py\ndocs/cookbooks/essentials/memory-expiration-short-and-long-term.mdx\ndocs/cookbooks/essentials/building-ai-companion.mdx\ndocs/cookbooks/essentials/exporting-memories.mdx\ndocs/cookbooks/essentials/tagging-and-organizing-memories.mdx\ndocs/cookbooks/essentials/choosing-memory-architecture-vector-vs-graph.mdx\ndocs/cookbooks/essentials/entity-partitioning-playbook.mdx\ndocs/cookbooks/essentials/controlling-memory-ingestion.mdx\nembedchain/notebooks/together.ipynb\nembedchain/notebooks/hugging_face_hub.ipynb\nembedchain/notebooks/embedchain-chromadb-server.ipynb\nembedchain/notebooks/anthropic.ipynb\nembedchain/notebooks/openai.ipynb\nembedchain/notebooks/elasticsearch.ipynb\nembedchain/notebooks/pinecone.ipynb\nembedchain/notebooks/ollama.ipynb\nembedchain/notebooks/aws-bedrock.ipynb\nembedchain/notebooks/vertex_ai.ipynb\nembedchain/notebooks/lancedb.ipynb\nembedchain/notebooks/azure-openai.ipynb\nembedchain/notebooks/openai_azure.yaml\nembedchain/notebooks/chromadb.ipynb\nembedchain/notebooks/azure_openai.yaml\nembedchain/notebooks/gpt4all.ipynb\nembedchain/notebooks/clarifai.ipynb\nembedchain/notebooks/llama2.ipynb\nembedchain/notebooks/cohere.ipynb\nembedchain/notebooks/opensearch.ipynb\nembedchain/notebooks/jina.ipynb\nembedchain/notebooks/embedchain-docs-site-example.ipynb\nembedchain/poetry.lock\nembedchain/poetry.toml\nopenmemory/backup-scripts/export_openmemory.sh\nopenmemory/ui/lib/utils.ts\nopenmemory/ui/lib/helpers.ts\nvercel-ai-sdk/tests/memory-core.test.ts\nvercel-ai-sdk/tests/mem0-toolcalls.test.ts\nvercel-ai-sdk/tests/generate-output.test.ts\nvercel-ai-sdk/tests/text-properties.test.ts\ntests/configs/test_prompts.py\ntests/__init__.py\ntests/test_memory_integration.py\nevaluation/metrics/llm_judge.py\nevaluation/metrics/utils.py\nopenmemory/ui/next.config.mjs\nopenmemory/api/alembic/versions/0b53c747049a_initial_migration.py\nopenmemory/compose/weaviate.yml\nopenmemory/api/alembic/versions/add_config_table.py\nopenmemory/api/alembic/versions/afd00efbd06b_add_unique_user_id_constraints.py\nopenmemory/compose/elasticsearch.yml\nopenmemory/compose/qdrant.yml\nopenmemory/compose/opensearch.yml\nopenmemory/compose/milvus.yml\nopenmemory/compose/pgvector.yml\nopenmemory/compose/faiss.yml\nopenmemory/compose/redis.yml\nopenmemory/compose/chroma.yml\nopenmemory/Makefile\nopenmemory/api/alembic/env.py\nopenmemory/api/alembic/script.py.mako\nopenmemory/api/README.md\nopenmemory/api/default_config.json\nopenmemory/api/config.json\nopenmemory/api/Dockerfile\nopenmemory/api/alembic.ini\nopenmemory/api/requirements.txt\nopenmemory/README.md\nopenmemory/run.sh\nopenmemory/CONTRIBUTING.md\ndocs/introduction.mdx\ndocs/cookbooks/frameworks/multimodal-retrieval.mdx\ndocs/cookbooks/frameworks/llamaindex-react.mdx\ndocs/cookbooks/frameworks/eliza-os-character.mdx\ndocs/cookbooks/frameworks/gemini-3-with-mem0-mcp.mdx\ndocs/cookbooks/frameworks/chrome-extension.mdx\ndocs/cookbooks/frameworks/llamaindex-multiagent.mdx\nmem0/graphs/utils.py\nmem0/graphs/__init__.py\nvercel-ai-sdk/tests/utils-test/cohere-integration.test.ts\nvercel-ai-sdk/tests/utils-test/google-integration.test.ts\nvercel-ai-sdk/tests/utils-test/groq-integration.test.ts\nvercel-ai-sdk/tests/utils-test/anthropic-integration.test.ts\nvercel-ai-sdk/tests/utils-test/openai-integration.test.ts\nembedchain/configs/chunker.yaml\nembedchain/configs/vllm.yaml\nembedchain/configs/gpt4.yaml\nembedchain/configs/google.yaml\nembedchain/configs/pipeline.yaml\nembedchain/configs/opensource.yaml\nembedchain/configs/cohere.yaml\nembedchain/configs/weaviate.yaml\nembedchain/configs/opensearch.yaml\nembedchain/configs/chroma.yaml\nembedchain/configs/gpt4all.yaml\nembedchain/configs/huggingface.yaml\nembedchain/configs/full-stack.yaml\nembedchain/configs/azure_openai.yaml\nembedchain/configs/ollama.yaml\nembedchain/configs/llama2.yaml\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_5","type":"reasoning","text":"**Assessing access restrictions for instructions**"}}
{"type":"item.completed","item":{"id":"item_6","type":"agent_message","text":"I‚Äôve confirmed repo layout and will now extract governance and CI truth sources (`CONTRIBUTING`, PR templates, workflows) so the contract is evidence-backed."}}
{"type":"item.started","item":{"id":"item_7","type":"command_execution","command":"/bin/zsh -lc 'ls -la .github && find .github -maxdepth 3 -type f | sort'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_8","type":"command_execution","command":"/bin/zsh -lc \"rg --files -g 'AGENTS.md' -g 'CONTRIBUTING*' -g 'CODEOWNERS' -g '.github/**'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_7","type":"command_execution","command":"/bin/zsh -lc 'ls -la .github && find .github -maxdepth 3 -type f | sort'","aggregated_output":"total 8\ndrwxr-xr-x@  5 yi  staff   160 Feb 24 17:22 .\ndrwxr-xr-x@ 28 yi  staff   896 Feb 24 21:16 ..\ndrwxr-xr-x@  6 yi  staff   192 Feb 24 17:22 ISSUE_TEMPLATE\n-rw-r--r--@  1 yi  staff  1619 Feb 24 17:22 PULL_REQUEST_TEMPLATE.md\ndrwxr-xr-x@  4 yi  staff   128 Feb 24 17:22 workflows\n.github/ISSUE_TEMPLATE/bug_report.yml\n.github/ISSUE_TEMPLATE/config.yml\n.github/ISSUE_TEMPLATE/documentation_issue.yml\n.github/ISSUE_TEMPLATE/feature_request.yml\n.github/PULL_REQUEST_TEMPLATE.md\n.github/workflows/cd.yml\n.github/workflows/ci.yml\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_8","type":"command_execution","command":"/bin/zsh -lc \"rg --files -g 'AGENTS.md' -g 'CONTRIBUTING*' -g 'CODEOWNERS' -g '.github/**'\"","aggregated_output":"CONTRIBUTING.md\nopenmemory/CONTRIBUTING.md\nembedchain/CONTRIBUTING.md\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_9","type":"reasoning","text":"**Planning bulk CONTRIBUTING read**"}}
{"type":"item.started","item":{"id":"item_10","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' .github/workflows/cd.yml\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_11","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' .github/PULL_REQUEST_TEMPLATE.md\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_12","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' CONTRIBUTING.md\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_13","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' .github/workflows/ci.yml\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_11","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' .github/PULL_REQUEST_TEMPLATE.md\"","aggregated_output":"## Description\n\nPlease include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.\n\nFixes # (issue)\n\n## Type of change\n\nPlease delete options that are not relevant.\n\n- [ ] Bug fix (non-breaking change which fixes an issue)\n- [ ] New feature (non-breaking change which adds functionality)\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\n- [ ] Refactor (does not change functionality, e.g. code style improvements, linting)\n- [ ] Documentation update\n\n## How Has This Been Tested?\n\nPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\n\nPlease delete options that are not relevant.\n\n- [ ] Unit Test\n- [ ] Test Script (please provide)\n\n## Checklist:\n\n- [ ] My code follows the style guidelines of this project\n- [ ] I have performed a self-review of my own code\n- [ ] I have commented my code, particularly in hard-to-understand areas\n- [ ] I have made corresponding changes to the documentation\n- [ ] My changes generate no new warnings\n- [ ] I have added tests that prove my fix is effective or that my feature works\n- [ ] New and existing unit tests pass locally with my changes\n- [ ] Any dependent changes have been merged and published in downstream modules\n- [ ] I have checked my code and corrected any misspellings\n\n## Maintainer Checklist\n\n- [ ] closes #xxxx (Replace xxxx with the GitHub issue number)\n- [ ] Made sure Checks passed\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_13","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' .github/workflows/ci.yml\"","aggregated_output":"name: ci\n\non:\n  push:\n    branches: [main]\n    paths:\n      - 'mem0/**'\n      - 'tests/**'\n      - 'embedchain/**'\n      - '.github/workflows/**'\n      - 'pyproject.toml'\n  pull_request:\n    paths:\n      - 'mem0/**'\n      - 'tests/**'\n      - 'embedchain/**'\n\njobs:\n  check_changes:\n    runs-on: ubuntu-latest\n    outputs:\n      mem0_changed: ${{ steps.filter.outputs.mem0 }}\n      embedchain_changed: ${{ steps.filter.outputs.embedchain }}\n    steps:\n    - uses: actions/checkout@v3\n    - uses: dorny/paths-filter@v2\n      id: filter\n      with:\n        filters: |\n          mem0:\n            - 'mem0/**'\n            - 'tests/**'\n            - '.github/workflows/**'\n            - 'pyproject.toml'\n          embedchain:\n            - 'embedchain/**'\n\n  build_mem0:\n    needs: check_changes\n    if: needs.check_changes.outputs.mem0_changed == 'true'\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [\"3.10\", \"3.11\", \"3.12\"]\n    steps:\n      - uses: actions/checkout@v3\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Clean up disk space\n        run: |\n          df -h\n          sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /opt/hostedtoolcache/CodeQL\n          sudo docker image prune --all --force\n          sudo docker builder prune -a\n          df -h\n      - name: Install Hatch\n        run: pip install hatch\n      - name: Load cached venv\n        id: cached-hatch-dependencies\n        uses: actions/cache@v3\n        with:\n          path: .venv\n          key: venv-mem0-${{ runner.os }}-${{ hashFiles('**/pyproject.toml') }}\n      - name: Install GEOS Libraries\n        run: sudo apt-get update && sudo apt-get install -y libgeos-dev\n      - name: Install dependencies\n        run: |\n          pip install --upgrade pip\n          pip install -e \".[test,graph,vector_stores,llms,extras]\"\n          pip install ruff\n        if: steps.cached-hatch-dependencies.outputs.cache-hit != 'true'\n      - name: Run Linting\n        run: make lint\n      - name: Run tests and generate coverage report\n        run: make test\n\n  build_embedchain:\n    needs: check_changes\n    if: needs.check_changes.outputs.embedchain_changed == 'true'\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [\"3.9\", \"3.10\", \"3.11\", \"3.12\"]\n    steps:\n      - uses: actions/checkout@v3\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Install Hatch\n        run: pip install hatch\n      - name: Load cached venv\n        id: cached-hatch-dependencies\n        uses: actions/cache@v3\n        with:\n          path: .venv\n          key: venv-embedchain-${{ runner.os }}-${{ hashFiles('**/pyproject.toml') }}\n      - name: Install dependencies\n        run: cd embedchain && make install_all\n        if: steps.cached-hatch-dependencies.outputs.cache-hit != 'true'\n      - name: Run Formatting\n        run: |\n          mkdir -p embedchain/.ruff_cache && chmod -R 777 embedchain/.ruff_cache\n          cd embedchain && hatch run format\n      - name: Lint with ruff\n        run: cd embedchain && make lint\n      - name: Run tests and generate coverage report\n        run: cd embedchain && make coverage\n      - name: Upload coverage reports to Codecov\n        uses: codecov/codecov-action@v3\n        with:\n          file: coverage.xml\n        env:\n          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_10","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' .github/workflows/cd.yml\"","aggregated_output":"name: Publish Python üêç distributions üì¶ to PyPI and TestPyPI\n\non:\n  release:\n    types: [published]\n\njobs:\n  build-n-publish:\n    name: Build and publish Python üêç distributions üì¶ to PyPI and TestPyPI\n    runs-on: ubuntu-latest\n    permissions:\n      id-token: write\n    steps:\n      - uses: actions/checkout@v2\n\n      - name: Set up Python\n        uses: actions/setup-python@v2\n        with:\n          python-version: '3.11'\n\n      - name: Install Hatch\n        run: |\n          pip install hatch\n\n      - name: Install dependencies\n        run: |\n          hatch env create\n\n      - name: Build a binary wheel and a source tarball\n        run: |\n          hatch build --clean\n\n      # TODO: Needs to setup mem0 repo on Test PyPI\n      # - name: Publish distribution üì¶ to Test PyPI\n      #   uses: pypa/gh-action-pypi-publish@release/v1\n      #   with:\n      #     repository_url: https://test.pypi.org/legacy/\n      #     packages_dir: dist/\n\n      - name: Publish distribution üì¶ to PyPI\n        if: startsWith(github.ref, 'refs/tags')\n        uses: pypa/gh-action-pypi-publish@release/v1\n        with:\n          packages_dir: dist/\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_12","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' CONTRIBUTING.md\"","aggregated_output":"# Contributing to mem0\n\nLet us make contribution easy, collaborative and fun.\n\n## Submit your Contribution through PR\n\nTo make a contribution, follow these steps:\n\n1. Fork and clone this repository\n2. Do the changes on your fork with dedicated feature branch `feature/f1`\n3. If you modified the code (new feature or bug-fix), please add tests for it\n4. Include proper documentation / docstring and examples to run the feature\n5. Ensure that all tests pass\n6. Submit a pull request\n\nFor more details about pull requests, please read [GitHub's guides](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request).\n\n\n### üì¶ Development Environment\n\nWe use `hatch` for managing development environments. To set up:\n\n```bash\n# Activate environment for specific Python version:\nhatch shell dev_py_3_9   # Python 3.9\nhatch shell dev_py_3_10  # Python 3.10  \nhatch shell dev_py_3_11  # Python 3.11\nhatch shell dev_py_3_12  # Python 3.12\n\n# The environment will automatically install all dev dependencies\n# Run tests within the activated shell:\nmake test\n```\n\n### üìå Pre-commit\n\nTo ensure our standards, make sure to install pre-commit before starting to contribute.\n\n```bash\npre-commit install\n```\n\n### üß™ Testing\n\nWe use `pytest` to test our code across multiple Python versions. You can run tests using:\n\n```bash\n# Run tests with default Python version\nmake test\n\n# Test specific Python versions:\nmake test-py-3.9   # Python 3.9 environment\nmake test-py-3.10  # Python 3.10 environment\nmake test-py-3.11  # Python 3.11 environment\nmake test-py-3.12  # Python 3.12 environment\n\n# When using hatch shells, run tests with:\nmake test  # After activating a shell with hatch shell test_XX\n```\n\nMake sure that all tests pass across all supported Python versions before submitting a pull request.\n\nWe look forward to your pull requests and can't wait to see your contributions!\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_14","type":"reasoning","text":"**Assessing repo integration surfaces**"}}
{"type":"item.completed","item":{"id":"item_15","type":"agent_message","text":"Governance and CI are captured. Next I‚Äôm mapping the actual integration surface in `mem0/llms` (provider selection, config normalization, and OpenAI path) to produce a concrete <=4-file edit plan."}}
{"type":"item.started","item":{"id":"item_16","type":"command_execution","command":"/bin/zsh -lc \"ls -la mem0/llms && sed -n '1,260p' mem0/llms/configs.py && sed -n '1,260p' mem0/llms/__init__.py\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_17","type":"command_execution","command":"/bin/zsh -lc \"rg -n \\\"class .*Config|provider|llm|openai|azure_openai|litellm|factory|registry|SUPPORTED|model\\\" mem0/llms mem0 -g '*.py' | head -n 300\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_18","type":"command_execution","command":"/bin/zsh -lc 'git branch -a && git remote -v && (git symbolic-ref refs/remotes/origin/HEAD 2>/dev/null || true)'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_16","type":"command_execution","command":"/bin/zsh -lc \"ls -la mem0/llms && sed -n '1,260p' mem0/llms/configs.py && sed -n '1,260p' mem0/llms/__init__.py\"","aggregated_output":"total 248\ndrwxr-xr-x@ 22 yi  staff    704 Feb 24 20:50 .\ndrwxr-xr-x@ 14 yi  staff    448 Feb 24 17:22 ..\n-rw-r--r--@  1 yi  staff      0 Feb 24 17:22 __init__.py\n-rw-r--r--@  1 yi  staff   3204 Feb 24 17:22 anthropic.py\n-rw-r--r--@  1 yi  staff  27314 Feb 24 17:22 aws_bedrock.py\n-rw-r--r--@  1 yi  staff   5268 Feb 24 17:22 azure_openai.py\n-rw-r--r--@  1 yi  staff   3361 Feb 24 17:22 azure_openai_structured.py\n-rw-r--r--@  1 yi  staff   4510 Feb 24 17:22 base.py\n-rw-r--r--@  1 yi  staff    997 Feb 24 17:22 configs.py\n-rw-r--r--@  1 yi  staff   3838 Feb 24 17:22 deepseek.py\n-rw-r--r--@  1 yi  staff   7272 Feb 24 17:22 gemini.py\n-rw-r--r--@  1 yi  staff   2964 Feb 24 17:22 groq.py\n-rw-r--r--@  1 yi  staff   3176 Feb 24 17:22 langchain.py\n-rw-r--r--@  1 yi  staff   3084 Feb 24 17:22 litellm.py\n-rw-r--r--@  1 yi  staff   4131 Feb 24 17:22 lmstudio.py\n-rw-r--r--@  1 yi  staff   4277 Feb 24 17:22 ollama.py\n-rw-r--r--@  1 yi  staff   5555 Feb 24 20:50 openai.py\n-rw-r--r--@  1 yi  staff   1695 Feb 24 17:22 openai_structured.py\n-rw-r--r--@  1 yi  staff   3381 Feb 24 17:22 sarvam.py\n-rw-r--r--@  1 yi  staff   3091 Feb 24 17:22 together.py\n-rw-r--r--@  1 yi  staff   3802 Feb 24 17:22 vllm.py\n-rw-r--r--@  1 yi  staff   1764 Feb 24 17:22 xai.py\nfrom typing import Optional\n\nfrom pydantic import BaseModel, Field, field_validator\n\n\nclass LlmConfig(BaseModel):\n    provider: str = Field(description=\"Provider of the LLM (e.g., 'ollama', 'openai')\", default=\"openai\")\n    config: Optional[dict] = Field(description=\"Configuration for the specific LLM\", default={})\n\n    @field_validator(\"config\")\n    def validate_config(cls, v, values):\n        provider = values.data.get(\"provider\")\n        if provider in (\n            \"openai\",\n            \"ollama\",\n            \"anthropic\",\n            \"groq\",\n            \"together\",\n            \"aws_bedrock\",\n            \"litellm\",\n            \"azure_openai\",\n            \"openai_structured\",\n            \"azure_openai_structured\",\n            \"gemini\",\n            \"deepseek\",\n            \"xai\",\n            \"sarvam\",\n            \"lmstudio\",\n            \"vllm\",\n            \"langchain\",\n        ):\n            return v\n        else:\n            raise ValueError(f\"Unsupported LLM provider: {provider}\")\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_17","type":"command_execution","command":"/bin/zsh -lc \"rg -n \\\"class .*Config|provider|llm|openai|azure_openai|litellm|factory|registry|SUPPORTED|model\\\" mem0/llms mem0 -g '*.py' | head -n 300\"","aggregated_output":"mem0/llms/base.py:4:from mem0.configs.llms.base import BaseLlmConfig\nmem0/llms/base.py:9:    Base class for all LLM providers.\nmem0/llms/base.py:10:    Handles common functionality and delegates provider-specific logic to subclasses.\nmem0/llms/base.py:33:        Override in subclasses to add provider-specific validation.\nmem0/llms/base.py:35:        if not hasattr(self.config, \"model\"):\nmem0/llms/base.py:36:            raise ValueError(\"Configuration must have a 'model' attribute\")\nmem0/llms/base.py:40:            # This will be handled by individual providers\nmem0/llms/base.py:43:    def _is_reasoning_model(self, model: str) -> bool:\nmem0/llms/base.py:45:        Check if the model is a reasoning model or GPT-5 series that doesn't support certain parameters.\nmem0/llms/base.py:48:            model: The model name to check\nmem0/llms/base.py:51:            bool: True if the model is a reasoning model or GPT-5 series\nmem0/llms/base.py:53:        reasoning_models = {\nmem0/llms/base.py:58:        if model.lower() in reasoning_models:\nmem0/llms/base.py:61:        model_lower = model.lower()\nmem0/llms/base.py:62:        if any(reasoning_model in model_lower for reasoning_model in [\"gpt-5\", \"o1\", \"o3\"]):\nmem0/llms/base.py:69:        Get parameters that are supported by the current model.\nmem0/llms/base.py:70:        Filters out unsupported parameters for reasoning models and GPT-5 series.\nmem0/llms/base.py:78:        model = getattr(self.config, 'model', '')\nmem0/llms/base.py:80:        if self._is_reasoning_model(model):\nmem0/llms/base.py:94:            # For regular models, include all common parameters\nmem0/llms/base.py:106:            tools (list, optional): List of tools that the model can call. Defaults to None.\nmem0/llms/base.py:108:            **kwargs: Additional provider-specific parameters.\nmem0/llms/base.py:117:        Get common parameters that most providers use.\nmem0/llms/base.py:128:        # Add provider-specific parameters from kwargs\nmem0/llms/base.py:4:from mem0.configs.llms.base import BaseLlmConfig\nmem0/llms/base.py:9:    Base class for all LLM providers.\nmem0/llms/base.py:10:    Handles common functionality and delegates provider-specific logic to subclasses.\nmem0/llms/base.py:33:        Override in subclasses to add provider-specific validation.\nmem0/llms/base.py:35:        if not hasattr(self.config, \"model\"):\nmem0/llms/base.py:36:            raise ValueError(\"Configuration must have a 'model' attribute\")\nmem0/llms/base.py:40:            # This will be handled by individual providers\nmem0/llms/base.py:43:    def _is_reasoning_model(self, model: str) -> bool:\nmem0/llms/base.py:45:        Check if the model is a reasoning model or GPT-5 series that doesn't support certain parameters.\nmem0/llms/base.py:48:            model: The model name to check\nmem0/llms/base.py:51:            bool: True if the model is a reasoning model or GPT-5 series\nmem0/llms/base.py:53:        reasoning_models = {\nmem0/llms/base.py:58:        if model.lower() in reasoning_models:\nmem0/llms/base.py:61:        model_lower = model.lower()\nmem0/llms/base.py:62:        if any(reasoning_model in model_lower for reasoning_model in [\"gpt-5\", \"o1\", \"o3\"]):\nmem0/llms/base.py:69:        Get parameters that are supported by the current model.\nmem0/llms/base.py:70:        Filters out unsupported parameters for reasoning models and GPT-5 series.\nmem0/llms/base.py:78:        model = getattr(self.config, 'model', '')\nmem0/llms/base.py:80:        if self._is_reasoning_model(model):\nmem0/llms/base.py:94:            # For regular models, include all common parameters\nmem0/llms/base.py:106:            tools (list, optional): List of tools that the model can call. Defaults to None.\nmem0/llms/base.py:108:            **kwargs: Additional provider-specific parameters.\nmem0/llms/base.py:117:        Get common parameters that most providers use.\nmem0/llms/base.py:128:        # Add provider-specific parameters from kwargs\nmem0/client/project.py:15:class ProjectConfig(BaseModel):\nmem0/client/project.py:24:    model_config = ConfigDict(validate_assignment=True, extra=\"forbid\")\nmem0/llms/ollama.py:8:from mem0.configs.llms.base import BaseLlmConfig\nmem0/llms/ollama.py:9:from mem0.configs.llms.ollama import OllamaConfig\nmem0/llms/ollama.py:10:from mem0.llms.base import LLMBase\nmem0/llms/ollama.py:23:                model=config.model,\nmem0/llms/ollama.py:36:        if not self.config.model:\nmem0/llms/ollama.py:37:            self.config.model = \"llama3.1:70b\"\nmem0/llms/ollama.py:83:            tools (list, optional): List of tools that the model can call. Defaults to None.\nmem0/llms/ollama.py:92:            \"model\": self.config.model,\nmem0/llms/xai.py:4:from openai import OpenAI\nmem0/llms/xai.py:6:from mem0.configs.llms.base import BaseLlmConfig\nmem0/llms/xai.py:7:from mem0.llms.base import LLMBase\nmem0/llms/xai.py:14:        if not self.config.model:\nmem0/llms/xai.py:15:            self.config.model = \"grok-2-latest\"\nmem0/llms/xai.py:34:            tools (list, optional): List of tools that the model can call. Defaults to None.\nmem0/llms/xai.py:41:            \"model\": self.config.model,\nmem0/llms/ollama.py:8:from mem0.configs.llms.base import BaseLlmConfig\nmem0/llms/ollama.py:9:from mem0.configs.llms.ollama import OllamaConfig\nmem0/llms/ollama.py:10:from mem0.llms.base import LLMBase\nmem0/llms/ollama.py:23:                model=config.model,\nmem0/llms/ollama.py:36:        if not self.config.model:\nmem0/llms/ollama.py:37:            self.config.model = \"llama3.1:70b\"\nmem0/llms/ollama.py:83:            tools (list, optional): List of tools that the model can call. Defaults to None.\nmem0/llms/ollama.py:92:            \"model\": self.config.model,\nmem0/llms/openai.py:6:from openai import OpenAI\nmem0/llms/openai.py:8:from mem0.configs.llms.base import BaseLlmConfig\nmem0/llms/openai.py:9:from mem0.configs.llms.openai import OpenAIConfig\nmem0/llms/openai.py:10:from mem0.llms.base import LLMBase\nmem0/llms/openai.py:24:                model=config.model,\nmem0/llms/openai.py:37:        if not self.config.model:\nmem0/llms/openai.py:38:            self.config.model = \"gpt-4.1-nano-2025-04-14\"\nmem0/llms/openai.py:49:            base_url = self.config.openai_base_url or os.getenv(\"OPENAI_BASE_URL\") or \"https://api.openai.com/v1\"\nmem0/llms/openai.py:97:            tools (list, optional): List of tools that the model can call. Defaults to None.\nmem0/llms/openai.py:107:            \"model\": self.config.model,\nmem0/llms/openai.py:113:            if self.config.models:\nmem0/llms/openai.py:114:                openrouter_params[\"models\"] = self.config.models\nmem0/llms/openai.py:116:                params.pop(\"model\")\nmem0/llms/openai.py:128:            openai_specific_generation_params = [\"store\"]\nmem0/llms/openai.py:129:            for param in openai_specific_generation_params:\nmem0/llms/langchain.py:3:from mem0.configs.llms.base import BaseLlmConfig\nmem0/llms/langchain.py:4:from mem0.llms.base import LLMBase\nmem0/llms/langchain.py:7:    from langchain.chat_models.base import BaseChatModel\nmem0/llms/langchain.py:17:        if self.config.model is None:\nmem0/llms/langchain.py:18:            raise ValueError(\"`model` parameter is required\")\nmem0/llms/langchain.py:20:        if not isinstance(self.config.model, BaseChatModel):\nmem0/llms/langchain.py:21:            raise ValueError(\"`model` must be an instance of BaseChatModel\")\nmem0/llms/langchain.py:23:        self.langchain_model = self.config.model\nmem0/llms/langchain.py:67:            tools (list, optional): List of tools that the model can call.\nmem0/llms/langchain.py:89:        langchain_model = self.langchain_model\nmem0/llms/langchain.py:91:            langchain_model = langchain_model.bind_tools(tools=tools, tool_choice=tool_choice)\nmem0/llms/langchain.py:93:        response: AIMessage = langchain_model.invoke(langchain_messages)\nmem0/llms/azure_openai_structured.py:4:from azure.identity import DefaultAzureCredential, get_bearer_token_provider\nmem0/llms/azure_openai_structured.py:5:from openai import AzureOpenAI\nmem0/llms/azure_openai_structured.py:7:from mem0.configs.llms.base import BaseLlmConfig\nmem0/llms/azure_openai_structured.py:8:from mem0.llms.base import LLMBase\nmem0/llms/azure_openai_structured.py:18:        if not self.config.model:\nmem0/llms/azure_openai_structured.py:19:            self.config.model = \"gpt-4.1-nano-2025-04-14\"\nmem0/llms/azure_openai_structured.py:30:            azure_ad_token_provider = get_bearer_token_provider(\nmem0/llms/azure_openai_structured.py:36:            azure_ad_token_provider = None\nmem0/llms/azure_openai_structured.py:38:        # Can display a warning if API version is of model and api-version\nmem0/llms/azure_openai_structured.py:42:            azure_ad_token_provider=azure_ad_token_provider,\nmem0/llms/azure_openai_structured.py:74:            \"model\": self.config.model,\nmem0/exceptions.py:202:class ConfigurationError(MemoryError):\nmem0/exceptions.py:270:        - Embedding model unavailable\nmem0/exceptions.py:348:    such as text embedding generation or embedding model errors.\nmem0/exceptions.py:354:            details={\"text_length\": 1000, \"model\": \"openai\"},\nmem0/exceptions.py:355:            suggestion=\"Please check your embedding model configuration\"\nmem0/exceptions.py:359:                 suggestion: str = \"Please check your embedding model configuration\", \nmem0/exceptions.py:368:    such as text generation, completion, or model inference errors.\nmem0/exceptions.py:374:            details={\"model\": \"gpt-4\", \"prompt_length\": 500},\nmem0/exceptions.py:408:    such as optional packages for specific providers or features.\nmem0/utils/factory.py:5:from mem0.configs.llms.anthropic import AnthropicConfig\nmem0/utils/factory.py:6:from mem0.configs.llms.azure import AzureOpenAIConfig\nmem0/utils/factory.py:7:from mem0.configs.llms.base import BaseLlmConfig\nmem0/utils/factory.py:8:from mem0.configs.llms.deepseek import DeepSeekConfig\nmem0/utils/factory.py:9:from mem0.configs.llms.lmstudio import LMStudioConfig\nmem0/utils/factory.py:10:from mem0.configs.llms.ollama import OllamaConfig\nmem0/utils/factory.py:11:from mem0.configs.llms.openai import OpenAIConfig\nmem0/utils/factory.py:12:from mem0.configs.llms.vllm import VllmConfig\nmem0/utils/factory.py:17:from mem0.configs.rerankers.llm import LLMRerankerConfig\nmem0/utils/factory.py:31:    Supports both old-style BaseLlmConfig and new provider-specific configs.\nmem0/utils/factory.py:35:    provider_to_class = {\nmem0/utils/factory.py:36:        \"ollama\": (\"mem0.llms.ollama.OllamaLLM\", OllamaConfig),\nmem0/utils/factory.py:37:        \"openai\": (\"mem0.llms.openai.OpenAILLM\", OpenAIConfig),\nmem0/utils/factory.py:38:        \"groq\": (\"mem0.llms.groq.GroqLLM\", BaseLlmConfig),\nmem0/utils/factory.py:39:        \"together\": (\"mem0.llms.together.TogetherLLM\", BaseLlmConfig),\nmem0/utils/factory.py:40:        \"aws_bedrock\": (\"mem0.llms.aws_bedrock.AWSBedrockLLM\", BaseLlmConfig),\nmem0/utils/factory.py:41:        \"litellm\": (\"mem0.llms.litellm.LiteLLM\", BaseLlmConfig),\nmem0/utils/factory.py:42:        \"azure_openai\": (\"mem0.llms.azure_openai.AzureOpenAILLM\", AzureOpenAIConfig),\nmem0/utils/factory.py:43:        \"openai_structured\": (\"mem0.llms.openai_structured.OpenAIStructuredLLM\", OpenAIConfig),\nmem0/utils/factory.py:44:        \"anthropic\": (\"mem0.llms.anthropic.AnthropicLLM\", AnthropicConfig),\nmem0/utils/factory.py:45:        \"azure_openai_structured\": (\"mem0.llms.azure_openai_structured.AzureOpenAIStructuredLLM\", AzureOpenAIConfig),\nmem0/utils/factory.py:46:        \"gemini\": (\"mem0.llms.gemini.GeminiLLM\", BaseLlmConfig),\nmem0/utils/factory.py:47:        \"deepseek\": (\"mem0.llms.deepseek.DeepSeekLLM\", DeepSeekConfig),\nmem0/utils/factory.py:48:        \"xai\": (\"mem0.llms.xai.XAILLM\", BaseLlmConfig),\nmem0/utils/factory.py:49:        \"sarvam\": (\"mem0.llms.sarvam.SarvamLLM\", BaseLlmConfig),\nmem0/utils/factory.py:50:        \"lmstudio\": (\"mem0.llms.lmstudio.LMStudioLLM\", LMStudioConfig),\nmem0/utils/factory.py:51:        \"vllm\": (\"mem0.llms.vllm.VllmLLM\", VllmConfig),\nmem0/utils/factory.py:52:        \"langchain\": (\"mem0.llms.langchain.LangchainLLM\", BaseLlmConfig),\nmem0/utils/factory.py:56:    def create(cls, provider_name: str, config: Optional[Union[BaseLlmConfig, Dict]] = None, **kwargs):\nmem0/utils/factory.py:61:            provider_name (str): The provider name (e.g., 'openai', 'anthropic')\nmem0/utils/factory.py:69:            ValueError: If provider is not supported\nmem0/utils/factory.py:71:        if provider_name not in cls.provider_to_class:\nmem0/utils/factory.py:72:            raise ValueError(f\"Unsupported Llm provider: {provider_name}\")\nmem0/utils/factory.py:74:        class_type, config_class = cls.provider_to_class[provider_name]\nmem0/utils/factory.py:75:        llm_class = load_class(class_type)\nmem0/utils/factory.py:86:            # Convert base config to provider-specific config if needed\nmem0/utils/factory.py:87:            if config_class != BaseLlmConfig:\nmem0/utils/factory.py:88:                # Convert to provider-specific config\nmem0/utils/factory.py:90:                    \"model\": config.model,\nmem0/utils/factory.py:109:        return llm_class(config)\nmem0/utils/factory.py:112:    def register_provider(cls, name: str, class_path: str, config_class=None):\nmem0/utils/factory.py:114:        Register a new provider.\nmem0/utils/factory.py:119:            config_class: Configuration class for the provider (defaults to BaseLlmConfig)\nmem0/utils/factory.py:122:            config_class = BaseLlmConfig\nmem0/utils/factory.py:123:        cls.provider_to_class[name] = (class_path, config_class)\nmem0/utils/factory.py:126:    def get_supported_providers(cls) -> list:\nmem0/utils/factory.py:128:        Get list of supported providers.\nmem0/utils/factory.py:131:            list: List of supported provider names\nmem0/utils/factory.py:133:        return list(cls.provider_to_class.keys())\nmem0/utils/factory.py:137:    provider_to_class = {\nmem0/utils/factory.py:138:        \"openai\": \"mem0.embeddings.openai.OpenAIEmbedding\",\nmem0/utils/factory.py:141:        \"azure_openai\": \"mem0.embeddings.azure_openai.AzureOpenAIEmbedding\",\nmem0/utils/factory.py:152:    def create(cls, provider_name, config, vector_config: Optional[dict]):\nmem0/utils/factory.py:153:        if provider_name == \"upstash_vector\" and vector_config and vector_config.enable_embeddings:\nmem0/utils/factory.py:155:        class_type = cls.provider_to_class.get(provider_name)\nmem0/utils/factory.py:161:            raise ValueError(f\"Unsupported Embedder provider: {provider_name}\")\nmem0/utils/factory.py:165:    provider_to_class = {\nmem0/utils/factory.py:192:    def create(cls, provider_name, config):\nmem0/utils/factory.py:193:        class_type = cls.provider_to_class.get(provider_name)\nmem0/utils/factory.py:196:                config = config.model_dump()\nmem0/utils/factory.py:200:            raise ValueError(f\"Unsupported VectorStore provider: {provider_name}\")\nmem0/utils/factory.py:210:    Factory for creating MemoryGraph instances for different graph store providers.\nmem0/utils/factory.py:211:    Usage: GraphStoreFactory.create(provider_name, config)\nmem0/utils/factory.py:214:    provider_to_class = {\nmem0/utils/factory.py:223:    def create(cls, provider_name, config):\nmem0/utils/factory.py:224:        class_type = cls.provider_to_class.get(provider_name, cls.provider_to_class[\"default\"])\nmem0/utils/factory.py:228:            raise ImportError(f\"Could not import MemoryGraph for provider '{provider_name}': {e}\")\nmem0/utils/factory.py:235:    Supports provider-specific configs following the same pattern as other factories.\nmem0/utils/factory.py:239:    provider_to_class = {\nmem0/utils/factory.py:243:        \"llm_reranker\": (\"mem0.reranker.llm_reranker.LLMReranker\", LLMRerankerConfig),\nmem0/utils/factory.py:248:    def create(cls, provider_name: str, config: Optional[Union[BaseRerankerConfig, Dict]] = None, **kwargs):\nmem0/utils/factory.py:250:        Create a reranker instance based on the provider and configuration.\nmem0/utils/factory.py:253:            provider_name: The reranker provider (e.g., 'cohere', 'sentence_transformer')\nmem0/utils/factory.py:258:            Reranker instance configured for the specified provider\nmem0/utils/factory.py:261:            ImportError: If the provider class cannot be imported\nmem0/utils/factory.py:262:            ValueError: If the provider is not supported\nmem0/utils/factory.py:264:        if provider_name not in cls.provider_to_class:\nmem0/utils/factory.py:265:            raise ValueError(f\"Unsupported reranker provider: {provider_name}\")\nmem0/utils/factory.py:267:        class_path, config_class = cls.provider_to_class[provider_name]\nmem0/utils/factory.py:281:            raise ImportError(f\"Could not import reranker for provider '{provider_name}': {e}\")\nmem0/llms/openai_structured.py:4:from openai import OpenAI\nmem0/llms/openai_structured.py:6:from mem0.configs.llms.base import BaseLlmConfig\nmem0/llms/openai_structured.py:7:from mem0.llms.base import LLMBase\nmem0/llms/openai_structured.py:14:        if not self.config.model:\nmem0/llms/openai_structured.py:15:            self.config.model = \"gpt-4o-2024-08-06\"\nmem0/llms/openai_structured.py:18:        base_url = self.config.openai_base_url or os.getenv(\"OPENAI_API_BASE\") or \"https://api.openai.com/v1\"\nmem0/llms/openai_structured.py:40:            \"model\": self.config.model,\nmem0/llms/anthropic.py:9:from mem0.configs.llms.anthropic import AnthropicConfig\nmem0/llms/anthropic.py:10:from mem0.configs.llms.base import BaseLlmConfig\nmem0/llms/anthropic.py:11:from mem0.llms.base import LLMBase\nmem0/llms/anthropic.py:24:                model=config.model,\nmem0/llms/anthropic.py:37:        if not self.config.model:\nmem0/llms/anthropic.py:38:            self.config.model = \"claude-3-5-sonnet-20240620\"\nmem0/llms/anthropic.py:57:            tools (list, optional): List of tools that the model can call. Defaults to None.\nmem0/llms/anthropic.py:76:                \"model\": self.config.model,\nmem0/llms/openai_structured.py:4:from openai import OpenAI\nmem0/llms/openai_structured.py:6:from mem0.configs.llms.base import BaseLlmConfig\nmem0/llms/openai_structured.py:7:from mem0.llms.base import LLMBase\nmem0/llms/openai_structured.py:14:        if not self.config.model:\nmem0/llms/openai_structured.py:15:            self.config.model = \"gpt-4o-2024-08-06\"\nmem0/llms/openai_structured.py:18:        base_url = self.config.openai_base_url or os.getenv(\"OPENAI_API_BASE\") or \"https://api.openai.com/v1\"\nmem0/llms/openai_structured.py:40:            \"model\": self.config.model,\nmem0/llms/langchain.py:3:from mem0.configs.llms.base import BaseLlmConfig\nmem0/llms/langchain.py:4:from mem0.llms.base import LLMBase\nmem0/llms/langchain.py:7:    from langchain.chat_models.base import BaseChatModel\nmem0/llms/langchain.py:17:        if self.config.model is None:\nmem0/llms/langchain.py:18:            raise ValueError(\"`model` parameter is required\")\nmem0/llms/langchain.py:20:        if not isinstance(self.config.model, BaseChatModel):\nmem0/llms/langchain.py:21:            raise ValueError(\"`model` must be an instance of BaseChatModel\")\nmem0/llms/langchain.py:23:        self.langchain_model = self.config.model\nmem0/llms/langchain.py:67:            tools (list, optional): List of tools that the model can call.\nmem0/llms/langchain.py:89:        langchain_model = self.langchain_model\nmem0/llms/langchain.py:91:            langchain_model = langchain_model.bind_tools(tools=tools, tool_choice=tool_choice)\nmem0/llms/langchain.py:93:        response: AIMessage = langchain_model.invoke(langchain_messages)\nmem0/llms/lmstudio.py:4:from openai import OpenAI\nmem0/llms/lmstudio.py:6:from mem0.configs.llms.base import BaseLlmConfig\nmem0/llms/lmstudio.py:7:from mem0.configs.llms.lmstudio import LMStudioConfig\nmem0/llms/lmstudio.py:8:from mem0.llms.base import LLMBase\nmem0/llms/lmstudio.py:22:                model=config.model,\nmem0/llms/lmstudio.py:35:        self.config.model = (\nmem0/llms/lmstudio.py:36:            self.config.model\nmem0/llms/lmstudio.py:87:            tools (list, optional): List of tools that the model can call. Defaults to None.\nmem0/llms/lmstudio.py:97:                \"model\": self.config.model,\nmem0/configs/base.py:8:from mem0.llms.configs import LlmConfig\nmem0/configs/base.py:30:class MemoryConfig(BaseModel):\nmem0/configs/base.py:33:        default_factory=VectorStoreConfig,\nmem0/configs/base.py:35:    llm: LlmConfig = Field(\nmem0/configs/base.py:36:        description=\"Configuration for the language model\",\nmem0/configs/base.py:37:        default_factory=LlmConfig,\nmem0/configs/base.py:40:        description=\"Configuration for the embedding model\",\nmem0/configs/base.py:41:        default_factory=EmbedderConfig,\nmem0/configs/base.py:49:        default_factory=GraphStoreConfig,\nmem0/configs/base.py:69:class AzureConfig(BaseModel):\nmem0/llms/sarvam.py:6:from mem0.configs.llms.base import BaseLlmConfig\nmem0/llms/sarvam.py:7:from mem0.llms.base import LLMBase\nmem0/llms/sarvam.py:14:        # Set default model if not provided\nmem0/llms/sarvam.py:15:        if not self.config.model:\nmem0/llms/sarvam.py:16:            self.config.model = \"sarvam-m\"\nmem0/llms/sarvam.py:50:            \"model\": self.config.model if isinstance(self.config.model, str) else \"sarvam-m\",\nmem0/llms/sarvam.py:63:        # Handle Sarvam-specific parameters if model is passed as dict\nmem0/llms/sarvam.py:64:        if isinstance(self.config.model, dict):\nmem0/llms/sarvam.py:65:            # Extract model name\nmem0/llms/sarvam.py:66:            params[\"model\"] = self.config.model.get(\"name\", \"sarvam-m\")\nmem0/llms/sarvam.py:72:                if param in self.config.model:\nmem0/llms/sarvam.py:73:                    params[param] = self.config.model[param]\nmem0/llms/vllm.py:5:from openai import OpenAI\nmem0/llms/vllm.py:7:from mem0.configs.llms.base import BaseLlmConfig\nmem0/llms/vllm.py:8:from mem0.configs.llms.vllm import VllmConfig\nmem0/llms/vllm.py:9:from mem0.llms.base import LLMBase\nmem0/llms/vllm.py:13:class VllmLLM(LLMBase):\nmem0/llms/vllm.py:14:    def __init__(self, config: Optional[Union[BaseLlmConfig, VllmConfig, Dict]] = None):\nmem0/llms/vllm.py:15:        # Convert to VllmConfig if needed\nmem0/llms/vllm.py:17:            config = VllmConfig()\nmem0/llms/vllm.py:19:            config = VllmConfig(**config)\nmem0/llms/vllm.py:20:        elif isinstance(config, BaseLlmConfig) and not isinstance(config, VllmConfig):\nmem0/llms/vllm.py:21:            # Convert BaseLlmConfig to VllmConfig\nmem0/llms/vllm.py:22:            config = VllmConfig(\nmem0/llms/vllm.py:23:                model=config.model,\nmem0/llms/vllm.py:36:        if not self.config.model:\nmem0/llms/vllm.py:37:            self.config.model = \"Qwen/Qwen2.5-32B-Instruct\"\nmem0/llms/vllm.py:39:        self.config.api_key = self.config.api_key or os.getenv(\"VLLM_API_KEY\") or \"vllm-api-key\"\nmem0/llms/vllm.py:40:        base_url = self.config.vllm_base_url or os.getenv(\"VLLM_BASE_URL\")\nmem0/llms/vllm.py:87:            tools (list, optional): List of tools that the model can call. Defaults to None.\nmem0/llms/vllm.py:97:                \"model\": self.config.model,\nmem0/llms/groq.py:10:from mem0.configs.llms.base import BaseLlmConfig\nmem0/llms/groq.py:11:from mem0.llms.base import LLMBase\nmem0/llms/groq.py:19:        if not self.config.model:\nmem0/llms/groq.py:20:            self.config.model = \"llama3-70b-8192\"\nmem0/llms/groq.py:68:            tools (list, optional): List of tools that the model can call. Defaults to None.\nmem0/llms/groq.py:75:            \"model\": self.config.model,\nmem0/embeddings/fastembed.py:15:        self.config.model = self.config.model or \"thenlper/gte-large\"\nmem0/embeddings/fastembed.py:16:        self.dense_model = TextEmbedding(model_name = self.config.model)\nmem0/embeddings/fastembed.py:28:        embeddings = list(self.dense_model.embed(text))\nmem0/llms/together.py:10:from mem0.configs.llms.base import BaseLlmConfig\nmem0/llms/together.py:11:from mem0.llms.base import LLMBase\nmem0/llms/together.py:19:        if not self.config.model:\nmem0/llms/together.py:20:            self.config.model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\nmem0/llms/together.py:68:            tools (list, optional): List of tools that the model can call. Defaults to None.\nmem0/llms/together.py:75:            \"model\": self.config.model,\nmem0/llms/together.py:10:from mem0.configs.llms.base import BaseLlmConfig\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_18","type":"command_execution","command":"/bin/zsh -lc 'git branch -a && git remote -v && (git symbolic-ref refs/remotes/origin/HEAD 2>/dev/null || true)'","aggregated_output":"  feature/forge-20260224-172250\n* feature/forge-20260224-205011\n  main\n  remotes/origin/HEAD -> origin/main\n  remotes/origin/api_get_memories\n  remotes/origin/aws_redict\n  remotes/origin/ci_test_main\n  remotes/origin/docs/events-api-cleanup\n  remotes/origin/docs/index_revamp\n  remotes/origin/docs_cleanup\n  remotes/origin/docs_revamp/iter_1\n  remotes/origin/docs_revamp/iteration1\n  remotes/origin/docs_revamp/part5\n  remotes/origin/docs_revamp/part6\n  remotes/origin/docs_revamp/part8\n  remotes/origin/docs_sweeps\n  remotes/origin/embedchain\n  remotes/origin/feature/add-forge-llm-provider\n  remotes/origin/feature/forge-20260224-172250\n  remotes/origin/filters_docs\n  remotes/origin/filters_fixes\n  remotes/origin/fix-substack-link\n  remotes/origin/fix/api-auth-header\n  remotes/origin/fix/embedchain-deprecation-imports\n  remotes/origin/fix/failing_linting_test_errors\n  remotes/origin/fix/migration-collection-name\n  remotes/origin/getall_id_fault\n  remotes/origin/graph\n  remotes/origin/graph_platform_docs\n  remotes/origin/icon_tab\n  remotes/origin/import_message\n  remotes/origin/langgraph-mem0-multiagent\n  remotes/origin/loader/s3\n  remotes/origin/loaders/mssql\n  remotes/origin/main\n  remotes/origin/mayank/memory_improvement\n  remotes/origin/mem0-1.0.0\n  remotes/origin/mem0-1.0.0-new\n  remotes/origin/migration-guide\n  remotes/origin/oss_quickstart_fix\n  remotes/origin/pipecat_pointer\n  remotes/origin/platform_redirect\n  remotes/origin/user/akshat/ci-pipeline-fix\n  remotes/origin/user/akshat/mem0-llm-gemini-migration\n  remotes/origin/user/akshat/mem0-llm-json-parsing\n  remotes/origin/user/akshat/node-tests\n  remotes/origin/user/akshat/openmemory-docker-memory\n  remotes/origin/user/akshat/openmemory-docs\n  remotes/origin/user/antaripa/add-sarvam\n  remotes/origin/user/antaripa/cartesia-agno-agent\n  remotes/origin/user/dev/add-audio-docs\n  remotes/origin/user/dev/async-doc\n  remotes/origin/user/dev/batch-apis\n  remotes/origin/user/dev/doc-multillm\n  remotes/origin/user/dev/docs-webhook\n  remotes/origin/user/dev/org-proj-update-api-reference\n  remotes/origin/user/dev/update-events\n  remotes/origin/user/dev/update-multimodal-example\n  remotes/origin/user/dyadav/add-qdrant-filtering\n  remotes/origin/user/dyadav/fixes\n  remotes/origin/user/mayank\n  remotes/origin/user/parshva/azure-mysql\n  remotes/origin/user/parshva/fix_dependency_conflict\n  remotes/origin/user/parshva/fix_discord_logo\n  remotes/origin/user/parshva/fix_docs_search_memories\n  remotes/origin/user/parshva/lmstudio_typescript\n  remotes/origin/user/parshva/update_banner\n  remotes/origin/user/parshva/update_default_llm\n  remotes/origin/user/parshva/update_req_docker\n  remotes/origin/user/prateek/docs-architecture-diagram\n  remotes/origin/user/prateek/new-add-version\n  remotes/origin/user/prateek/procedural-memory-separate-feature\n  remotes/origin/user/prateek/procedural_memory\n  remotes/origin/user/prateek/type-hinting\n  remotes/origin/user/sidmohanty11/oss-rag-example\n  remotes/origin/user/test/mem0\n  remotes/origin/user/vikram/enable-openmemory-vector-stores\n  remotes/origin/user/vikram/general-vector-search\n  remotes/origin/user_id_doc\n  remotes/origin/zoomeffect\n  remotes/upstream/bump-openclaw-version\n  remotes/upstream/copilot/fix-per-agent-memory-isolation\n  remotes/upstream/main\norigin\thttps://github.com/Yiiii0/mem0.git (fetch)\norigin\thttps://github.com/Yiiii0/mem0.git (push)\nupstream\thttps://github.com/mem0ai/mem0.git (fetch)\nupstream\thttps://github.com/mem0ai/mem0.git (push)\nrefs/remotes/origin/main\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_19","type":"reasoning","text":"**Inspecting OpenAI and Forge integration**"}}
{"type":"item.started","item":{"id":"item_20","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' mem0/utils/factory.py\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_21","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,320p' mem0/llms/openai.py\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_22","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' mem0/configs/llms/openai.py\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_23","type":"command_execution","command":"/bin/zsh -lc 'git status --short'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_22","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' mem0/configs/llms/openai.py\"","aggregated_output":"from typing import Any, Callable, List, Optional\n\nfrom mem0.configs.llms.base import BaseLlmConfig\n\n\nclass OpenAIConfig(BaseLlmConfig):\n    \"\"\"\n    Configuration class for OpenAI and OpenRouter-specific parameters.\n    Inherits from BaseLlmConfig and adds OpenAI-specific settings.\n    \"\"\"\n\n    def __init__(\n        self,\n        # Base parameters\n        model: Optional[str] = None,\n        temperature: float = 0.1,\n        api_key: Optional[str] = None,\n        max_tokens: int = 2000,\n        top_p: float = 0.1,\n        top_k: int = 1,\n        enable_vision: bool = False,\n        vision_details: Optional[str] = \"auto\",\n        http_client_proxies: Optional[dict] = None,\n        # OpenAI-specific parameters\n        openai_base_url: Optional[str] = None,\n        models: Optional[List[str]] = None,\n        route: Optional[str] = \"fallback\",\n        openrouter_base_url: Optional[str] = None,\n        site_url: Optional[str] = None,\n        app_name: Optional[str] = None,\n        store: bool = False,\n        # Response monitoring callback\n        response_callback: Optional[Callable[[Any, dict, dict], None]] = None,\n    ):\n        \"\"\"\n        Initialize OpenAI configuration.\n\n        Args:\n            model: OpenAI model to use, defaults to None\n            temperature: Controls randomness, defaults to 0.1\n            api_key: OpenAI API key, defaults to None\n            max_tokens: Maximum tokens to generate, defaults to 2000\n            top_p: Nucleus sampling parameter, defaults to 0.1\n            top_k: Top-k sampling parameter, defaults to 1\n            enable_vision: Enable vision capabilities, defaults to False\n            vision_details: Vision detail level, defaults to \"auto\"\n            http_client_proxies: HTTP client proxy settings, defaults to None\n            openai_base_url: OpenAI API base URL, defaults to None\n            models: List of models for OpenRouter, defaults to None\n            route: OpenRouter route strategy, defaults to \"fallback\"\n            openrouter_base_url: OpenRouter base URL, defaults to None\n            site_url: Site URL for OpenRouter, defaults to None\n            app_name: Application name for OpenRouter, defaults to None\n            response_callback: Optional callback for monitoring LLM responses.\n        \"\"\"\n        # Initialize base parameters\n        super().__init__(\n            model=model,\n            temperature=temperature,\n            api_key=api_key,\n            max_tokens=max_tokens,\n            top_p=top_p,\n            top_k=top_k,\n            enable_vision=enable_vision,\n            vision_details=vision_details,\n            http_client_proxies=http_client_proxies,\n        )\n\n        # OpenAI-specific parameters\n        self.openai_base_url = openai_base_url\n        self.models = models\n        self.route = route\n        self.openrouter_base_url = openrouter_base_url\n        self.site_url = site_url\n        self.app_name = app_name\n        self.store = store\n\n        # Response monitoring\n        self.response_callback = response_callback\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_20","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' mem0/utils/factory.py\"","aggregated_output":"import importlib\nfrom typing import Dict, Optional, Union\n\nfrom mem0.configs.embeddings.base import BaseEmbedderConfig\nfrom mem0.configs.llms.anthropic import AnthropicConfig\nfrom mem0.configs.llms.azure import AzureOpenAIConfig\nfrom mem0.configs.llms.base import BaseLlmConfig\nfrom mem0.configs.llms.deepseek import DeepSeekConfig\nfrom mem0.configs.llms.lmstudio import LMStudioConfig\nfrom mem0.configs.llms.ollama import OllamaConfig\nfrom mem0.configs.llms.openai import OpenAIConfig\nfrom mem0.configs.llms.vllm import VllmConfig\nfrom mem0.configs.rerankers.base import BaseRerankerConfig\nfrom mem0.configs.rerankers.cohere import CohereRerankerConfig\nfrom mem0.configs.rerankers.sentence_transformer import SentenceTransformerRerankerConfig\nfrom mem0.configs.rerankers.zero_entropy import ZeroEntropyRerankerConfig\nfrom mem0.configs.rerankers.llm import LLMRerankerConfig\nfrom mem0.configs.rerankers.huggingface import HuggingFaceRerankerConfig\nfrom mem0.embeddings.mock import MockEmbeddings\n\n\ndef load_class(class_type):\n    module_path, class_name = class_type.rsplit(\".\", 1)\n    module = importlib.import_module(module_path)\n    return getattr(module, class_name)\n\n\nclass LlmFactory:\n    \"\"\"\n    Factory for creating LLM instances with appropriate configurations.\n    Supports both old-style BaseLlmConfig and new provider-specific configs.\n    \"\"\"\n\n    # Provider mappings with their config classes\n    provider_to_class = {\n        \"ollama\": (\"mem0.llms.ollama.OllamaLLM\", OllamaConfig),\n        \"openai\": (\"mem0.llms.openai.OpenAILLM\", OpenAIConfig),\n        \"groq\": (\"mem0.llms.groq.GroqLLM\", BaseLlmConfig),\n        \"together\": (\"mem0.llms.together.TogetherLLM\", BaseLlmConfig),\n        \"aws_bedrock\": (\"mem0.llms.aws_bedrock.AWSBedrockLLM\", BaseLlmConfig),\n        \"litellm\": (\"mem0.llms.litellm.LiteLLM\", BaseLlmConfig),\n        \"azure_openai\": (\"mem0.llms.azure_openai.AzureOpenAILLM\", AzureOpenAIConfig),\n        \"openai_structured\": (\"mem0.llms.openai_structured.OpenAIStructuredLLM\", OpenAIConfig),\n        \"anthropic\": (\"mem0.llms.anthropic.AnthropicLLM\", AnthropicConfig),\n        \"azure_openai_structured\": (\"mem0.llms.azure_openai_structured.AzureOpenAIStructuredLLM\", AzureOpenAIConfig),\n        \"gemini\": (\"mem0.llms.gemini.GeminiLLM\", BaseLlmConfig),\n        \"deepseek\": (\"mem0.llms.deepseek.DeepSeekLLM\", DeepSeekConfig),\n        \"xai\": (\"mem0.llms.xai.XAILLM\", BaseLlmConfig),\n        \"sarvam\": (\"mem0.llms.sarvam.SarvamLLM\", BaseLlmConfig),\n        \"lmstudio\": (\"mem0.llms.lmstudio.LMStudioLLM\", LMStudioConfig),\n        \"vllm\": (\"mem0.llms.vllm.VllmLLM\", VllmConfig),\n        \"langchain\": (\"mem0.llms.langchain.LangchainLLM\", BaseLlmConfig),\n    }\n\n    @classmethod\n    def create(cls, provider_name: str, config: Optional[Union[BaseLlmConfig, Dict]] = None, **kwargs):\n        \"\"\"\n        Create an LLM instance with the appropriate configuration.\n\n        Args:\n            provider_name (str): The provider name (e.g., 'openai', 'anthropic')\n            config: Configuration object or dict. If None, will create default config\n            **kwargs: Additional configuration parameters\n\n        Returns:\n            Configured LLM instance\n\n        Raises:\n            ValueError: If provider is not supported\n        \"\"\"\n        if provider_name not in cls.provider_to_class:\n            raise ValueError(f\"Unsupported Llm provider: {provider_name}\")\n\n        class_type, config_class = cls.provider_to_class[provider_name]\n        llm_class = load_class(class_type)\n\n        # Handle configuration\n        if config is None:\n            # Create default config with kwargs\n            config = config_class(**kwargs)\n        elif isinstance(config, dict):\n            # Merge dict config with kwargs\n            config.update(kwargs)\n            config = config_class(**config)\n        elif isinstance(config, BaseLlmConfig):\n            # Convert base config to provider-specific config if needed\n            if config_class != BaseLlmConfig:\n                # Convert to provider-specific config\n                config_dict = {\n                    \"model\": config.model,\n                    \"temperature\": config.temperature,\n                    \"api_key\": config.api_key,\n                    \"max_tokens\": config.max_tokens,\n                    \"top_p\": config.top_p,\n                    \"top_k\": config.top_k,\n                    \"enable_vision\": config.enable_vision,\n                    \"vision_details\": config.vision_details,\n                    \"http_client_proxies\": config.http_client,\n                }\n                config_dict.update(kwargs)\n                config = config_class(**config_dict)\n            else:\n                # Use base config as-is\n                pass\n        else:\n            # Assume it's already the correct config type\n            pass\n\n        return llm_class(config)\n\n    @classmethod\n    def register_provider(cls, name: str, class_path: str, config_class=None):\n        \"\"\"\n        Register a new provider.\n\n        Args:\n            name (str): Provider name\n            class_path (str): Full path to LLM class\n            config_class: Configuration class for the provider (defaults to BaseLlmConfig)\n        \"\"\"\n        if config_class is None:\n            config_class = BaseLlmConfig\n        cls.provider_to_class[name] = (class_path, config_class)\n\n    @classmethod\n    def get_supported_providers(cls) -> list:\n        \"\"\"\n        Get list of supported providers.\n\n        Returns:\n            list: List of supported provider names\n        \"\"\"\n        return list(cls.provider_to_class.keys())\n\n\nclass EmbedderFactory:\n    provider_to_class = {\n        \"openai\": \"mem0.embeddings.openai.OpenAIEmbedding\",\n        \"ollama\": \"mem0.embeddings.ollama.OllamaEmbedding\",\n        \"huggingface\": \"mem0.embeddings.huggingface.HuggingFaceEmbedding\",\n        \"azure_openai\": \"mem0.embeddings.azure_openai.AzureOpenAIEmbedding\",\n        \"gemini\": \"mem0.embeddings.gemini.GoogleGenAIEmbedding\",\n        \"vertexai\": \"mem0.embeddings.vertexai.VertexAIEmbedding\",\n        \"together\": \"mem0.embeddings.together.TogetherEmbedding\",\n        \"lmstudio\": \"mem0.embeddings.lmstudio.LMStudioEmbedding\",\n        \"langchain\": \"mem0.embeddings.langchain.LangchainEmbedding\",\n        \"aws_bedrock\": \"mem0.embeddings.aws_bedrock.AWSBedrockEmbedding\",\n        \"fastembed\": \"mem0.embeddings.fastembed.FastEmbedEmbedding\",\n    }\n\n    @classmethod\n    def create(cls, provider_name, config, vector_config: Optional[dict]):\n        if provider_name == \"upstash_vector\" and vector_config and vector_config.enable_embeddings:\n            return MockEmbeddings()\n        class_type = cls.provider_to_class.get(provider_name)\n        if class_type:\n            embedder_instance = load_class(class_type)\n            base_config = BaseEmbedderConfig(**config)\n            return embedder_instance(base_config)\n        else:\n            raise ValueError(f\"Unsupported Embedder provider: {provider_name}\")\n\n\nclass VectorStoreFactory:\n    provider_to_class = {\n        \"qdrant\": \"mem0.vector_stores.qdrant.Qdrant\",\n        \"chroma\": \"mem0.vector_stores.chroma.ChromaDB\",\n        \"pgvector\": \"mem0.vector_stores.pgvector.PGVector\",\n        \"milvus\": \"mem0.vector_stores.milvus.MilvusDB\",\n        \"upstash_vector\": \"mem0.vector_stores.upstash_vector.UpstashVector\",\n        \"azure_ai_search\": \"mem0.vector_stores.azure_ai_search.AzureAISearch\",\n        \"azure_mysql\": \"mem0.vector_stores.azure_mysql.AzureMySQL\",\n        \"pinecone\": \"mem0.vector_stores.pinecone.PineconeDB\",\n        \"mongodb\": \"mem0.vector_stores.mongodb.MongoDB\",\n        \"redis\": \"mem0.vector_stores.redis.RedisDB\",\n        \"valkey\": \"mem0.vector_stores.valkey.ValkeyDB\",\n        \"databricks\": \"mem0.vector_stores.databricks.Databricks\",\n        \"elasticsearch\": \"mem0.vector_stores.elasticsearch.ElasticsearchDB\",\n        \"vertex_ai_vector_search\": \"mem0.vector_stores.vertex_ai_vector_search.GoogleMatchingEngine\",\n        \"opensearch\": \"mem0.vector_stores.opensearch.OpenSearchDB\",\n        \"supabase\": \"mem0.vector_stores.supabase.Supabase\",\n        \"weaviate\": \"mem0.vector_stores.weaviate.Weaviate\",\n        \"faiss\": \"mem0.vector_stores.faiss.FAISS\",\n        \"langchain\": \"mem0.vector_stores.langchain.Langchain\",\n        \"s3_vectors\": \"mem0.vector_stores.s3_vectors.S3Vectors\",\n        \"baidu\": \"mem0.vector_stores.baidu.BaiduDB\",\n        \"cassandra\": \"mem0.vector_stores.cassandra.CassandraDB\",\n        \"neptune\": \"mem0.vector_stores.neptune_analytics.NeptuneAnalyticsVector\",\n    }\n\n    @classmethod\n    def create(cls, provider_name, config):\n        class_type = cls.provider_to_class.get(provider_name)\n        if class_type:\n            if not isinstance(config, dict):\n                config = config.model_dump()\n            vector_store_instance = load_class(class_type)\n            return vector_store_instance(**config)\n        else:\n            raise ValueError(f\"Unsupported VectorStore provider: {provider_name}\")\n\n    @classmethod\n    def reset(cls, instance):\n        instance.reset()\n        return instance\n\n\nclass GraphStoreFactory:\n    \"\"\"\n    Factory for creating MemoryGraph instances for different graph store providers.\n    Usage: GraphStoreFactory.create(provider_name, config)\n    \"\"\"\n\n    provider_to_class = {\n        \"memgraph\": \"mem0.memory.memgraph_memory.MemoryGraph\",\n        \"neptune\": \"mem0.graphs.neptune.neptunegraph.MemoryGraph\",\n        \"neptunedb\": \"mem0.graphs.neptune.neptunedb.MemoryGraph\",\n        \"kuzu\": \"mem0.memory.kuzu_memory.MemoryGraph\",\n        \"default\": \"mem0.memory.graph_memory.MemoryGraph\",\n    }\n\n    @classmethod\n    def create(cls, provider_name, config):\n        class_type = cls.provider_to_class.get(provider_name, cls.provider_to_class[\"default\"])\n        try:\n            GraphClass = load_class(class_type)\n        except (ImportError, AttributeError) as e:\n            raise ImportError(f\"Could not import MemoryGraph for provider '{provider_name}': {e}\")\n        return GraphClass(config)\n\n\nclass RerankerFactory:\n    \"\"\"\n    Factory for creating reranker instances with appropriate configurations.\n    Supports provider-specific configs following the same pattern as other factories.\n    \"\"\"\n\n    # Provider mappings with their config classes\n    provider_to_class = {\n        \"cohere\": (\"mem0.reranker.cohere_reranker.CohereReranker\", CohereRerankerConfig),\n        \"sentence_transformer\": (\"mem0.reranker.sentence_transformer_reranker.SentenceTransformerReranker\", SentenceTransformerRerankerConfig),\n        \"zero_entropy\": (\"mem0.reranker.zero_entropy_reranker.ZeroEntropyReranker\", ZeroEntropyRerankerConfig),\n        \"llm_reranker\": (\"mem0.reranker.llm_reranker.LLMReranker\", LLMRerankerConfig),\n        \"huggingface\": (\"mem0.reranker.huggingface_reranker.HuggingFaceReranker\", HuggingFaceRerankerConfig),\n    }\n\n    @classmethod\n    def create(cls, provider_name: str, config: Optional[Union[BaseRerankerConfig, Dict]] = None, **kwargs):\n        \"\"\"\n        Create a reranker instance based on the provider and configuration.\n\n        Args:\n            provider_name: The reranker provider (e.g., 'cohere', 'sentence_transformer')\n            config: Configuration object or dictionary\n            **kwargs: Additional configuration parameters\n\n        Returns:\n            Reranker instance configured for the specified provider\n\n        Raises:\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_21","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,320p' mem0/llms/openai.py\"","aggregated_output":"import json\nimport logging\nimport os\nfrom typing import Dict, List, Optional, Union\n\nfrom openai import OpenAI\n\nfrom mem0.configs.llms.base import BaseLlmConfig\nfrom mem0.configs.llms.openai import OpenAIConfig\nfrom mem0.llms.base import LLMBase\nfrom mem0.memory.utils import extract_json\n\n\nclass OpenAILLM(LLMBase):\n    def __init__(self, config: Optional[Union[BaseLlmConfig, OpenAIConfig, Dict]] = None):\n        # Convert to OpenAIConfig if needed\n        if config is None:\n            config = OpenAIConfig()\n        elif isinstance(config, dict):\n            config = OpenAIConfig(**config)\n        elif isinstance(config, BaseLlmConfig) and not isinstance(config, OpenAIConfig):\n            # Convert BaseLlmConfig to OpenAIConfig\n            config = OpenAIConfig(\n                model=config.model,\n                temperature=config.temperature,\n                api_key=config.api_key,\n                max_tokens=config.max_tokens,\n                top_p=config.top_p,\n                top_k=config.top_k,\n                enable_vision=config.enable_vision,\n                vision_details=config.vision_details,\n                http_client_proxies=config.http_client,\n            )\n\n        super().__init__(config)\n\n        if not self.config.model:\n            self.config.model = \"gpt-4.1-nano-2025-04-14\"\n\n        if os.environ.get(\"OPENROUTER_API_KEY\"):  # Use OpenRouter\n            self.client = OpenAI(\n                api_key=os.environ.get(\"OPENROUTER_API_KEY\"),\n                base_url=self.config.openrouter_base_url\n                or os.getenv(\"OPENROUTER_API_BASE\")\n                or \"https://openrouter.ai/api/v1\",\n            )\n        else:\n            api_key = self.config.api_key or os.getenv(\"OPENAI_API_KEY\")\n            base_url = self.config.openai_base_url or os.getenv(\"OPENAI_BASE_URL\") or \"https://api.openai.com/v1\"\n\n            self.client = OpenAI(api_key=api_key, base_url=base_url)\n\n    def _parse_response(self, response, tools):\n        \"\"\"\n        Process the response based on whether tools are used or not.\n\n        Args:\n            response: The raw response from API.\n            tools: The list of tools provided in the request.\n\n        Returns:\n            str or dict: The processed response.\n        \"\"\"\n        if tools:\n            processed_response = {\n                \"content\": response.choices[0].message.content,\n                \"tool_calls\": [],\n            }\n\n            if response.choices[0].message.tool_calls:\n                for tool_call in response.choices[0].message.tool_calls:\n                    processed_response[\"tool_calls\"].append(\n                        {\n                            \"name\": tool_call.function.name,\n                            \"arguments\": json.loads(extract_json(tool_call.function.arguments)),\n                        }\n                    )\n\n            return processed_response\n        else:\n            return response.choices[0].message.content\n\n    def generate_response(\n        self,\n        messages: List[Dict[str, str]],\n        response_format=None,\n        tools: Optional[List[Dict]] = None,\n        tool_choice: str = \"auto\",\n        **kwargs,\n    ):\n        \"\"\"\n        Generate a JSON response based on the given messages using OpenAI.\n\n        Args:\n            messages (list): List of message dicts containing 'role' and 'content'.\n            response_format (str or object, optional): Format of the response. Defaults to \"text\".\n            tools (list, optional): List of tools that the model can call. Defaults to None.\n            tool_choice (str, optional): Tool choice method. Defaults to \"auto\".\n            **kwargs: Additional OpenAI-specific parameters.\n\n        Returns:\n            json: The generated response.\n        \"\"\"\n        params = self._get_supported_params(messages=messages, **kwargs)\n        \n        params.update({\n            \"model\": self.config.model,\n            \"messages\": messages,\n        })\n\n        if os.getenv(\"OPENROUTER_API_KEY\"):\n            openrouter_params = {}\n            if self.config.models:\n                openrouter_params[\"models\"] = self.config.models\n                openrouter_params[\"route\"] = self.config.route\n                params.pop(\"model\")\n\n            if self.config.site_url and self.config.app_name:\n                extra_headers = {\n                    \"HTTP-Referer\": self.config.site_url,\n                    \"X-Title\": self.config.app_name,\n                }\n                openrouter_params[\"extra_headers\"] = extra_headers\n\n            params.update(**openrouter_params)\n        \n        else:\n            openai_specific_generation_params = [\"store\"]\n            for param in openai_specific_generation_params:\n                if hasattr(self.config, param):\n                    params[param] = getattr(self.config, param)\n            \n        if response_format:\n            params[\"response_format\"] = response_format\n        if tools:  # TODO: Remove tools if no issues found with new memory addition logic\n            params[\"tools\"] = tools\n            params[\"tool_choice\"] = tool_choice\n        response = self.client.chat.completions.create(**params)\n        parsed_response = self._parse_response(response, tools)\n        if self.config.response_callback:\n            try:\n                self.config.response_callback(self, response, params)\n            except Exception as e:\n                # Log error but don't propagate\n                logging.error(f\"Error due to callback: {e}\")\n                pass\n        return parsed_response\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_23","type":"command_execution","command":"/bin/zsh -lc 'git status --short'","aggregated_output":"","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_24","type":"reasoning","text":"**Inspecting base config class**"}}
{"type":"item.started","item":{"id":"item_25","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' mem0/configs/llms/base.py\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_26","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' tests/llms/test_openai.py\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_27","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' tests/llms/test_litellm.py\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_28","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' mem0/configs/base.py\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_25","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' mem0/configs/llms/base.py\"","aggregated_output":"from abc import ABC\nfrom typing import Dict, Optional, Union\n\nimport httpx\n\n\nclass BaseLlmConfig(ABC):\n    \"\"\"\n    Base configuration for LLMs with only common parameters.\n    Provider-specific configurations should be handled by separate config classes.\n\n    This class contains only the parameters that are common across all LLM providers.\n    For provider-specific parameters, use the appropriate provider config class.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Optional[Union[str, Dict]] = None,\n        temperature: float = 0.1,\n        api_key: Optional[str] = None,\n        max_tokens: int = 2000,\n        top_p: float = 0.1,\n        top_k: int = 1,\n        enable_vision: bool = False,\n        vision_details: Optional[str] = \"auto\",\n        http_client_proxies: Optional[Union[Dict, str]] = None,\n    ):\n        \"\"\"\n        Initialize a base configuration class instance for the LLM.\n\n        Args:\n            model: The model identifier to use (e.g., \"gpt-4.1-nano-2025-04-14\", \"claude-3-5-sonnet-20240620\")\n                Defaults to None (will be set by provider-specific configs)\n            temperature: Controls the randomness of the model's output.\n                Higher values (closer to 1) make output more random, lower values make it more deterministic.\n                Range: 0.0 to 2.0. Defaults to 0.1\n            api_key: API key for the LLM provider. If None, will try to get from environment variables.\n                Defaults to None\n            max_tokens: Maximum number of tokens to generate in the response.\n                Range: 1 to 4096 (varies by model). Defaults to 2000\n            top_p: Nucleus sampling parameter. Controls diversity via nucleus sampling.\n                Higher values (closer to 1) make word selection more diverse.\n                Range: 0.0 to 1.0. Defaults to 0.1\n            top_k: Top-k sampling parameter. Limits the number of tokens considered for each step.\n                Higher values make word selection more diverse.\n                Range: 1 to 40. Defaults to 1\n            enable_vision: Whether to enable vision capabilities for the model.\n                Only applicable to vision-enabled models. Defaults to False\n            vision_details: Level of detail for vision processing.\n                Options: \"low\", \"high\", \"auto\". Defaults to \"auto\"\n            http_client_proxies: Proxy settings for HTTP client.\n                Can be a dict or string. Defaults to None\n        \"\"\"\n        self.model = model\n        self.temperature = temperature\n        self.api_key = api_key\n        self.max_tokens = max_tokens\n        self.top_p = top_p\n        self.top_k = top_k\n        self.enable_vision = enable_vision\n        self.vision_details = vision_details\n        self.http_client = httpx.Client(proxies=http_client_proxies) if http_client_proxies else None\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_26","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' tests/llms/test_openai.py\"","aggregated_output":"import os\nfrom unittest.mock import Mock, patch\n\nimport pytest\n\nfrom mem0.configs.llms.openai import OpenAIConfig\nfrom mem0.llms.openai import OpenAILLM\n\n\n@pytest.fixture\ndef mock_openai_client():\n    with patch(\"mem0.llms.openai.OpenAI\") as mock_openai:\n        mock_client = Mock()\n        mock_openai.return_value = mock_client\n        yield mock_client\n\n\ndef test_openai_llm_base_url():\n    # case1: default config: with openai official base url\n    config = OpenAIConfig(model=\"gpt-4.1-nano-2025-04-14\", temperature=0.7, max_tokens=100, top_p=1.0, api_key=\"api_key\")\n    llm = OpenAILLM(config)\n    # Note: openai client will parse the raw base_url into a URL object, which will have a trailing slash\n    assert str(llm.client.base_url) == \"https://api.openai.com/v1/\"\n\n    # case2: with env variable OPENAI_API_BASE\n    provider_base_url = \"https://api.provider.com/v1\"\n    os.environ[\"OPENAI_BASE_URL\"] = provider_base_url\n    config = OpenAIConfig(model=\"gpt-4.1-nano-2025-04-14\", temperature=0.7, max_tokens=100, top_p=1.0, api_key=\"api_key\")\n    llm = OpenAILLM(config)\n    # Note: openai client will parse the raw base_url into a URL object, which will have a trailing slash\n    assert str(llm.client.base_url) == provider_base_url + \"/\"\n\n    # case3: with config.openai_base_url\n    config_base_url = \"https://api.config.com/v1\"\n    config = OpenAIConfig(\n        model=\"gpt-4.1-nano-2025-04-14\", temperature=0.7, max_tokens=100, top_p=1.0, api_key=\"api_key\", openai_base_url=config_base_url\n    )\n    llm = OpenAILLM(config)\n    # Note: openai client will parse the raw base_url into a URL object, which will have a trailing slash\n    assert str(llm.client.base_url) == config_base_url + \"/\"\n\n\ndef test_generate_response_without_tools(mock_openai_client):\n    config = OpenAIConfig(model=\"gpt-4.1-nano-2025-04-14\", temperature=0.7, max_tokens=100, top_p=1.0)\n    llm = OpenAILLM(config)\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n    ]\n\n    mock_response = Mock()\n    mock_response.choices = [Mock(message=Mock(content=\"I'm doing well, thank you for asking!\"))]\n    mock_openai_client.chat.completions.create.return_value = mock_response\n\n    response = llm.generate_response(messages)\n\n    mock_openai_client.chat.completions.create.assert_called_once_with(\n        model=\"gpt-4.1-nano-2025-04-14\", messages=messages, temperature=0.7, max_tokens=100, top_p=1.0, store=False\n    )\n    assert response == \"I'm doing well, thank you for asking!\"\n\n\ndef test_generate_response_with_tools(mock_openai_client):\n    config = OpenAIConfig(model=\"gpt-4.1-nano-2025-04-14\", temperature=0.7, max_tokens=100, top_p=1.0)\n    llm = OpenAILLM(config)\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Add a new memory: Today is a sunny day.\"},\n    ]\n    tools = [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"add_memory\",\n                \"description\": \"Add a memory\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\"data\": {\"type\": \"string\", \"description\": \"Data to add to memory\"}},\n                    \"required\": [\"data\"],\n                },\n            },\n        }\n    ]\n\n    mock_response = Mock()\n    mock_message = Mock()\n    mock_message.content = \"I've added the memory for you.\"\n\n    mock_tool_call = Mock()\n    mock_tool_call.function.name = \"add_memory\"\n    mock_tool_call.function.arguments = '{\"data\": \"Today is a sunny day.\"}'\n\n    mock_message.tool_calls = [mock_tool_call]\n    mock_response.choices = [Mock(message=mock_message)]\n    mock_openai_client.chat.completions.create.return_value = mock_response\n\n    response = llm.generate_response(messages, tools=tools)\n\n    mock_openai_client.chat.completions.create.assert_called_once_with(\n        model=\"gpt-4.1-nano-2025-04-14\", messages=messages, temperature=0.7, max_tokens=100, top_p=1.0, tools=tools, tool_choice=\"auto\", store=False\n    )\n\n    assert response[\"content\"] == \"I've added the memory for you.\"\n    assert len(response[\"tool_calls\"]) == 1\n    assert response[\"tool_calls\"][0][\"name\"] == \"add_memory\"\n    assert response[\"tool_calls\"][0][\"arguments\"] == {\"data\": \"Today is a sunny day.\"}\n\n\ndef test_response_callback_invocation(mock_openai_client):\n    # Setup mock callback\n    mock_callback = Mock()\n    \n    config = OpenAIConfig(model=\"gpt-4.1-nano-2025-04-14\", response_callback=mock_callback)\n    llm = OpenAILLM(config)\n    messages = [{\"role\": \"user\", \"content\": \"Test callback\"}]\n    \n    # Mock response\n    mock_response = Mock()\n    mock_response.choices = [Mock(message=Mock(content=\"Response\"))]\n    mock_openai_client.chat.completions.create.return_value = mock_response\n    \n    # Call method\n    llm.generate_response(messages)\n    \n    # Verify callback called with correct arguments\n    mock_callback.assert_called_once()\n    args = mock_callback.call_args[0]\n    assert args[0] is llm  # llm_instance\n    assert args[1] == mock_response  # raw_response\n    assert \"messages\" in args[2]  # params\n\n\ndef test_no_response_callback(mock_openai_client):\n    config = OpenAIConfig(model=\"gpt-4.1-nano-2025-04-14\")\n    llm = OpenAILLM(config)\n    messages = [{\"role\": \"user\", \"content\": \"Test no callback\"}]\n    \n    # Mock response\n    mock_response = Mock()\n    mock_response.choices = [Mock(message=Mock(content=\"Response\"))]\n    mock_openai_client.chat.completions.create.return_value = mock_response\n    \n    # Should complete without calling any callback\n    response = llm.generate_response(messages)\n    assert response == \"Response\"\n    \n    # Verify no callback is set\n    assert llm.config.response_callback is None\n\n\ndef test_callback_exception_handling(mock_openai_client):\n    # Callback that raises exception\n    def faulty_callback(*args):\n        raise ValueError(\"Callback error\")\n    \n    config = OpenAIConfig(model=\"gpt-4.1-nano-2025-04-14\", response_callback=faulty_callback)\n    llm = OpenAILLM(config)\n    messages = [{\"role\": \"user\", \"content\": \"Test exception\"}]\n    \n    # Mock response\n    mock_response = Mock()\n    mock_response.choices = [Mock(message=Mock(content=\"Expected response\"))]\n    mock_openai_client.chat.completions.create.return_value = mock_response\n    \n    # Should complete without raising\n    response = llm.generate_response(messages)\n    assert response == \"Expected response\"\n    \n    # Verify callback was called (even though it raised an exception)\n    assert llm.config.response_callback is faulty_callback\n\n\ndef test_callback_with_tools(mock_openai_client):\n    mock_callback = Mock()\n    config = OpenAIConfig(model=\"gpt-4.1-nano-2025-04-14\", response_callback=mock_callback)\n    llm = OpenAILLM(config)\n    messages = [{\"role\": \"user\", \"content\": \"Test tools\"}]\n    tools = [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"test_tool\",\n                \"description\": \"A test tool\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\"param1\": {\"type\": \"string\"}},\n                    \"required\": [\"param1\"],\n                },\n            }\n        }\n    ]\n    \n    # Mock tool response\n    mock_response = Mock()\n    mock_message = Mock()\n    mock_message.content = \"Tool response\"\n    mock_tool_call = Mock()\n    mock_tool_call.function.name = \"test_tool\"\n    mock_tool_call.function.arguments = '{\"param1\": \"value1\"}'\n    mock_message.tool_calls = [mock_tool_call]\n    mock_response.choices = [Mock(message=mock_message)]\n    mock_openai_client.chat.completions.create.return_value = mock_response\n    \n    llm.generate_response(messages, tools=tools)\n    \n    # Verify callback called with tool response\n    mock_callback.assert_called_once()\n    # Check that tool_calls exists in the message\n    assert hasattr(mock_callback.call_args[0][1].choices[0].message, 'tool_calls')\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_28","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' mem0/configs/base.py\"","aggregated_output":"import os\nfrom typing import Any, Dict, Optional\n\nfrom pydantic import BaseModel, Field\n\nfrom mem0.embeddings.configs import EmbedderConfig\nfrom mem0.graphs.configs import GraphStoreConfig\nfrom mem0.llms.configs import LlmConfig\nfrom mem0.vector_stores.configs import VectorStoreConfig\nfrom mem0.configs.rerankers.config import RerankerConfig\n\n# Set up the directory path\nhome_dir = os.path.expanduser(\"~\")\nmem0_dir = os.environ.get(\"MEM0_DIR\") or os.path.join(home_dir, \".mem0\")\n\n\nclass MemoryItem(BaseModel):\n    id: str = Field(..., description=\"The unique identifier for the text data\")\n    memory: str = Field(\n        ..., description=\"The memory deduced from the text data\"\n    )  # TODO After prompt changes from platform, update this\n    hash: Optional[str] = Field(None, description=\"The hash of the memory\")\n    # The metadata value can be anything and not just string. Fix it\n    metadata: Optional[Dict[str, Any]] = Field(None, description=\"Additional metadata for the text data\")\n    score: Optional[float] = Field(None, description=\"The score associated with the text data\")\n    created_at: Optional[str] = Field(None, description=\"The timestamp when the memory was created\")\n    updated_at: Optional[str] = Field(None, description=\"The timestamp when the memory was updated\")\n\n\nclass MemoryConfig(BaseModel):\n    vector_store: VectorStoreConfig = Field(\n        description=\"Configuration for the vector store\",\n        default_factory=VectorStoreConfig,\n    )\n    llm: LlmConfig = Field(\n        description=\"Configuration for the language model\",\n        default_factory=LlmConfig,\n    )\n    embedder: EmbedderConfig = Field(\n        description=\"Configuration for the embedding model\",\n        default_factory=EmbedderConfig,\n    )\n    history_db_path: str = Field(\n        description=\"Path to the history database\",\n        default=os.path.join(mem0_dir, \"history.db\"),\n    )\n    graph_store: GraphStoreConfig = Field(\n        description=\"Configuration for the graph\",\n        default_factory=GraphStoreConfig,\n    )\n    reranker: Optional[RerankerConfig] = Field(\n        description=\"Configuration for the reranker\",\n        default=None,\n    )\n    version: str = Field(\n        description=\"The version of the API\",\n        default=\"v1.1\",\n    )\n    custom_fact_extraction_prompt: Optional[str] = Field(\n        description=\"Custom prompt for the fact extraction\",\n        default=None,\n    )\n    custom_update_memory_prompt: Optional[str] = Field(\n        description=\"Custom prompt for the update memory\",\n        default=None,\n    )\n\n\nclass AzureConfig(BaseModel):\n    \"\"\"\n    Configuration settings for Azure.\n\n    Args:\n        api_key (str): The API key used for authenticating with the Azure service.\n        azure_deployment (str): The name of the Azure deployment.\n        azure_endpoint (str): The endpoint URL for the Azure service.\n        api_version (str): The version of the Azure API being used.\n        default_headers (Dict[str, str]): Headers to include in requests to the Azure API.\n    \"\"\"\n\n    api_key: str = Field(\n        description=\"The API key used for authenticating with the Azure service.\",\n        default=None,\n    )\n    azure_deployment: str = Field(description=\"The name of the Azure deployment.\", default=None)\n    azure_endpoint: str = Field(description=\"The endpoint URL for the Azure service.\", default=None)\n    api_version: str = Field(description=\"The version of the Azure API being used.\", default=None)\n    default_headers: Optional[Dict[str, str]] = Field(\n        description=\"Headers to include in requests to the Azure API.\", default=None\n    )\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_27","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' tests/llms/test_litellm.py\"","aggregated_output":"from unittest.mock import Mock, patch\n\nimport pytest\n\nfrom mem0.configs.llms.base import BaseLlmConfig\nfrom mem0.llms import litellm\n\n\n@pytest.fixture\ndef mock_litellm():\n    with patch(\"mem0.llms.litellm.litellm\") as mock_litellm:\n        yield mock_litellm\n\n\ndef test_generate_response_with_unsupported_model(mock_litellm):\n    config = BaseLlmConfig(model=\"unsupported-model\", temperature=0.7, max_tokens=100, top_p=1)\n    llm = litellm.LiteLLM(config)\n    messages = [{\"role\": \"user\", \"content\": \"Hello\"}]\n\n    mock_litellm.supports_function_calling.return_value = False\n\n    with pytest.raises(ValueError, match=\"Model 'unsupported-model' in litellm does not support function calling.\"):\n        llm.generate_response(messages)\n\n\ndef test_generate_response_without_tools(mock_litellm):\n    config = BaseLlmConfig(model=\"gpt-4.1-nano-2025-04-14\", temperature=0.7, max_tokens=100, top_p=1)\n    llm = litellm.LiteLLM(config)\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n    ]\n\n    mock_response = Mock()\n    mock_response.choices = [Mock(message=Mock(content=\"I'm doing well, thank you for asking!\"))]\n    mock_litellm.completion.return_value = mock_response\n    mock_litellm.supports_function_calling.return_value = True\n\n    response = llm.generate_response(messages)\n\n    mock_litellm.completion.assert_called_once_with(\n        model=\"gpt-4.1-nano-2025-04-14\", messages=messages, temperature=0.7, max_tokens=100, top_p=1.0\n    )\n    assert response == \"I'm doing well, thank you for asking!\"\n\n\ndef test_generate_response_with_tools(mock_litellm):\n    config = BaseLlmConfig(model=\"gpt-4.1-nano-2025-04-14\", temperature=0.7, max_tokens=100, top_p=1)\n    llm = litellm.LiteLLM(config)\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Add a new memory: Today is a sunny day.\"},\n    ]\n    tools = [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"add_memory\",\n                \"description\": \"Add a memory\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\"data\": {\"type\": \"string\", \"description\": \"Data to add to memory\"}},\n                    \"required\": [\"data\"],\n                },\n            },\n        }\n    ]\n\n    mock_response = Mock()\n    mock_message = Mock()\n    mock_message.content = \"I've added the memory for you.\"\n\n    mock_tool_call = Mock()\n    mock_tool_call.function.name = \"add_memory\"\n    mock_tool_call.function.arguments = '{\"data\": \"Today is a sunny day.\"}'\n\n    mock_message.tool_calls = [mock_tool_call]\n    mock_response.choices = [Mock(message=mock_message)]\n    mock_litellm.completion.return_value = mock_response\n    mock_litellm.supports_function_calling.return_value = True\n\n    response = llm.generate_response(messages, tools=tools)\n\n    mock_litellm.completion.assert_called_once_with(\n        model=\"gpt-4.1-nano-2025-04-14\", messages=messages, temperature=0.7, max_tokens=100, top_p=1, tools=tools, tool_choice=\"auto\"\n    )\n\n    assert response[\"content\"] == \"I've added the memory for you.\"\n    assert len(response[\"tool_calls\"]) == 1\n    assert response[\"tool_calls\"][0][\"name\"] == \"add_memory\"\n    assert response[\"tool_calls\"][0][\"arguments\"] == {\"data\": \"Today is a sunny day.\"}\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_29","type":"reasoning","text":"**Searching test coverage for providers**"}}
{"type":"item.started","item":{"id":"item_30","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' mem0/llms/base.py\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_31","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' Makefile\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_32","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' pyproject.toml\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_33","type":"command_execution","command":"/bin/zsh -lc \"rg -n \\\"Unsupported LLM provider|get_supported_providers|openai_base_url|OPENAI_BASE_URL|OPENAI_API_BASE|provider\\\\s*:\\\\s*\\\\\\\"openai\\\\\\\"|llm.provider\\\" tests mem0 docs README.md LLM.md -g '*.py' -g '*.md' -g '*.mdx' | head -n 300\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_31","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' Makefile\"","aggregated_output":".PHONY: format sort lint\n\n# Variables\nISORT_OPTIONS = --profile black\nPROJECT_NAME := mem0ai\n\n# Default target\nall: format sort lint\n\ninstall:\n\thatch env create\n\ninstall_all:\n\tpip install ruff==0.6.9 groq together boto3 litellm ollama chromadb weaviate weaviate-client sentence_transformers vertexai \\\n\t            google-generativeai elasticsearch opensearch-py vecs \"pinecone<7.0.0\" pinecone-text faiss-cpu langchain-community \\\n\t\t\t\t\t\t\tupstash-vector azure-search-documents langchain-memgraph langchain-neo4j langchain-aws rank-bm25 pymochow pymongo psycopg kuzu databricks-sdk valkey\n\n# Format code with ruff\nformat:\n\thatch run format\n\n# Sort imports with isort\nsort:\n\thatch run isort mem0/\n\n# Lint code with ruff\nlint:\n\thatch run lint\n\ndocs:\n\tcd docs && mintlify dev\n\nbuild:\n\thatch build\n\npublish:\n\thatch publish\n\nclean:\n\trm -rf dist\n\ntest:\n\thatch run test\n\ntest-py-3.9:\n\thatch run dev_py_3_9:test\n\ntest-py-3.10:\n\thatch run dev_py_3_10:test\n\ntest-py-3.11:\n\thatch run dev_py_3_11:test\n\ntest-py-3.12:\n\thatch run dev_py_3_12:test\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_32","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' pyproject.toml\"","aggregated_output":"[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[project]\nname = \"mem0ai\"\nversion = \"1.0.4\"\ndescription = \"Long-term memory for AI Agents\"\nauthors = [\n    { name = \"Mem0\", email = \"founders@mem0.ai\" }\n]\nreadme = \"README.md\"\nlicense = \"Apache-2.0\"\nlicense-files = [\"LICENSE\"]\nrequires-python = \">=3.9,<4.0\"\ndependencies = [\n    \"qdrant-client>=1.9.1\",\n    \"pydantic>=2.7.3\",\n    \"openai>=1.90.0\",\n    \"posthog>=3.5.0\",\n    \"pytz>=2024.1\",\n    \"sqlalchemy>=2.0.31\",\n    \"protobuf>=5.29.0,<6.0.0\",\n]\n\n[project.optional-dependencies]\ngraph = [\n    \"langchain-neo4j>=0.4.0\",\n    \"langchain-aws>=0.2.23\",\n    \"langchain-memgraph>=0.1.0\",\n    \"neo4j>=5.23.1\",\n    \"rank-bm25>=0.2.2\",\n    \"kuzu>=0.11.0\",\n]\nvector_stores = [\n    \"vecs>=0.4.0\",\n    \"chromadb>=0.4.24\",\n    \"cassandra-driver>=3.29.0\",\n    \"weaviate-client>=4.4.0,<4.15.0\",\n    \"pinecone<=7.3.0\",\n    \"pinecone-text>=0.10.0\",\n    \"faiss-cpu>=1.7.4\",\n    \"upstash-vector>=0.1.0\",\n    \"azure-search-documents>=11.4.0b8\",\n    \"psycopg>=3.2.8\",\n    \"psycopg-pool>=3.2.6,<4.0.0\",\n    \"pymongo>=4.13.2\",\n    \"pymochow>=2.2.9\",\n    \"pymysql>=1.1.0\",\n    \"dbutils>=3.0.3\",\n    \"valkey>=6.0.0\",\n    \"databricks-sdk>=0.63.0\",\n    \"azure-identity>=1.24.0\",\n    \"redis>=5.0.0,<6.0.0\",\n    \"redisvl>=0.1.0,<1.0.0\",\n    \"elasticsearch>=8.0.0,<9.0.0\",\n    \"pymilvus>=2.4.0,<2.6.0\",\n    \"langchain-aws>=0.2.23\",\n]\nllms = [\n    \"groq>=0.3.0\",\n    \"together>=0.2.10\",\n    \"litellm>=1.74.0\",\n    \"openai>=1.90.0\",\n    \"ollama>=0.1.0\",\n    \"vertexai>=0.1.0\",\n    \"google-generativeai>=0.3.0\",\n    \"google-genai>=1.0.0\",\n]\nextras = [\n    \"boto3>=1.34.0\",\n    \"langchain-community>=0.0.0\",\n    \"sentence-transformers>=5.0.0\",\n    \"elasticsearch>=8.0.0,<9.0.0\",\n    \"opensearch-py>=2.0.0\",\n    \"fastembed>=0.3.1\",\n]\ntest = [\n    \"pytest>=8.2.2\",\n    \"pytest-mock>=3.14.0\",\n    \"pytest-asyncio>=0.23.7\",\n]\ndev = [\n    \"ruff>=0.6.5\",\n    \"isort>=5.13.2\",\n    \"pytest>=8.2.2\",\n]\n\n[tool.pytest.ini_options]\npythonpath = [\".\"]\n\n[tool.hatch.build]\ninclude = [\n    \"mem0/**/*.py\",\n]\nexclude = [\n    \"**/*\",\n    \"!mem0/**/*.py\",\n]\n\n[tool.hatch.build.targets.wheel]\npackages = [\"mem0\"]\nonly-include = [\"mem0\"]\n\n[tool.hatch.build.targets.wheel.shared-data]\n\"README.md\" = \"README.md\"\n\n[tool.hatch.envs.dev_py_3_9]\npython = \"3.9\"\nfeatures = [\n  \"test\",\n  \"graph\",\n  \"vector_stores\",\n  \"llms\",\n  \"extras\",\n]\n\n[tool.hatch.envs.dev_py_3_10]\npython = \"3.10\"\nfeatures = [\n  \"test\",\n  \"graph\",\n  \"vector_stores\",\n  \"llms\",\n  \"extras\",\n]\n\n[tool.hatch.envs.dev_py_3_11]\npython = \"3.11\"\nfeatures = [\n  \"test\",\n  \"graph\",\n  \"vector_stores\",\n  \"llms\",\n  \"extras\",\n]\n\n[tool.hatch.envs.dev_py_3_12]\npython = \"3.12\"\nfeatures = [\n  \"test\",\n  \"graph\",\n  \"vector_stores\",\n  \"llms\",\n  \"extras\",\n]\n\n[tool.hatch.envs.default.scripts]\nformat = [\n    \"ruff format\",\n]\nformat-check = [\n    \"ruff format --check\",\n]\nlint = [\n    \"ruff check\",\n]\nlint-fix = [\n    \"ruff check --fix\",\n]\ntest = [\n    \"pytest tests/ {args}\",\n]\n\n[tool.ruff]\nline-length = 120\nexclude = [\"embedchain/\", \"openmemory/\"]\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_30","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' mem0/llms/base.py\"","aggregated_output":"from abc import ABC, abstractmethod\nfrom typing import Dict, List, Optional, Union\n\nfrom mem0.configs.llms.base import BaseLlmConfig\n\n\nclass LLMBase(ABC):\n    \"\"\"\n    Base class for all LLM providers.\n    Handles common functionality and delegates provider-specific logic to subclasses.\n    \"\"\"\n\n    def __init__(self, config: Optional[Union[BaseLlmConfig, Dict]] = None):\n        \"\"\"Initialize a base LLM class\n\n        :param config: LLM configuration option class or dict, defaults to None\n        :type config: Optional[Union[BaseLlmConfig, Dict]], optional\n        \"\"\"\n        if config is None:\n            self.config = BaseLlmConfig()\n        elif isinstance(config, dict):\n            # Handle dict-based configuration (backward compatibility)\n            self.config = BaseLlmConfig(**config)\n        else:\n            self.config = config\n\n        # Validate configuration\n        self._validate_config()\n\n    def _validate_config(self):\n        \"\"\"\n        Validate the configuration.\n        Override in subclasses to add provider-specific validation.\n        \"\"\"\n        if not hasattr(self.config, \"model\"):\n            raise ValueError(\"Configuration must have a 'model' attribute\")\n\n        if not hasattr(self.config, \"api_key\") and not hasattr(self.config, \"api_key\"):\n            # Check if API key is available via environment variable\n            # This will be handled by individual providers\n            pass\n\n    def _is_reasoning_model(self, model: str) -> bool:\n        \"\"\"\n        Check if the model is a reasoning model or GPT-5 series that doesn't support certain parameters.\n        \n        Args:\n            model: The model name to check\n            \n        Returns:\n            bool: True if the model is a reasoning model or GPT-5 series\n        \"\"\"\n        reasoning_models = {\n            \"o1\", \"o1-preview\", \"o3-mini\", \"o3\",\n            \"gpt-5\", \"gpt-5o\", \"gpt-5o-mini\", \"gpt-5o-micro\",\n        }\n        \n        if model.lower() in reasoning_models:\n            return True\n        \n        model_lower = model.lower()\n        if any(reasoning_model in model_lower for reasoning_model in [\"gpt-5\", \"o1\", \"o3\"]):\n            return True\n            \n        return False\n\n    def _get_supported_params(self, **kwargs) -> Dict:\n        \"\"\"\n        Get parameters that are supported by the current model.\n        Filters out unsupported parameters for reasoning models and GPT-5 series.\n        \n        Args:\n            **kwargs: Additional parameters to include\n            \n        Returns:\n            Dict: Filtered parameters dictionary\n        \"\"\"\n        model = getattr(self.config, 'model', '')\n        \n        if self._is_reasoning_model(model):\n            supported_params = {}\n            \n            if \"messages\" in kwargs:\n                supported_params[\"messages\"] = kwargs[\"messages\"]\n            if \"response_format\" in kwargs:\n                supported_params[\"response_format\"] = kwargs[\"response_format\"]\n            if \"tools\" in kwargs:\n                supported_params[\"tools\"] = kwargs[\"tools\"]\n            if \"tool_choice\" in kwargs:\n                supported_params[\"tool_choice\"] = kwargs[\"tool_choice\"]\n                \n            return supported_params\n        else:\n            # For regular models, include all common parameters\n            return self._get_common_params(**kwargs)\n\n    @abstractmethod\n    def generate_response(\n        self, messages: List[Dict[str, str]], tools: Optional[List[Dict]] = None, tool_choice: str = \"auto\", **kwargs\n    ):\n        \"\"\"\n        Generate a response based on the given messages.\n\n        Args:\n            messages (list): List of message dicts containing 'role' and 'content'.\n            tools (list, optional): List of tools that the model can call. Defaults to None.\n            tool_choice (str, optional): Tool choice method. Defaults to \"auto\".\n            **kwargs: Additional provider-specific parameters.\n\n        Returns:\n            str or dict: The generated response.\n        \"\"\"\n        pass\n\n    def _get_common_params(self, **kwargs) -> Dict:\n        \"\"\"\n        Get common parameters that most providers use.\n\n        Returns:\n            Dict: Common parameters dictionary.\n        \"\"\"\n        params = {\n            \"temperature\": self.config.temperature,\n            \"max_tokens\": self.config.max_tokens,\n            \"top_p\": self.config.top_p,\n        }\n\n        # Add provider-specific parameters from kwargs\n        params.update(kwargs)\n\n        return params\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_33","type":"command_execution","command":"/bin/zsh -lc \"rg -n \\\"Unsupported LLM provider|get_supported_providers|openai_base_url|OPENAI_BASE_URL|OPENAI_API_BASE|provider\\\\s*:\\\\s*\\\\\\\"openai\\\\\\\"|llm.provider\\\" tests mem0 docs README.md LLM.md -g '*.py' -g '*.md' -g '*.mdx' | head -n 300\"","aggregated_output":"docs/integrations/vercel-ai-sdk.mdx:39:      provider: \"openai\",\ndocs/integrations/keywords.mdx:61:            \"openai_base_url\": base_url,\ndocs/integrations/openclaw.mdx:148:| `oss.llm.provider` | `string` | `\"openai\"` | LLM provider (`\"openai\"`, `\"anthropic\"`, `\"ollama\"`, etc.) |\nmem0/llms/openai_structured.py:18:        base_url = self.config.openai_base_url or os.getenv(\"OPENAI_API_BASE\") or \"https://api.openai.com/v1\"\ndocs/open-source/features/custom-fact-extraction-prompt.mdx:126:    provider: \"openai\",\ntests/llms/test_openai.py:25:    # case2: with env variable OPENAI_API_BASE\ntests/llms/test_openai.py:27:    os.environ[\"OPENAI_BASE_URL\"] = provider_base_url\ntests/llms/test_openai.py:33:    # case3: with config.openai_base_url\ntests/llms/test_openai.py:36:        model=\"gpt-4.1-nano-2025-04-14\", temperature=0.7, max_tokens=100, top_p=1.0, api_key=\"api_key\", openai_base_url=config_base_url\nmem0/llms/openai.py:49:            base_url = self.config.openai_base_url or os.getenv(\"OPENAI_BASE_URL\") or \"https://api.openai.com/v1\"\ndocs/open-source/node-quickstart.mdx:81:    provider: \"openai\",\ndocs/open-source/node-quickstart.mdx:95:    provider: \"openai\",\ndocs/open-source/node-quickstart.mdx:247:    provider: \"openai\",\ndocs/open-source/node-quickstart.mdx:261:    provider: \"openai\",\nmem0/configs/embeddings/base.py:23:        openai_base_url: Optional[str] = None,\nmem0/configs/embeddings/base.py:59:        :param openai_base_url: Openai base URL to be use, defaults to \"https://api.openai.com/v1\"\nmem0/configs/embeddings/base.py:60:        :type openai_base_url: Optional[str], optional\nmem0/configs/embeddings/base.py:79:        self.openai_base_url = openai_base_url\nmem0/memory/graph_memory.py:59:        self.llm_provider = \"openai\"\nmem0/memory/graph_memory.py:60:        if self.config.llm and self.config.llm.provider:\nmem0/memory/graph_memory.py:61:            self.llm_provider = self.config.llm.provider\nmem0/memory/graph_memory.py:62:        if self.config.graph_store and self.config.graph_store.llm and self.config.graph_store.llm.provider:\nmem0/memory/graph_memory.py:63:            self.llm_provider = self.config.graph_store.llm.provider\nmem0/memory/graph_memory.py:71:        self.llm = LlmFactory.create(self.llm_provider, llm_config)\nmem0/memory/graph_memory.py:199:        if self.llm_provider in [\"azure_openai_structured\", \"openai_structured\"]:\nmem0/memory/graph_memory.py:222:                f\"Error in search tool: {e}, llm_provider={self.llm_provider}, search_results={search_results}\"\nmem0/memory/graph_memory.py:255:        if self.llm_provider in [\"azure_openai_structured\", \"openai_structured\"]:\nmem0/memory/graph_memory.py:336:        if self.llm_provider in [\"azure_openai_structured\", \"openai_structured\"]:\ndocs/components/llms/config.mdx:10:    - `llm`: Specifies the llm provider and its configuration\ndocs/components/llms/config.mdx:30:2. Environment variables (e.g., `OPENAI_API_KEY`, `OPENAI_BASE_URL`)\ndocs/components/llms/config.mdx:107:    | `openai_base_url`    | Base URL for OpenAI API                       | OpenAI            |\ntests/memory/test_neptune_analytics_memory.py:19:        self.config.llm.provider = \"openai_structured\"\ntests/memory/test_neptune_analytics_memory.py:63:        self.assertEqual(self.memory_graph.llm_provider, \"openai_structured\")\nmem0/llms/configs.py:34:            raise ValueError(f\"Unsupported LLM provider: {provider}\")\nmem0/configs/llms/openai.py:25:        openai_base_url: Optional[str] = None,\nmem0/configs/llms/openai.py:48:            openai_base_url: OpenAI API base URL, defaults to None\nmem0/configs/llms/openai.py:70:        self.openai_base_url = openai_base_url\nmem0/memory/main.py:186:        self.llm = LlmFactory.create(self.config.llm.provider, self.config.llm.config)\nmem0/memory/main.py:1253:        self.llm = LlmFactory.create(self.config.llm.provider, self.config.llm.config)\ntests/memory/test_neptune_memory.py:19:        self.config.llm.provider = \"openai_structured\"\ntests/memory/test_neptune_memory.py:71:        self.assertEqual(self.memory_graph.llm_provider, \"openai_structured\")\ntests/memory/test_neptune_memory.py:84:        config1.llm.provider = \"openai\"\ntests/memory/test_neptune_memory.py:97:        config2.llm.provider = \"openai\"\ntests/memory/test_neptune_memory.py:111:        config3.llm.provider = \"openai\"\ntests/memory/test_kuzu.py:46:        config.llm.provider = \"mock_llm\"\nmem0/graphs/neptune/base.py:43:    def _create_llm(config, llm_provider):\nmem0/graphs/neptune/base.py:47:        return LlmFactory.create(llm_provider, config.llm.config)\nmem0/graphs/neptune/base.py:81:        if self.llm_provider in [\"azure_openai_structured\", \"openai_structured\"]:\nmem0/graphs/neptune/base.py:104:                f\"Error in search tool: {e}, llm_provider={self.llm_provider}, search_results={search_results}\"\nmem0/graphs/neptune/base.py:137:        if self.llm_provider in [\"azure_openai_structured\", \"openai_structured\"]:\nmem0/graphs/neptune/base.py:169:        if self.llm_provider in [\"azure_openai_structured\", \"openai_structured\"]:\nmem0/graphs/neptune/neptunegraph.py:34:        self.llm_provider = \"openai\"\nmem0/graphs/neptune/neptunegraph.py:35:        if self.config.llm.provider:\nmem0/graphs/neptune/neptunegraph.py:36:            self.llm_provider = self.config.llm.provider\nmem0/graphs/neptune/neptunegraph.py:38:            self.llm_provider = self.config.graph_store.llm.provider\nmem0/graphs/neptune/neptunegraph.py:40:        self.llm = NeptuneBase._create_llm(self.config, self.llm_provider)\nmem0/memory/kuzu_memory.py:51:        self.llm_provider = \"openai\"\nmem0/memory/kuzu_memory.py:52:        if self.config.llm and self.config.llm.provider:\nmem0/memory/kuzu_memory.py:53:            self.llm_provider = self.config.llm.provider\nmem0/memory/kuzu_memory.py:54:        if self.config.graph_store and self.config.graph_store.llm and self.config.graph_store.llm.provider:\nmem0/memory/kuzu_memory.py:55:            self.llm_provider = self.config.graph_store.llm.provider\nmem0/memory/kuzu_memory.py:62:        self.llm = LlmFactory.create(self.llm_provider, llm_config)\nmem0/memory/kuzu_memory.py:225:        if self.llm_provider in [\"azure_openai_structured\", \"openai_structured\"]:\nmem0/memory/kuzu_memory.py:248:                f\"Error in search tool: {e}, llm_provider={self.llm_provider}, search_results={search_results}\"\nmem0/memory/kuzu_memory.py:281:        if self.llm_provider in [\"azure_openai_structured\", \"openai_structured\"]:\nmem0/memory/kuzu_memory.py:363:        if self.llm_provider in [\"azure_openai_structured\", \"openai_structured\"]:\nmem0/graphs/neptune/neptunedb.py:38:        self.llm_provider = \"openai\"\nmem0/graphs/neptune/neptunedb.py:40:            self.llm_provider = self.config.graph_store.llm.provider\nmem0/graphs/neptune/neptunedb.py:41:        elif self.config.llm.provider:\nmem0/graphs/neptune/neptunedb.py:42:            self.llm_provider = self.config.llm.provider\nmem0/graphs/neptune/neptunedb.py:57:        self.llm = NeptuneBase._create_llm(self.config, self.llm_provider)\nmem0/memory/memgraph_memory.py:44:        self.llm_provider = \"openai\"\nmem0/memory/memgraph_memory.py:45:        if self.config.llm and self.config.llm.provider:\nmem0/memory/memgraph_memory.py:46:            self.llm_provider = self.config.llm.provider\nmem0/memory/memgraph_memory.py:47:        if self.config.graph_store and self.config.graph_store.llm and self.config.graph_store.llm.provider:\nmem0/memory/memgraph_memory.py:48:            self.llm_provider = self.config.graph_store.llm.provider\nmem0/memory/memgraph_memory.py:56:        self.llm = LlmFactory.create(self.llm_provider, llm_config)\nmem0/memory/memgraph_memory.py:202:        if self.llm_provider in [\"azure_openai_structured\", \"openai_structured\"]:\nmem0/memory/memgraph_memory.py:225:                f\"Error in search tool: {e}, llm_provider={self.llm_provider}, search_results={search_results}\"\nmem0/memory/memgraph_memory.py:257:        if self.llm_provider in [\"azure_openai_structured\", \"openai_structured\"]:\nmem0/memory/memgraph_memory.py:342:        if self.llm_provider in [\"azure_openai_structured\", \"openai_structured\"]:\nmem0/utils/factory.py:126:    def get_supported_providers(cls) -> list:\ndocs/components/embedders/config.mdx:81:| `openai_base_url`    | Base URL for OpenAI API                       | OpenAI            |\nmem0/embeddings/openai.py:20:            self.config.openai_base_url\nmem0/embeddings/openai.py:21:            or os.getenv(\"OPENAI_API_BASE\")\nmem0/embeddings/openai.py:22:            or os.getenv(\"OPENAI_BASE_URL\")\nmem0/embeddings/openai.py:25:        if os.environ.get(\"OPENAI_API_BASE\"):\nmem0/embeddings/openai.py:27:                \"The environment variable 'OPENAI_API_BASE' is deprecated and will be removed in the 0.1.80. \"\nmem0/embeddings/openai.py:28:                \"Please use 'OPENAI_BASE_URL' instead.\",\ndocs/components/rerankers/config.mdx:66:| `llm.provider` | LLM provider for reranking  | `str`  | Required |\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_34","type":"reasoning","text":"**Exploring provider integration and docs impact**"}}
{"type":"item.completed","item":{"id":"item_35","type":"agent_message","text":"I‚Äôm now checking whether Forge is already partially integrated or documented, and identifying the closest existing pattern (likely OpenRouter/OpenAI-compatible paths) to anchor a minimal-diff plan."}}
{"type":"item.started","item":{"id":"item_36","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' docs/components/llms/config.mdx\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_37","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,220p' mem0/configs/embeddings/base.py\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_38","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' README.md\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_39","type":"command_execution","command":"/bin/zsh -lc \"rg -n \\\"forge|FORGE|openrouter|OPENROUTER|OpenRouter|base_url|api_key|provider\\\" mem0 tests docs README.md LLM.md mem0-ts -g '*.py' -g '*.md' -g '*.mdx' -g '*.ts' -g '*.tsx' -g '*.json' | head -n 400\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_36","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' docs/components/llms/config.mdx\"","aggregated_output":"---\ntitle: Configurations\n---\n\n## How to define configurations?\n\n<Tabs>\n  <Tab title=\"Python\">\n    The `config` is defined as a Python dictionary with two main keys:\n    - `llm`: Specifies the llm provider and its configuration\n      - `provider`: The name of the llm (e.g., \"openai\", \"groq\")\n      - `config`: A nested dictionary containing provider-specific settings\n  </Tab>\n  <Tab title=\"TypeScript\">\n    The `config` is defined as a TypeScript object with these keys:\n    - `llm`: Specifies the LLM provider and its configuration (required)\n      - `provider`: The name of the LLM (e.g., \"openai\", \"groq\")\n      - `config`: A nested object containing provider-specific settings\n    - `embedder`: Specifies the embedder provider and its configuration (optional)\n    - `vectorStore`: Specifies the vector store provider and its configuration (optional)\n    - `historyDbPath`: Path to the history database file (optional)\n  </Tab>\n</Tabs>\n\n### Config Values Precedence\n\nConfig values are applied in the following order of precedence (from highest to lowest):\n\n1. Values explicitly set in the `config` object/dictionary\n2. Environment variables (e.g., `OPENAI_API_KEY`, `OPENAI_BASE_URL`)\n3. Default values defined in the LLM implementation\n\nThis means that values specified in the `config` will override corresponding environment variables, which in turn override default values.\n\n## How to Use Config\n\nHere's a general example of how to use the config with Mem0:\n\n<CodeGroup>\n```python Python\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-xx\" # for embedder\n\nconfig = {\n    \"llm\": {\n        \"provider\": \"your_chosen_provider\",\n        \"config\": {\n            # Provider-specific settings go here\n        }\n    }\n}\n\nm = Memory.from_config(config)\nm.add(\"Your text here\", user_id=\"user\", metadata={\"category\": \"example\"})\n\n```\n\n```typescript TypeScript\nimport { Memory } from 'mem0ai/oss';\n\n// Minimal configuration with just the LLM settings\nconst config = {\n  llm: {\n    provider: 'your_chosen_provider',\n    config: {\n      // Provider-specific settings go here\n    }\n  }\n};\n\nconst memory = new Memory(config);\nawait memory.add(\"Your text here\", { userId: \"user123\", metadata: { category: \"example\" } });\n```\n\n</CodeGroup>\n\n## Why is Config Needed?\n\nConfig is essential for:\n1. Specifying which LLM to use.\n2. Providing necessary connection details (e.g., model, api_key, temperature).\n3. Ensuring proper initialization and connection to your chosen LLM.\n\n## Master List of All Params in Config\n\nHere's a comprehensive list of all parameters that can be used across different LLMs:\n\n<Tabs>\n  <Tab title=\"Python\">\n    | Parameter            | Description                                   | Provider          |\n    |----------------------|-----------------------------------------------|-------------------|\n    | `model`              | Embedding model to use                        | All               |\n    | `temperature`        | Temperature of the model                      | All               |\n    | `api_key`            | API key to use                                | All               |\n    | `max_tokens`         | Tokens to generate                            | All               |\n    | `top_p`              | Probability threshold for nucleus sampling    | All               |\n    | `top_k`              | Number of highest probability tokens to keep  | All               |\n    | `http_client_proxies`| Allow proxy server settings                   | AzureOpenAI       |\n    | `models`             | List of models                                | Openrouter        |\n    | `route`              | Routing strategy                              | Openrouter        |\n    | `openrouter_base_url`| Base URL for Openrouter API                   | Openrouter        |\n    | `site_url`           | Site URL                                      | Openrouter        |\n    | `app_name`           | Application name                              | Openrouter        |\n    | `ollama_base_url`    | Base URL for Ollama API                       | Ollama            |\n    | `openai_base_url`    | Base URL for OpenAI API                       | OpenAI            |\n    | `azure_kwargs`       | Azure LLM args for initialization             | AzureOpenAI       |\n    | `deepseek_base_url`  | Base URL for DeepSeek API                     | DeepSeek          |\n    | `xai_base_url`       | Base URL for XAI API                          | XAI               |\n    | `sarvam_base_url`    | Base URL for Sarvam API                       | Sarvam            |\n    | `reasoning_effort`   | Reasoning level (low, medium, high)           | Sarvam            |\n    | `frequency_penalty`  | Penalize frequent tokens (-2.0 to 2.0)        | Sarvam            |\n    | `presence_penalty`   | Penalize existing tokens (-2.0 to 2.0)        | Sarvam            |\n    | `seed`               | Seed for deterministic sampling               | Sarvam            |\n    | `stop`               | Stop sequences (max 4)                        | Sarvam            |\n    | `lmstudio_base_url`  | Base URL for LM Studio API                    | LM Studio         |\n    | `response_callback`  | LLM response callback function                | OpenAI            |\n  </Tab>\n  <Tab title=\"TypeScript\">\n    | Parameter            | Description                                   | Provider          |\n    |----------------------|-----------------------------------------------|-------------------|\n    | `model`              | Embedding model to use                        | All               |\n    | `temperature`        | Temperature of the model                      | All               |\n    | `apiKey`             | API key to use                                | All               |\n    | `maxTokens`          | Tokens to generate                            | All               |\n    | `topP`               | Probability threshold for nucleus sampling    | All               |\n    | `topK`               | Number of highest probability tokens to keep  | All               |\n    | `openaiBaseUrl`      | Base URL for OpenAI API                       | OpenAI            |\n  </Tab>\n</Tabs>\n\n## Supported LLMs\n\nFor detailed information on configuring specific LLMs, please visit the [LLMs](./models) section. There you'll find information for each supported LLM with provider-specific usage examples and configuration details.\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_38","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' README.md\"","aggregated_output":"<p align=\"center\">\n  <a href=\"https://github.com/mem0ai/mem0\">\n    <img src=\"docs/images/banner-sm.png\" width=\"800px\" alt=\"Mem0 - The Memory Layer for Personalized AI\">\n  </a>\n</p>\n<p align=\"center\" style=\"display: flex; justify-content: center; gap: 20px; align-items: center;\">\n  <a href=\"https://trendshift.io/repositories/11194\" target=\"blank\">\n    <img src=\"https://trendshift.io/api/badge/repositories/11194\" alt=\"mem0ai%2Fmem0 | Trendshift\" width=\"250\" height=\"55\"/>\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://mem0.ai\">Learn more</a>\n  ¬∑\n  <a href=\"https://mem0.dev/DiG\">Join Discord</a>\n  ¬∑\n  <a href=\"https://mem0.dev/demo\">Demo</a>\n  ¬∑\n  <a href=\"https://mem0.dev/openmemory\">OpenMemory</a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://mem0.dev/DiG\">\n    <img src=\"https://img.shields.io/badge/Discord-%235865F2.svg?&logo=discord&logoColor=white\" alt=\"Mem0 Discord\">\n  </a>\n  <a href=\"https://pepy.tech/project/mem0ai\">\n    <img src=\"https://img.shields.io/pypi/dm/mem0ai\" alt=\"Mem0 PyPI - Downloads\">\n  </a>\n  <a href=\"https://github.com/mem0ai/mem0\">\n    <img src=\"https://img.shields.io/github/commit-activity/m/mem0ai/mem0?style=flat-square\" alt=\"GitHub commit activity\">\n  </a>\n  <a href=\"https://pypi.org/project/mem0ai\" target=\"blank\">\n    <img src=\"https://img.shields.io/pypi/v/mem0ai?color=%2334D058&label=pypi%20package\" alt=\"Package version\">\n  </a>\n  <a href=\"https://www.npmjs.com/package/mem0ai\" target=\"blank\">\n    <img src=\"https://img.shields.io/npm/v/mem0ai\" alt=\"Npm package\">\n  </a>\n  <a href=\"https://www.ycombinator.com/companies/mem0\">\n    <img src=\"https://img.shields.io/badge/Y%20Combinator-S24-orange?style=flat-square\" alt=\"Y Combinator S24\">\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://mem0.ai/research\"><strong>üìÑ Building Production-Ready AI Agents with Scalable Long-Term Memory ‚Üí</strong></a>\n</p>\n<p align=\"center\">\n  <strong>‚ö° +26% Accuracy vs. OpenAI Memory ‚Ä¢ üöÄ 91% Faster ‚Ä¢ üí∞ 90% Fewer Tokens</strong>\n</p>\n\n> **üéâ mem0ai v1.0.0 is now available!** This major release includes API modernization, improved vector store support, and enhanced GCP integration. [See migration guide ‚Üí](MIGRATION_GUIDE_v1.0.md)\n\n##  üî• Research Highlights\n- **+26% Accuracy** over OpenAI Memory on the LOCOMO benchmark\n- **91% Faster Responses** than full-context, ensuring low-latency at scale\n- **90% Lower Token Usage** than full-context, cutting costs without compromise\n- [Read the full paper](https://mem0.ai/research)\n\n# Introduction\n\n[Mem0](https://mem0.ai) (\"mem-zero\") enhances AI assistants and agents with an intelligent memory layer, enabling personalized AI interactions. It remembers user preferences, adapts to individual needs, and continuously learns over time‚Äîideal for customer support chatbots, AI assistants, and autonomous systems.\n\n### Key Features & Use Cases\n\n**Core Capabilities:**\n- **Multi-Level Memory**: Seamlessly retains User, Session, and Agent state with adaptive personalization\n- **Developer-Friendly**: Intuitive API, cross-platform SDKs, and a fully managed service option\n\n**Applications:**\n- **AI Assistants**: Consistent, context-rich conversations\n- **Customer Support**: Recall past tickets and user history for tailored help\n- **Healthcare**: Track patient preferences and history for personalized care\n- **Productivity & Gaming**: Adaptive workflows and environments based on user behavior\n\n## üöÄ Quickstart Guide <a name=\"quickstart\"></a>\n\nChoose between our hosted platform or self-hosted package:\n\n### Hosted Platform\n\nGet up and running in minutes with automatic updates, analytics, and enterprise security.\n\n1. Sign up on [Mem0 Platform](https://app.mem0.ai)\n2. Embed the memory layer via SDK or API keys\n\n### Self-Hosted (Open Source)\n\nInstall the sdk via pip:\n\n```bash\npip install mem0ai\n```\n\nInstall sdk via npm:\n```bash\nnpm install mem0ai\n```\n\n### Basic Usage\n\nMem0 requires an LLM to function, with `gpt-4.1-nano-2025-04-14 from OpenAI as the default. However, it supports a variety of LLMs; for details, refer to our [Supported LLMs documentation](https://docs.mem0.ai/components/llms/overview).\n\nFirst step is to instantiate the memory:\n\n```python\nfrom openai import OpenAI\nfrom mem0 import Memory\n\nopenai_client = OpenAI()\nmemory = Memory()\n\ndef chat_with_memories(message: str, user_id: str = \"default_user\") -> str:\n    # Retrieve relevant memories\n    relevant_memories = memory.search(query=message, user_id=user_id, limit=3)\n    memories_str = \"\\n\".join(f\"- {entry['memory']}\" for entry in relevant_memories[\"results\"])\n\n    # Generate Assistant response\n    system_prompt = f\"You are a helpful AI. Answer the question based on query and memories.\\nUser Memories:\\n{memories_str}\"\n    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": message}]\n    response = openai_client.chat.completions.create(model=\"gpt-4.1-nano-2025-04-14\", messages=messages)\n    assistant_response = response.choices[0].message.content\n\n    # Create new memories from the conversation\n    messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n    memory.add(messages, user_id=user_id)\n\n    return assistant_response\n\ndef main():\n    print(\"Chat with AI (type 'exit' to quit)\")\n    while True:\n        user_input = input(\"You: \").strip()\n        if user_input.lower() == 'exit':\n            print(\"Goodbye!\")\n            break\n        print(f\"AI: {chat_with_memories(user_input)}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nFor detailed integration steps, see the [Quickstart](https://docs.mem0.ai/quickstart) and [API Reference](https://docs.mem0.ai/api-reference).\n\n## üîó Integrations & Demos\n\n- **ChatGPT with Memory**: Personalized chat powered by Mem0 ([Live Demo](https://mem0.dev/demo))\n- **Browser Extension**: Store memories across ChatGPT, Perplexity, and Claude ([Chrome Extension](https://chromewebstore.google.com/detail/onihkkbipkfeijkadecaafbgagkhglop?utm_source=item-share-cb))\n- **Langgraph Support**: Build a customer bot with Langgraph + Mem0 ([Guide](https://docs.mem0.ai/integrations/langgraph))\n- **CrewAI Integration**: Tailor CrewAI outputs with Mem0 ([Example](https://docs.mem0.ai/integrations/crewai))\n\n## üìö Documentation & Support\n\n- Full docs: https://docs.mem0.ai\n- Community: [Discord](https://mem0.dev/DiG) ¬∑ [Twitter](https://x.com/mem0ai)\n- Contact: founders@mem0.ai\n\n## Citation\n\nWe now have a paper you can cite:\n\n```bibtex\n@article{mem0,\n  title={Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory},\n  author={Chhikara, Prateek and Khant, Dev and Aryan, Saket and Singh, Taranjeet and Yadav, Deshraj},\n  journal={arXiv preprint arXiv:2504.19413},\n  year={2025}\n}\n```\n\n## ‚öñÔ∏è License\n\nApache 2.0 ‚Äî see the [LICENSE](https://github.com/mem0ai/mem0/blob/main/LICENSE) file for details.","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_37","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,220p' mem0/configs/embeddings/base.py\"","aggregated_output":"import os\nfrom abc import ABC\nfrom typing import Dict, Optional, Union\n\nimport httpx\n\nfrom mem0.configs.base import AzureConfig\n\n\nclass BaseEmbedderConfig(ABC):\n    \"\"\"\n    Config for Embeddings.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Optional[str] = None,\n        api_key: Optional[str] = None,\n        embedding_dims: Optional[int] = None,\n        # Ollama specific\n        ollama_base_url: Optional[str] = None,\n        # Openai specific\n        openai_base_url: Optional[str] = None,\n        # Huggingface specific\n        model_kwargs: Optional[dict] = None,\n        huggingface_base_url: Optional[str] = None,\n        # AzureOpenAI specific\n        azure_kwargs: Optional[AzureConfig] = {},\n        http_client_proxies: Optional[Union[Dict, str]] = None,\n        # VertexAI specific\n        vertex_credentials_json: Optional[str] = None,\n        memory_add_embedding_type: Optional[str] = None,\n        memory_update_embedding_type: Optional[str] = None,\n        memory_search_embedding_type: Optional[str] = None,\n        # Gemini specific\n        output_dimensionality: Optional[str] = None,\n        # LM Studio specific\n        lmstudio_base_url: Optional[str] = \"http://localhost:1234/v1\",\n        # AWS Bedrock specific\n        aws_access_key_id: Optional[str] = None,\n        aws_secret_access_key: Optional[str] = None,\n        aws_region: Optional[str] = None,\n    ):\n        \"\"\"\n        Initializes a configuration class instance for the Embeddings.\n\n        :param model: Embedding model to use, defaults to None\n        :type model: Optional[str], optional\n        :param api_key: API key to be use, defaults to None\n        :type api_key: Optional[str], optional\n        :param embedding_dims: The number of dimensions in the embedding, defaults to None\n        :type embedding_dims: Optional[int], optional\n        :param ollama_base_url: Base URL for the Ollama API, defaults to None\n        :type ollama_base_url: Optional[str], optional\n        :param model_kwargs: key-value arguments for the huggingface embedding model, defaults a dict inside init\n        :type model_kwargs: Optional[Dict[str, Any]], defaults a dict inside init\n        :param huggingface_base_url: Huggingface base URL to be use, defaults to None\n        :type huggingface_base_url: Optional[str], optional\n        :param openai_base_url: Openai base URL to be use, defaults to \"https://api.openai.com/v1\"\n        :type openai_base_url: Optional[str], optional\n        :param azure_kwargs: key-value arguments for the AzureOpenAI embedding model, defaults a dict inside init\n        :type azure_kwargs: Optional[Dict[str, Any]], defaults a dict inside init\n        :param http_client_proxies: The proxy server settings used to create self.http_client, defaults to None\n        :type http_client_proxies: Optional[Dict | str], optional\n        :param vertex_credentials_json: The path to the Vertex AI credentials JSON file, defaults to None\n        :type vertex_credentials_json: Optional[str], optional\n        :param memory_add_embedding_type: The type of embedding to use for the add memory action, defaults to None\n        :type memory_add_embedding_type: Optional[str], optional\n        :param memory_update_embedding_type: The type of embedding to use for the update memory action, defaults to None\n        :type memory_update_embedding_type: Optional[str], optional\n        :param memory_search_embedding_type: The type of embedding to use for the search memory action, defaults to None\n        :type memory_search_embedding_type: Optional[str], optional\n        :param lmstudio_base_url: LM Studio base URL to be use, defaults to \"http://localhost:1234/v1\"\n        :type lmstudio_base_url: Optional[str], optional\n        \"\"\"\n\n        self.model = model\n        self.api_key = api_key\n        self.openai_base_url = openai_base_url\n        self.embedding_dims = embedding_dims\n\n        # AzureOpenAI specific\n        self.http_client = httpx.Client(proxies=http_client_proxies) if http_client_proxies else None\n\n        # Ollama specific\n        self.ollama_base_url = ollama_base_url\n\n        # Huggingface specific\n        self.model_kwargs = model_kwargs or {}\n        self.huggingface_base_url = huggingface_base_url\n        # AzureOpenAI specific\n        self.azure_kwargs = AzureConfig(**azure_kwargs) or {}\n\n        # VertexAI specific\n        self.vertex_credentials_json = vertex_credentials_json\n        self.memory_add_embedding_type = memory_add_embedding_type\n        self.memory_update_embedding_type = memory_update_embedding_type\n        self.memory_search_embedding_type = memory_search_embedding_type\n\n        # Gemini specific\n        self.output_dimensionality = output_dimensionality\n\n        # LM Studio specific\n        self.lmstudio_base_url = lmstudio_base_url\n\n        # AWS Bedrock specific\n        self.aws_access_key_id = aws_access_key_id\n        self.aws_secret_access_key = aws_secret_access_key\n        self.aws_region = aws_region or os.environ.get(\"AWS_REGION\") or \"us-west-2\"\n\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_39","type":"command_execution","command":"/bin/zsh -lc \"rg -n \\\"forge|FORGE|openrouter|OPENROUTER|OpenRouter|base_url|api_key|provider\\\" mem0 tests docs README.md LLM.md mem0-ts -g '*.py' -g '*.md' -g '*.mdx' -g '*.ts' -g '*.tsx' -g '*.json' | head -n 400\"","aggregated_output":"LLM.md:50:client = MemoryClient(api_key=\"your-api-key\")\nLLM.md:81:  embedder: { provider: 'openai', config: { apiKey: 'key' } },\nLLM.md:82:  vectorStore: { provider: 'memory', config: { dimension: 1536 } },\nLLM.md:83:  llm: { provider: 'openai', config: { apiKey: 'key' } }\nLLM.md:105:    vector_store={\"provider\": \"qdrant\", \"config\": {\"host\": \"localhost\"}},\nLLM.md:106:    llm={\"provider\": \"openai\", \"config\": {\"model\": \"gpt-4.1-nano-2025-04-14\"}},\nLLM.md:107:    embedder={\"provider\": \"openai\", \"config\": {\"model\": \"text-embedding-3-small\"}}\nLLM.md:170:    api_key=\"your-api-key\",  # or set MEM0_API_KEY env var\nLLM.md:263:    vector_store=VectorStoreConfig(provider=\"qdrant\", config={...}),\nLLM.md:264:    llm=LlmConfig(provider=\"openai\", config={...}),\nLLM.md:265:    embedder=EmbedderConfig(provider=\"openai\", config={...}),\nLLM.md:266:    graph_store=GraphStoreConfig(provider=\"neo4j\", config={...}),  # optional\nLLM.md:340:        \"provider\": \"openai\",\nLLM.md:348:        \"provider\": \"openai\",\nLLM.md:360:        \"provider\": \"ollama\",\nLLM.md:363:            \"ollama_base_url\": \"http://localhost:11434\"\nLLM.md:367:        \"provider\": \"ollama\",\nLLM.md:373:        \"provider\": \"chroma\",\nLLM.md:386:        \"provider\": \"neo4j\",\nLLM.md:401:        \"provider\": \"azure_openai\",\nLLM.md:405:            \"api_key\": \"your-api-key\",\nLLM.md:410:        \"provider\": \"pinecone\",\nLLM.md:412:            \"api_key\": \"your-pinecone-key\",\nLLM.md:521:    provider: 'openai',\nLLM.md:525:    provider: 'qdrant',\nLLM.md:529:    provider: 'openai',\nLLM.md:597:        \"provider\": \"neo4j\",\nLLM.md:734:        self.memory = MemoryClient(api_key=\"your-key\")\nLLM.md:1140:memory_client = MemoryClient(api_key=\"your-api-key\")\nLLM.md:1269:           \"provider\": \"qdrant\",\ndocs/cookbooks/essentials/memory-expiration-short-and-long-term.mdx:31:client = MemoryClient(api_key=\"your-api-key\")\ndocs/cookbooks/essentials/building-ai-companion.mdx:32:openai_client = OpenAI(api_key=\"your-openai-key\")\ndocs/cookbooks/essentials/building-ai-companion.mdx:33:mem0_client = MemoryClient(api_key=\"your-mem0-key\")\ndocs/cookbooks/essentials/building-ai-companion.mdx:321:mem0_client = MemoryClient(api_key=\"your-mem0-key\")\nmem0-ts/src/client/telemetry.ts:43:      api_key: this.apiKey,\ndocs/cookbooks/essentials/exporting-memories.mdx:18:client = MemoryClient(api_key=\"your-api-key\")\ntests/llms/test_deepseek.py:19:def test_deepseek_llm_base_url():\ntests/llms/test_deepseek.py:21:    config = BaseLlmConfig(model=\"deepseek-chat\", temperature=0.7, max_tokens=100, top_p=1.0, api_key=\"api_key\")\ntests/llms/test_deepseek.py:23:    assert str(llm.client.base_url) == \"https://api.deepseek.com\"\ntests/llms/test_deepseek.py:26:    provider_base_url = \"https://api.provider.com/v1/\"\ntests/llms/test_deepseek.py:27:    os.environ[\"DEEPSEEK_API_BASE\"] = provider_base_url\ntests/llms/test_deepseek.py:28:    config = DeepSeekConfig(model=\"deepseek-chat\", temperature=0.7, max_tokens=100, top_p=1.0, api_key=\"api_key\")\ntests/llms/test_deepseek.py:30:    assert str(llm.client.base_url) == provider_base_url\ntests/llms/test_deepseek.py:32:    # case3: with config.deepseek_base_url\ntests/llms/test_deepseek.py:33:    config_base_url = \"https://api.config.com/v1/\"\ntests/llms/test_deepseek.py:39:        api_key=\"api_key\",\ntests/llms/test_deepseek.py:40:        deepseek_base_url=config_base_url,\ntests/llms/test_deepseek.py:43:    assert str(llm.client.base_url) == config_base_url\ndocs/components/llms/models/vllm.mdx:35:        \"provider\": \"vllm\",\ndocs/components/llms/models/vllm.mdx:38:            \"vllm_base_url\": \"http://localhost:8000/v1\",\ndocs/components/llms/models/vllm.mdx:60:| `vllm_base_url` | vLLM server URL                   | `\"http://localhost:8000/v1\"`  | `VLLM_BASE_URL`      |\ndocs/components/llms/models/vllm.mdx:61:| `api_key`       | API key (dummy for local)         | `\"vllm-api-key\"`              | `VLLM_API_KEY`       |\ndocs/components/llms/models/vllm.mdx:80:- **Easy Integration**: Drop-in replacement for other LLM providers\ndocs/components/llms/models/vllm.mdx:94:   \"vllm_base_url\": \"http://localhost:8000/v1\"  # Note the /v1\ndocs/cookbooks/essentials/tagging-and-organizing-memories.mdx:18:client = MemoryClient(api_key=\"your-api-key\")\nmem0/client/main.py:31:        api_key (str): The API key for authenticating with the Mem0 API.\nmem0/client/main.py:41:        api_key: Optional[str] = None,\nmem0/client/main.py:50:            api_key: The API key for authenticating with the Mem0 API. If not\nmem0/client/main.py:58:                    used instead of creating a new one. Note that base_url and\nmem0/client/main.py:64:        self.api_key = api_key or os.getenv(\"MEM0_API_KEY\")\nmem0/client/main.py:70:        if not self.api_key:\nmem0/client/main.py:74:        self.user_id = hashlib.md5(self.api_key.encode()).hexdigest()\nmem0/client/main.py:78:            # Ensure the client has the correct base_url and headers\nmem0/client/main.py:79:            self.client.base_url = httpx.URL(self.host)\nmem0/client/main.py:82:                    \"Authorization\": f\"Token {self.api_key}\",\nmem0/client/main.py:88:                base_url=self.host,\nmem0/client/main.py:90:                    \"Authorization\": f\"Token {self.api_key}\",\nmem0/client/main.py:95:        self.user_email = self._validate_api_key()\nmem0/client/main.py:107:    def _validate_api_key(self):\nmem0/client/main.py:962:        api_key: Optional[str] = None,\nmem0/client/main.py:971:            api_key: The API key for authenticating with the Mem0 API. If not\nmem0/client/main.py:979:                    be used instead of creating a new one. Note that base_url\nmem0/client/main.py:985:        self.api_key = api_key or os.getenv(\"MEM0_API_KEY\")\nmem0/client/main.py:991:        if not self.api_key:\nmem0/client/main.py:995:        self.user_id = hashlib.md5(self.api_key.encode()).hexdigest()\nmem0/client/main.py:999:            # Ensure the client has the correct base_url and headers\nmem0/client/main.py:1000:            self.async_client.base_url = httpx.URL(self.host)\nmem0/client/main.py:1003:                    \"Authorization\": f\"Token {self.api_key}\",\nmem0/client/main.py:1009:                base_url=self.host,\nmem0/client/main.py:1011:                    \"Authorization\": f\"Token {self.api_key}\",\nmem0/client/main.py:1017:        self.user_email = self._validate_api_key()\nmem0/client/main.py:1029:    def _validate_api_key(self):\nmem0/client/main.py:1036:                    \"Authorization\": f\"Token {self.api_key}\",\ndocs/components/llms/config.mdx:10:    - `llm`: Specifies the llm provider and its configuration\ndocs/components/llms/config.mdx:11:      - `provider`: The name of the llm (e.g., \"openai\", \"groq\")\ndocs/components/llms/config.mdx:12:      - `config`: A nested dictionary containing provider-specific settings\ndocs/components/llms/config.mdx:16:    - `llm`: Specifies the LLM provider and its configuration (required)\ndocs/components/llms/config.mdx:17:      - `provider`: The name of the LLM (e.g., \"openai\", \"groq\")\ndocs/components/llms/config.mdx:18:      - `config`: A nested object containing provider-specific settings\ndocs/components/llms/config.mdx:19:    - `embedder`: Specifies the embedder provider and its configuration (optional)\ndocs/components/llms/config.mdx:20:    - `vectorStore`: Specifies the vector store provider and its configuration (optional)\ndocs/components/llms/config.mdx:48:        \"provider\": \"your_chosen_provider\",\ndocs/components/llms/config.mdx:66:    provider: 'your_chosen_provider',\ndocs/components/llms/config.mdx:83:2. Providing necessary connection details (e.g., model, api_key, temperature).\ndocs/components/llms/config.mdx:96:    | `api_key`            | API key to use                                | All               |\ndocs/components/llms/config.mdx:103:    | `openrouter_base_url`| Base URL for Openrouter API                   | Openrouter        |\ndocs/components/llms/config.mdx:106:    | `ollama_base_url`    | Base URL for Ollama API                       | Ollama            |\ndocs/components/llms/config.mdx:107:    | `openai_base_url`    | Base URL for OpenAI API                       | OpenAI            |\ndocs/components/llms/config.mdx:109:    | `deepseek_base_url`  | Base URL for DeepSeek API                     | DeepSeek          |\ndocs/components/llms/config.mdx:110:    | `xai_base_url`       | Base URL for XAI API                          | XAI               |\ndocs/components/llms/config.mdx:111:    | `sarvam_base_url`    | Base URL for Sarvam API                       | Sarvam            |\ndocs/components/llms/config.mdx:117:    | `lmstudio_base_url`  | Base URL for LM Studio API                    | LM Studio         |\ndocs/components/llms/config.mdx:135:For detailed information on configuring specific LLMs, please visit the [LLMs](./models) section. There you'll find information for each supported LLM with provider-specific usage examples and configuration details.\ndocs/components/llms/models/ollama.mdx:18:        \"provider\": \"ollama\",\ndocs/components/llms/models/ollama.mdx:42:    provider: 'ollama',\ndocs/templates/quickstart_template.mdx:95:memory = Memory(api_key=\"sk-...\")\ndocs/cookbooks/essentials/choosing-memory-architecture-vector-vs-graph.mdx:30:client = MemoryClient(api_key=\"your-api-key\")\ndocs/components/llms/models/lmstudio.mdx:18:        \"provider\": \"lmstudio\",\ndocs/components/llms/models/lmstudio.mdx:23:            \"lmstudio_base_url\": \"http://localhost:1234/v1\", # default LM Studio API URL\ndocs/components/llms/models/lmstudio.mdx:50:        \"provider\": \"lmstudio\"\ndocs/components/llms/models/lmstudio.mdx:53:        \"provider\": \"lmstudio\"\ndocs/components/llms/models/lmstudio.mdx:78:  3. Set the appropriate `lmstudio_base_url` in your configuration (default is usually http://localhost:1234/v1)\ndocs/templates/operation_guide_template.mdx:98:memory = Memory(api_key=os.environ[\"MEM0_API_KEY\"])\nmem0-ts/README.md:37:- **Vector Store Integration**: Supports various vector store providers for efficient memory retrieval.\nmem0-ts/README.md:38:- **LLM Support**: Integrates with multiple LLM providers for generating responses.\nmem0-ts/README.md:39:- **Customizable Configuration**: Easily configure memory settings and providers.\ndocs/components/llms/models/together.mdx:18:        \"provider\": \"together\",\ndocs/cookbooks/essentials/entity-partitioning-playbook.mdx:17:client = MemoryClient(api_key=\"m0-...\")\ndocs/components/embedders/models/ollama.mdx:10:os.environ[\"OPENAI_API_KEY\"] = \"your_api_key\" # For LLM\ndocs/components/embedders/models/ollama.mdx:14:        \"provider\": \"ollama\",\ndocs/components/embedders/models/ollama.mdx:36:    provider: 'ollama',\ndocs/components/embedders/models/ollama.mdx:65:| `ollama_base_url` | Base URL for ollama connection | `None` |\nmem0/llms/base.py:9:    Base class for all LLM providers.\nmem0/llms/base.py:10:    Handles common functionality and delegates provider-specific logic to subclasses.\nmem0/llms/base.py:33:        Override in subclasses to add provider-specific validation.\nmem0/llms/base.py:38:        if not hasattr(self.config, \"api_key\") and not hasattr(self.config, \"api_key\"):\nmem0/llms/base.py:40:            # This will be handled by individual providers\nmem0/llms/base.py:108:            **kwargs: Additional provider-specific parameters.\nmem0/llms/base.py:117:        Get common parameters that most providers use.\nmem0/llms/base.py:128:        # Add provider-specific parameters from kwargs\ndocs/components/llms/models/sarvam.mdx:20:        \"provider\": \"sarvam\",\ndocs/components/llms/models/sarvam.mdx:46:        \"provider\": \"sarvam\",\ndocs/components/llms/models/sarvam.mdx:56:            \"api_key\": \"your-sarvam-api-key\"\ndocs/cookbooks/essentials/controlling-memory-ingestion.mdx:37:client = MemoryClient(api_key=\"your-api-key\")\ndocs/cookbooks/essentials/controlling-memory-ingestion.mdx:385:client = MemoryClient(api_key=os.getenv(\"MEM0_API_KEY\"))\ndocs/components/embedders/models/lmstudio.mdx:9:os.environ[\"OPENAI_API_KEY\"] = \"your_api_key\" # For LLM\ndocs/components/embedders/models/lmstudio.mdx:13:        \"provider\": \"lmstudio\",\ndocs/components/embedders/models/lmstudio.mdx:38:| `lmstudio_base_url` | Base URL for LM Studio connection | `http://localhost:1234/v1` |\nmem0/llms/ollama.py:25:                api_key=config.api_key,\nmem0/llms/ollama.py:39:        self.client = Client(host=self.config.ollama_base_url)\ndocs/components/embedders/models/together.mdx:15:os.environ[\"TOGETHER_API_KEY\"] = \"your_api_key\"\ndocs/components/embedders/models/together.mdx:16:os.environ[\"OPENAI_API_KEY\"] = \"your_api_key\" # For LLM\ndocs/components/embedders/models/together.mdx:20:        \"provider\": \"together\",\ndocs/components/embedders/models/together.mdx:45:| `api_key` | The Together API key | `None` |\ndocs/components/llms/models/anthropic.mdx:20:        \"provider\": \"anthropic\",\ndocs/components/llms/models/anthropic.mdx:44:    provider: 'anthropic',\ndocs/components/embedders/models/aws_bedrock.mdx:34:        \"provider\": \"aws_bedrock\",\ndocs/components/llms/models/aws_bedrock.mdx:22:        \"provider\": \"aws_bedrock\",\ndocs/components/embedders/config.mdx:11:- `embedder`: Specifies the embedder provider and its configuration\ndocs/components/embedders/config.mdx:12:  - `provider`: The name of the embedder (e.g., \"openai\", \"ollama\")\ndocs/components/embedders/config.mdx:13:  - `config`: A nested object or dictionary containing provider-specific settings\ndocs/components/embedders/config.mdx:29:        \"provider\": \"your_chosen_provider\",\ndocs/components/embedders/config.mdx:45:    provider: 'openai',\ndocs/components/embedders/config.mdx:63:2. Providing necessary connection details (e.g., model, api_key, embedding_dims).\ndocs/components/embedders/config.mdx:75:| `api_key` | API key of the provider | All |\ndocs/components/embedders/config.mdx:78:| `ollama_base_url` | Base URL for the Ollama embedding model | Ollama |\ndocs/components/embedders/config.mdx:81:| `openai_base_url`    | Base URL for OpenAI API                       | OpenAI            |\ndocs/components/embedders/config.mdx:86:| `lmstudio_base_url` | Base URL for LM Studio API                    | LM Studio         |\ndocs/components/embedders/config.mdx:92:| `apiKey` | API key of the provider | All |\ndocs/components/embedders/config.mdx:99:For detailed information on configuring specific embedders, please visit the [Embedding Models](./models) section. There you'll find information for each supported embedder with provider-specific usage examples and configuration details.\ndocs/templates/feature_guide_template.mdx:25:- Add a `## Configure access` snippet (even if it‚Äôs ‚ÄúConfirm your Mem0 API key is already configured‚Äù) so contributors never forget to mention the baseline setup.\ndocs/templates/feature_guide_template.mdx:39:- If providers differ meaningfully, summarize them in a final accordion (`<AccordionGroup>` with one `<Accordion>` per provider) so readers can expand what they need without scrolling walls of configuration.\ndocs/templates/feature_guide_template.mdx:110:  Outline configuration or link to provider docs here.\nmem0/llms/openai_structured.py:17:        api_key = self.config.api_key or os.getenv(\"OPENAI_API_KEY\")\nmem0/llms/openai_structured.py:18:        base_url = self.config.openai_base_url or os.getenv(\"OPENAI_API_BASE\") or \"https://api.openai.com/v1\"\nmem0/llms/openai_structured.py:19:        self.client = OpenAI(api_key=api_key, base_url=base_url)\ndocs/components/embedders/models/google_AI.mdx:15:os.environ[\"OPENAI_API_KEY\"] = \"your_api_key\" # For LLM\ndocs/components/embedders/models/google_AI.mdx:19:        \"provider\": \"gemini\",\ndocs/components/embedders/models/google_AI.mdx:41:      provider: \"google\",\ndocs/components/embedders/models/google_AI.mdx:70:| `api_key`        | The Google API key                   | `None`                  |\ndocs/components/llms/models/google_AI.mdx:23:        \"provider\": \"gemini\",\ndocs/components/llms/models/google_AI.mdx:50:        // You can also use \"google\" as provider ( for backward compatibility )\ndocs/components/llms/models/google_AI.mdx:51:        provider: \"gemini\",\ndocs/cookbooks/frameworks/llamaindex-react.mdx:37:    api_key=os.environ[\"MEM0_API_KEY\"],\ntests/llms/test_langchain.py:28:    config = BaseLlmConfig(model=mock_langchain_model, temperature=0.7, max_tokens=100, api_key=\"test-api-key\")\ntests/llms/test_langchain.py:40:    config = BaseLlmConfig(model=mock_langchain_model, temperature=0.7, max_tokens=100, api_key=\"test-api-key\")\ntests/llms/test_langchain.py:72:    config = BaseLlmConfig(model=mock_langchain_model, temperature=0.7, max_tokens=100, api_key=\"test-api-key\")\ntests/llms/test_langchain.py:118:    config = BaseLlmConfig(model=\"not-a-valid-model-instance\", temperature=0.7, max_tokens=100, api_key=\"test-api-key\")\ntests/llms/test_langchain.py:126:    config = BaseLlmConfig(model=None, temperature=0.7, max_tokens=100, api_key=\"test-api-key\")\ndocs/components/embedders/models/openai.mdx:14:os.environ[\"OPENAI_API_KEY\"] = \"your_api_key\"\ndocs/components/embedders/models/openai.mdx:18:        \"provider\": \"openai\",\ndocs/components/embedders/models/openai.mdx:40:    provider: 'openai',\ndocs/components/embedders/models/openai.mdx:63:| `api_key` | The OpenAI API key | `None` |\ndocs/components/llms/models/deepseek.mdx:18:        \"provider\": \"deepseek\",\ndocs/components/llms/models/deepseek.mdx:43:        \"provider\": \"deepseek\",\ndocs/components/llms/models/deepseek.mdx:46:            \"deepseek_base_url\": \"https://your-custom-endpoint.com\",\ndocs/components/llms/models/deepseek.mdx:47:            \"api_key\": \"your-api-key\"  # alternatively to using environment variable\ndocs/cookbooks/frameworks/eliza-os-character.mdx:48:MEM0_PROVIDER_API_KEY= # API Key for the provider (OpenAI, Anthropic, etc.)\ndocs/components/rerankers/models/sentence_transformer.mdx:30:        \"provider\": \"chroma\",\ndocs/components/rerankers/models/sentence_transformer.mdx:37:        \"provider\": \"openai\",\ndocs/components/rerankers/models/sentence_transformer.mdx:43:        \"provider\": \"sentence_transformer\",\ndocs/components/rerankers/models/sentence_transformer.mdx:64:        \"provider\": \"sentence_transformer\",\ndocs/components/rerankers/models/sentence_transformer.mdx:81:    \"vector_store\": {\"provider\": \"chroma\"},\ndocs/components/rerankers/models/sentence_transformer.mdx:82:    \"llm\": {\"provider\": \"openai\", \"config\": {\"model\": \"gpt-4o-mini\"}},\ndocs/components/rerankers/models/sentence_transformer.mdx:84:        \"provider\": \"sentence_transformer\",\ndocs/components/rerankers/models/sentence_transformer.mdx:121:        \"provider\": \"sentence_transformer\", \ndocs/components/embedders/models/huggingface.mdx:13:os.environ[\"OPENAI_API_KEY\"] = \"your_api_key\" # For LLM\ndocs/components/embedders/models/huggingface.mdx:17:        \"provider\": \"huggingface\",\ndocs/components/embedders/models/huggingface.mdx:42:os.environ[\"OPENAI_API_KEY\"] = \"your_api_key\" # For LLM\ndocs/components/embedders/models/huggingface.mdx:47:        \"provider\": \"huggingface\",\ndocs/components/embedders/models/huggingface.mdx:49:            \"huggingface_base_url\": \"http://localhost:3000/v1\"\ndocs/components/embedders/models/huggingface.mdx:75:| `huggingface_base_url` | URL to connect to Text Embeddings Inference (TEI) API | `None` |\nmem0/llms/together.py:22:        api_key = self.config.api_key or os.getenv(\"TOGETHER_API_KEY\")\nmem0/llms/together.py:23:        self.client = Together(api_key=api_key)\ndocs/components/llms/models/openai.mdx:20:        \"provider\": \"openai\",\ndocs/components/llms/models/openai.mdx:30:# os.environ[\"OPENROUTER_API_KEY\"] = \"your-api-key\"\ndocs/components/llms/models/openai.mdx:33:#        \"provider\": \"openai\",\ndocs/components/llms/models/openai.mdx:55:    provider: 'openai',\ndocs/components/llms/models/openai.mdx:86:        \"provider\": \"openai_structured\",\ndocs/components/rerankers/models/cohere.mdx:29:        \"provider\": \"chroma\",\ndocs/components/rerankers/models/cohere.mdx:36:        \"provider\": \"openai\",\ndocs/components/rerankers/models/cohere.mdx:42:        \"provider\": \"cohere\",\ndocs/components/rerankers/models/cohere.mdx:45:            \"api_key\": \"your-cohere-api-key\",  # or set COHERE_API_KEY\ndocs/components/rerankers/models/cohere.mdx:75:    \"vector_store\": {\"provider\": \"chroma\"},\ndocs/components/rerankers/models/cohere.mdx:76:    \"llm\": {\"provider\": \"openai\", \"config\": {\"model\": \"gpt-4o-mini\"}},\ndocs/components/rerankers/models/cohere.mdx:78:        \"provider\": \"cohere\",\ndocs/components/rerankers/models/cohere.mdx:114:        \"provider\": \"cohere\",\ndocs/components/rerankers/models/cohere.mdx:128:| `api_key`            | Cohere API key                   | `str`  | `None`                  |\ndocs/templates/integration_guide_template.mdx:108:memory = Memory(api_key=os.environ[\"MEM0_API_KEY\"])\ndocs/templates/integration_guide_template.mdx:109:partner_client = Client(api_key=os.environ[\"PARTNER_API_KEY\"])\ntests/llms/test_azure_openai.py:102:    azure_kwargs = {\"api_key\": \"test\"}\ntests/llms/test_azure_openai.py:115:            api_key=\"test\",\ntests/llms/test_azure_openai.py:123:            api_key=\"test\",\ntests/llms/test_azure_openai.py:127:            azure_ad_token_provider=None,\ntests/llms/test_azure_openai.py:134:def test_init_with_api_key(monkeypatch):\ntests/llms/test_azure_openai.py:148:    config.azure_kwargs.api_key = \"test-key\"\ntests/llms/test_azure_openai.py:160:            azure_ad_token_provider=None,\ntests/llms/test_azure_openai.py:162:            api_key=\"test-key\",\ntests/llms/test_azure_openai.py:176:    config.azure_kwargs.api_key = None\ntests/llms/test_azure_openai.py:188:            azure_ad_token_provider=None,\ntests/llms/test_azure_openai.py:190:            api_key=\"env-key\",\ntests/llms/test_azure_openai.py:202:    config.azure_kwargs.api_key = None\ntests/llms/test_azure_openai.py:211:        patch(\"mem0.llms.azure_openai.get_bearer_token_provider\") as mock_token_provider,\ntests/llms/test_azure_openai.py:215:        mock_token_provider.return_value = \"token-provider\"\ntests/llms/test_azure_openai.py:218:        mock_token_provider.assert_called_once_with(mock_cred_instance, \"https://cognitiveservices.azure.com/.default\")\ntests/llms/test_azure_openai.py:222:            azure_ad_token_provider=\"token-provider\",\ntests/llms/test_azure_openai.py:224:            api_key=None,\ntests/llms/test_azure_openai.py:230:def test_init_with_placeholder_api_key(monkeypatch):\ntests/llms/test_azure_openai.py:233:    config.azure_kwargs.api_key = \"your-api-key\"\ntests/llms/test_azure_openai.py:242:        patch(\"mem0.llms.azure_openai.get_bearer_token_provider\") as mock_token_provider,\ntests/llms/test_azure_openai.py:246:        mock_token_provider.return_value = \"token-provider\"\ntests/llms/test_azure_openai.py:249:        mock_token_provider.assert_called_once_with(mock_cred_instance, \"https://cognitiveservices.azure.com/.default\")\ntests/llms/test_azure_openai.py:253:            azure_ad_token_provider=\"token-provider\",\ntests/llms/test_azure_openai.py:255:            api_key=None,\ndocs/components/embedders/models/langchain.mdx:5:Mem0 supports LangChain as a provider to access a wide range of embedding models. LangChain is a framework for developing applications powered by language models, making it easy to integrate various embedding providers through a consistent interface.\ndocs/components/embedders/models/langchain.mdx:17:# Set necessary environment variables for your chosen LangChain provider\ndocs/components/embedders/models/langchain.mdx:29:        \"provider\": \"langchain\",\ndocs/components/embedders/models/langchain.mdx:59:    provider: 'langchain',\ndocs/components/embedders/models/langchain.mdx:79:LangChain supports a wide range of embedding providers, including:\ndocs/components/embedders/models/langchain.mdx:91:You can use any of these model instances directly in your configuration. For a complete and up-to-date list of available embedding providers, refer to the [LangChain Text Embedding documentation](https://python.langchain.com/docs/integrations/text_embedding/).\ndocs/components/embedders/models/langchain.mdx:95:When using LangChain as an embedder provider, you'll need to:\ndocs/components/embedders/models/langchain.mdx:97:1. Set the appropriate environment variables for your chosen embedding provider\ndocs/components/embedders/models/langchain.mdx:117:        \"provider\": \"langchain\",\ndocs/components/embedders/models/langchain.mdx:139:    provider: 'langchain',\ndocs/components/embedders/models/langchain.mdx:161:        \"provider\": \"langchain\",\ndocs/components/embedders/models/langchain.mdx:181:    provider: 'langchain',\ndocs/components/embedders/models/langchain.mdx:191:  Make sure to install the necessary LangChain packages and any provider-specific dependencies.\nmem0/llms/xai.py:17:        api_key = self.config.api_key or os.getenv(\"XAI_API_KEY\")\nmem0/llms/xai.py:18:        base_url = self.config.xai_base_url or os.getenv(\"XAI_API_BASE\") or \"https://api.x.ai/v1\"\nmem0/llms/xai.py:19:        self.client = OpenAI(api_key=api_key, base_url=base_url)\ndocs/components/llms/models/groq.mdx:21:        \"provider\": \"groq\",\ndocs/components/llms/models/groq.mdx:45:    provider: 'groq',\ndocs/components/rerankers/models/llm.mdx:14:Any LLM provider supported by Mem0 can be used for reranking:\ndocs/components/rerankers/models/llm.mdx:30:        \"provider\": \"chroma\",\ndocs/components/rerankers/models/llm.mdx:37:        \"provider\": \"openai\",\ndocs/components/rerankers/models/llm.mdx:43:        \"provider\": \"llm\",\ndocs/components/rerankers/models/llm.mdx:46:            \"provider\": \"openai\",\ndocs/components/rerankers/models/llm.mdx:47:            \"api_key\": \"your-openai-api-key\",  # or set OPENAI_API_KEY\ndocs/components/rerankers/models/llm.mdx:90:    \"vector_store\": {\"provider\": \"chroma\"},\ndocs/components/rerankers/models/llm.mdx:91:    \"llm\": {\"provider\": \"openai\", \"config\": {\"model\": \"gpt-4o-mini\"}},\ndocs/components/rerankers/models/llm.mdx:93:        \"provider\": \"llm\",\ndocs/components/rerankers/models/llm.mdx:96:            \"provider\": \"openai\",\ndocs/components/rerankers/models/llm.mdx:153:        \"provider\": \"llm\",\ndocs/components/rerankers/models/llm.mdx:156:            \"provider\": \"openai\",\ndocs/components/rerankers/models/llm.mdx:166:Use different LLM providers for reranking:\ndocs/components/rerankers/models/llm.mdx:172:        \"provider\": \"llm\",\ndocs/components/rerankers/models/llm.mdx:175:            \"provider\": \"anthropic\",\ndocs/components/rerankers/models/llm.mdx:184:        \"provider\": \"llm\",\ndocs/components/rerankers/models/llm.mdx:187:            \"provider\": \"ollama\",\ndocs/components/rerankers/models/llm.mdx:199:| `provider` | LLM provider name | `str` | `\"openai\"` |\ndocs/components/rerankers/models/llm.mdx:200:| `api_key` | API key for the LLM provider | `str` | `None` |\ntests/llms/test_azure_openai_structured.py:9:        api_key=None,\ntests/llms/test_azure_openai_structured.py:15:        self.api_key = api_key\ntests/llms/test_azure_openai_structured.py:41:def test_init_with_api_key(mock_azure_openai):\ntests/llms/test_azure_openai_structured.py:42:    config = DummyConfig(model=\"test-model\", azure_kwargs=DummyAzureKwargs(api_key=\"real-key\"))\ntests/llms/test_azure_openai_structured.py:47:    assert kwargs[\"api_key\"] == \"real-key\"\ntests/llms/test_azure_openai_structured.py:48:    assert kwargs[\"azure_ad_token_provider\"] is None\ntests/llms/test_azure_openai_structured.py:52:@mock.patch(\"mem0.llms.azure_openai_structured.get_bearer_token_provider\")\ntests/llms/test_azure_openai_structured.py:54:def test_init_with_default_credential(mock_credential, mock_token_provider, mock_azure_openai):\ntests/llms/test_azure_openai_structured.py:55:    config = DummyConfig(model=None, azure_kwargs=DummyAzureKwargs(api_key=None))\ntests/llms/test_azure_openai_structured.py:56:    mock_token_provider.return_value = \"token-provider\"\ntests/llms/test_azure_openai_structured.py:61:    mock_token_provider.assert_called_once_with(mock_credential.return_value, SCOPE)\ntests/llms/test_azure_openai_structured.py:64:    assert kwargs[\"api_key\"] is None\ntests/llms/test_azure_openai_structured.py:65:    assert kwargs[\"azure_ad_token_provider\"] == \"token-provider\"\ntests/llms/test_azure_openai_structured.py:73:    config = DummyConfig(model=\"test-model\", azure_kwargs=DummyAzureKwargs(api_key=None))\ntests/llms/test_azure_openai_structured.py:77:    assert kwargs[\"api_key\"] is None\ntests/llms/test_azure_openai_structured.py:84:def test_init_with_placeholder_api_key_uses_default_credential(\ntests/llms/test_azure_openai_structured.py:89:        mock.patch(\"mem0.llms.azure_openai_structured.get_bearer_token_provider\") as mock_token_provider,\ntests/llms/test_azure_openai_structured.py:91:        config = DummyConfig(model=None, azure_kwargs=DummyAzureKwargs(api_key=\"your-api-key\"))\ntests/llms/test_azure_openai_structured.py:92:        mock_token_provider.return_value = \"token-provider\"\ntests/llms/test_azure_openai_structured.py:96:        mock_token_provider.assert_called_once_with(mock_credential.return_value, SCOPE)\ntests/llms/test_azure_openai_structured.py:99:        assert kwargs[\"api_key\"] is None\ntests/llms/test_azure_openai_structured.py:100:        assert kwargs[\"azure_ad_token_provider\"] == \"token-provider\"\nmem0/llms/anthropic.py:26:                api_key=config.api_key,\nmem0/llms/anthropic.py:40:        api_key = self.config.api_key or os.getenv(\"ANTHROPIC_API_KEY\")\nmem0/llms/anthropic.py:41:        self.client = anthropic.Anthropic(api_key=api_key)\ndocs/components/embedders/models/azure_openai.mdx:19:os.environ[\"OPENAI_API_KEY\"] = \"your_api_key\" # For LLM\ndocs/components/embedders/models/azure_openai.mdx:24:        \"provider\": \"azure_openai\",\ndocs/components/embedders/models/azure_openai.mdx:31:                  \"api_key\": \"\",\ndocs/components/embedders/models/azure_openai.mdx:55:        provider: \"azure_openai\",\ndocs/components/embedders/models/azure_openai.mdx:97:        \"provider\": \"azure_openai_structured\",\ndocs/components/llms/models/mistral_AI.mdx:19:        \"provider\": \"litellm\",\ndocs/components/llms/models/mistral_AI.mdx:43:    provider: 'mistral',\ndocs/components/rerankers/models/zero_entropy.mdx:28:        \"provider\": \"chroma\",\ndocs/components/rerankers/models/zero_entropy.mdx:35:        \"provider\": \"openai\",\ndocs/components/rerankers/models/zero_entropy.mdx:41:        \"provider\": \"zero_entropy\",\ndocs/components/rerankers/models/zero_entropy.mdx:44:            \"api_key\": \"your-zero-entropy-api-key\",  # or set ZERO_ENTROPY_API_KEY\ndocs/components/rerankers/models/zero_entropy.mdx:72:    \"vector_store\": {\"provider\": \"chroma\"},\ndocs/components/rerankers/models/zero_entropy.mdx:73:    \"llm\": {\"provider\": \"openai\", \"config\": {\"model\": \"gpt-4o-mini\"}},\ndocs/components/rerankers/models/zero_entropy.mdx:74:    \"rerank\": {\"provider\": \"zero_entropy\", \"config\": {\"model\": \"zerank-1\"}}\ndocs/components/rerankers/models/zero_entropy.mdx:103:| `api_key` | Zero Entropy API key | `str` | `None` |\ndocs/open-source/multimodal-support.mdx:23:        \"provider\": \"openai\",\ndocs/components/embedders/models/vertexai.mdx:13:os.environ[\"OPENAI_API_KEY\"] = \"your_api_key\" # For LLM\ndocs/components/embedders/models/vertexai.mdx:17:        \"provider\": \"vertexai\",\ndocs/cookbooks/companions/nodejs-companion.mdx:121:export OPENAI_API_KEY=your_api_key\nmem0-ts/src/oss/src/llms/langchain.ts:51:        \"Langchain provider requires an initialized Langchain instance passed via the 'model' field in the LLM config.\",\ndocs/components/llms/models/xAI.mdx:20:        \"provider\": \"xai\",\nmem0/llms/sarvam.py:19:        self.api_key = self.config.api_key or os.getenv(\"SARVAM_API_KEY\")\nmem0/llms/sarvam.py:21:        if not self.api_key:\nmem0/llms/sarvam.py:23:                \"Sarvam API key is required. Set SARVAM_API_KEY environment variable or provide api_key in config.\"\nmem0/llms/sarvam.py:27:        self.base_url = (\nmem0/llms/sarvam.py:28:            getattr(self.config, \"sarvam_base_url\", None) or os.getenv(\"SARVAM_API_BASE\") or \"https://api.sarvam.ai/v1\"\nmem0/llms/sarvam.py:43:        url = f\"{self.base_url}/chat/completions\"\nmem0/llms/sarvam.py:45:        headers = {\"Authorization\": f\"Bearer {self.api_key}\", \"Content-Type\": \"application/json\"}\ndocs/components/rerankers/models/huggingface.mdx:8:The Hugging Face reranker provider gives you access to thousands of reranking models available on the Hugging Face Hub. This includes popular models like BAAI's BGE rerankers and other state-of-the-art cross-encoder models.\ndocs/components/rerankers/models/huggingface.mdx:19:        \"provider\": \"huggingface\",\ndocs/components/rerankers/models/huggingface.mdx:45:        \"provider\": \"huggingface\",\ndocs/components/rerankers/models/huggingface.mdx:68:        \"provider\": \"huggingface\",\ndocs/components/rerankers/models/huggingface.mdx:79:        \"provider\": \"huggingface\",\ndocs/components/rerankers/models/huggingface.mdx:90:        \"provider\": \"huggingface\",\ndocs/components/rerankers/models/huggingface.mdx:105:        \"provider\": \"huggingface\",\ndocs/components/rerankers/models/huggingface.mdx:120:        \"provider\": \"huggingface\",\ndocs/components/rerankers/models/huggingface.mdx:131:        \"provider\": \"huggingface\",\ndocs/components/rerankers/models/huggingface.mdx:190:        \"provider\": \"huggingface\",\ndocs/components/rerankers/models/huggingface.mdx:206:        \"provider\": \"huggingface\",\ndocs/components/rerankers/models/huggingface.mdx:256:        \"provider\": \"huggingface\",\ndocs/components/rerankers/models/huggingface.mdx:272:        \"provider\": \"huggingface\",\ndocs/components/rerankers/models/huggingface.mdx:298:        \"provider\": \"huggingface\",\ndocs/components/rerankers/models/huggingface.mdx:317:        \"provider\": \"huggingface\",\ndocs/components/rerankers/models/huggingface.mdx:332:        \"provider\": \"huggingface\",\ntests/llms/test_openai.py:18:def test_openai_llm_base_url():\ntests/llms/test_openai.py:20:    config = OpenAIConfig(model=\"gpt-4.1-nano-2025-04-14\", temperature=0.7, max_tokens=100, top_p=1.0, api_key=\"api_key\")\ntests/llms/test_openai.py:22:    # Note: openai client will parse the raw base_url into a URL object, which will have a trailing slash\ntests/llms/test_openai.py:23:    assert str(llm.client.base_url) == \"https://api.openai.com/v1/\"\ntests/llms/test_openai.py:26:    provider_base_url = \"https://api.provider.com/v1\"\ntests/llms/test_openai.py:27:    os.environ[\"OPENAI_BASE_URL\"] = provider_base_url\ntests/llms/test_openai.py:28:    config = OpenAIConfig(model=\"gpt-4.1-nano-2025-04-14\", temperature=0.7, max_tokens=100, top_p=1.0, api_key=\"api_key\")\ntests/llms/test_openai.py:30:    # Note: openai client will parse the raw base_url into a URL object, which will have a trailing slash\ntests/llms/test_openai.py:31:    assert str(llm.client.base_url) == provider_base_url + \"/\"\ntests/llms/test_openai.py:33:    # case3: with config.openai_base_url\ntests/llms/test_openai.py:34:    config_base_url = \"https://api.config.com/v1\"\ntests/llms/test_openai.py:36:        model=\"gpt-4.1-nano-2025-04-14\", temperature=0.7, max_tokens=100, top_p=1.0, api_key=\"api_key\", openai_base_url=config_base_url\ntests/llms/test_openai.py:39:    # Note: openai client will parse the raw base_url into a URL object, which will have a trailing slash\ntests/llms/test_openai.py:40:    assert str(llm.client.base_url) == config_base_url + \"/\"\nmem0/llms/deepseek.py:25:                api_key=config.api_key,\nmem0/llms/deepseek.py:39:        api_key = self.config.api_key or os.getenv(\"DEEPSEEK_API_KEY\")\nmem0/llms/deepseek.py:40:        base_url = self.config.deepseek_base_url or os.getenv(\"DEEPSEEK_API_BASE\") or \"https://api.deepseek.com\"\nmem0/llms/deepseek.py:41:        self.client = OpenAI(api_key=api_key, base_url=base_url)\ndocs/components/llms/models/litellm.mdx:1:[Litellm](https://litellm.vercel.app/docs/) is compatible with over 100 large language models (LLMs), all using a standardized input/output format. You can explore the [available models](https://litellm.vercel.app/docs/providers) to use with Litellm. Ensure you set the `API_KEY` for the model you choose to use.\ndocs/components/llms/models/litellm.mdx:13:        \"provider\": \"litellm\",\ntests/test_proxy.py:38:def test_mem0_initialization_with_api_key(mock_openai_embedding_client, mock_openai_llm_client):\ndocs/components/llms/models/azure_openai.mdx:9:Optionally, you can use Azure Identity to authenticate with Azure OpenAI, which allows you to use managed identities or service principals for production and Azure CLI login for development instead of an API key. If an Azure Identity is to be used, ***do not*** set the `LLM_AZURE_OPENAI_API_KEY` environment variable or the api_key in the config dictionary.\ndocs/components/llms/models/azure_openai.mdx:30:        \"provider\": \"azure_openai\",\ndocs/components/llms/models/azure_openai.mdx:39:                  \"api_key\": \"\",\ndocs/components/llms/models/azure_openai.mdx:63:    provider: 'azure_openai',\ndocs/components/llms/models/azure_openai.mdx:102:        \"provider\": \"azure_openai_structured\",\ndocs/components/llms/models/azure_openai.mdx:111:                  \"api_key\": \"\",\ndocs/components/llms/models/azure_openai.mdx:138:        \"provider\": \"azure_openai_structured\",\ndocs/components/rerankers/models/llm_reranker.mdx:19:        \"provider\": \"llm_reranker\",\ndocs/components/rerankers/models/llm_reranker.mdx:22:                \"provider\": \"openai\",\ndocs/components/rerankers/models/llm_reranker.mdx:25:                    \"api_key\": \"your-openai-api-key\"\ndocs/components/rerankers/models/llm_reranker.mdx:50:        \"provider\": \"llm_reranker\",\ndocs/components/rerankers/models/llm_reranker.mdx:53:                \"provider\": \"anthropic\",\ndocs/components/rerankers/models/llm_reranker.mdx:56:                    \"api_key\": \"your-anthropic-api-key\"\ndocs/components/rerankers/models/llm_reranker.mdx:79:        \"provider\": \"llm_reranker\",\ndocs/components/rerankers/models/llm_reranker.mdx:82:                \"provider\": \"openai\",\ndocs/components/rerankers/models/llm_reranker.mdx:85:                    \"api_key\": \"your-openai-api-key\",\ndocs/components/rerankers/models/llm_reranker.mdx:99:        \"provider\": \"llm_reranker\",\ndocs/components/rerankers/models/llm_reranker.mdx:102:                \"provider\": \"anthropic\",\ndocs/components/rerankers/models/llm_reranker.mdx:105:                    \"api_key\": \"your-anthropic-api-key\"\ndocs/components/rerankers/models/llm_reranker.mdx:118:        \"provider\": \"llm_reranker\",\ndocs/components/rerankers/models/llm_reranker.mdx:121:                \"provider\": \"ollama\",\ndocs/components/rerankers/models/llm_reranker.mdx:124:                    \"ollama_base_url\": \"http://localhost:11434\"\ndocs/components/rerankers/models/llm_reranker.mdx:137:        \"provider\": \"llm_reranker\",\ndocs/components/rerankers/models/llm_reranker.mdx:140:                \"provider\": \"azure_openai\",\ndocs/components/rerankers/models/llm_reranker.mdx:143:                    \"api_key\": \"your-azure-api-key\",\ndocs/components/rerankers/models/llm_reranker.mdx:190:        \"provider\": \"llm_reranker\",\ndocs/components/rerankers/models/llm_reranker.mdx:193:                \"provider\": \"openai\",\ndocs/components/rerankers/models/llm_reranker.mdx:196:                    \"api_key\": \"your-api-key\"\ndocs/components/rerankers/models/llm_reranker.mdx:309:        \"provider\": \"llm_reranker\",\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_40","type":"reasoning","text":"**Reviewing OpenAI model docs**"}}
{"type":"item.started","item":{"id":"item_41","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' docs/components/llms/models/openai.mdx\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_41","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' docs/components/llms/models/openai.mdx\"","aggregated_output":"---\ntitle: OpenAI\n---\n\nTo use OpenAI LLM models, you have to set the `OPENAI_API_KEY` environment variable. You can obtain the OpenAI API key from the [OpenAI Platform](https://platform.openai.com/account/api-keys).\n\n> **Note**: The following are currently unsupported with reasoning models `Parallel tool calling`,`temperature`, `top_p`, `presence_penalty`, `frequency_penalty`, `logprobs`, `top_logprobs`, `logit_bias`, `max_tokens`\n\n## Usage\n\n<CodeGroup>\n```python Python\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n\nconfig = {\n    \"llm\": {\n        \"provider\": \"openai\",\n        \"config\": {\n            \"model\": \"gpt-4.1-nano-2025-04-14\",\n            \"temperature\": 0.2,\n            \"max_tokens\": 2000,\n        }\n    }\n}\n\n# Use Openrouter by passing it's api key\n# os.environ[\"OPENROUTER_API_KEY\"] = \"your-api-key\"\n# config = {\n#    \"llm\": {\n#        \"provider\": \"openai\",\n#        \"config\": {\n#            \"model\": \"meta-llama/llama-3.1-70b-instruct\",\n#        }\n#    }\n# }\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I‚Äôm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n```typescript TypeScript\nimport { Memory } from 'mem0ai/oss';\n\nconst config = {\n  llm: {\n    provider: 'openai',\n    config: {\n      apiKey: process.env.OPENAI_API_KEY || '',\n      model: 'gpt-4-turbo-preview',\n      temperature: 0.2,\n      maxTokens: 1500,\n    },\n  },\n};\n\nconst memory = new Memory(config);\nconst messages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I‚Äôm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nawait memory.add(messages, { userId: \"alice\", metadata: { category: \"movies\" } });\n```\n</CodeGroup>\n\nWe also support the new [OpenAI structured-outputs](https://platform.openai.com/docs/guides/structured-outputs/introduction) model.\n\n```python\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n\nconfig = {\n    \"llm\": {\n        \"provider\": \"openai_structured\",\n        \"config\": {\n            \"model\": \"gpt-4.1-nano-2025-04-14\",\n            \"temperature\": 0.0,\n        }\n    }\n}\n\nm = Memory.from_config(config)\n```\n\n## Config\n\nAll available parameters for the `openai` config are present in [Master List of All Params in Config](../config).\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_42","type":"reasoning","text":"**Searching for schema examples**"}}
{"type":"item.started","item":{"id":"item_43","type":"command_execution","command":"/bin/zsh -lc 'find .agentpr_runtime -maxdepth 4 -type f | sort | head -n 200'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_44","type":"command_execution","command":"/bin/zsh -lc \"rg -n \\\"contract_schema|preflight contract|status\\\\\\\": \\\\\\\"needs_review\\\\\\\"|minimal_diff|required_commands|ci_commands|agentpr.task_packet\\\" .agentpr_runtime . -g '*.md' -g '*.json' -g '*.yaml' -g '*.yml' | head -n 300\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_43","type":"command_execution","command":"/bin/zsh -lc 'find .agentpr_runtime -maxdepth 4 -type f | sort | head -n 200'","aggregated_output":"","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_44","type":"command_execution","command":"/bin/zsh -lc \"rg -n \\\"contract_schema|preflight contract|status\\\\\\\": \\\\\\\"needs_review\\\\\\\"|minimal_diff|required_commands|ci_commands|agentpr.task_packet\\\" .agentpr_runtime . -g '*.md' -g '*.json' -g '*.yaml' -g '*.yml' | head -n 300\"","aggregated_output":"","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_45","type":"reasoning","text":"**Checking hidden runtime directory**"}}
{"type":"item.started","item":{"id":"item_46","type":"command_execution","command":"/bin/zsh -lc 'ls -la .agentpr_runtime'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_46","type":"command_execution","command":"/bin/zsh -lc 'ls -la .agentpr_runtime'","aggregated_output":"total 0\ndrwxr-xr-x@ 13 yi  staff  416 Feb 24 21:16 .\ndrwxr-xr-x@ 28 yi  staff  896 Feb 24 21:16 ..\ndrwxr-xr-x@  2 yi  staff   64 Feb 24 21:16 bun-global\ndrwxr-xr-x@ 12 yi  staff  384 Feb 24 21:16 cache\ndrwxr-xr-x@  2 yi  staff   64 Feb 24 21:16 coverage\ndrwxr-xr-x@  4 yi  staff  128 Feb 24 21:16 data\ndrwxr-xr-x@  2 yi  staff   64 Feb 24 21:16 npm-global\ndrwxr-xr-x@  2 yi  staff   64 Feb 24 21:16 pnpm-home\ndrwxr-xr-x@  3 yi  staff   96 Feb 24 21:16 pycache\ndrwxr-xr-x@  2 yi  staff   64 Feb 24 21:16 rye-home\ndrwxr-xr-x@  2 yi  staff   64 Feb 24 21:16 tmp\ndrwxr-xr-x@  2 yi  staff   64 Feb 24 21:16 tox\ndrwxr-xr-x@  2 yi  staff   64 Feb 24 21:16 uv-tools\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_47","type":"reasoning","text":"**Assessing repo governance files**"}}
{"type":"item.started","item":{"id":"item_48","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,220p' openmemory/CONTRIBUTING.md\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_49","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,220p' embedchain/CONTRIBUTING.md\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_48","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,220p' openmemory/CONTRIBUTING.md\"","aggregated_output":"# Contributing to OpenMemory\n\nWe are a team of developers passionate about the future of AI and open-source software. With years of experience in both fields, we believe in the power of community-driven development and are excited to build tools that make AI more accessible and personalized.\n\n## Ways to Contribute\n\nWe welcome all forms of contributions:\n- Bug reports and feature requests through GitHub Issues\n- Documentation improvements\n- Code contributions\n- Testing and feedback\n- Community support and discussions\n\n## Development Workflow\n\n1. Fork the repository\n2. Create your feature branch (`git checkout -b openmemory/feature/amazing-feature`)\n3. Commit your changes (`git commit -m 'Add some amazing feature'`)\n4. Push to the branch (`git push origin openmemory/feature/amazing-feature`)\n5. Open a Pull Request\n\n## Development Setup\n\n### Backend Setup\n\n```bash\n# Copy environment file and edit file to update OPENAI_API_KEY and other secrets\nmake env\n\n# Build the containers\nmake build\n\n# Start the services\nmake up\n```\n\n### Frontend Setup\n\nThe frontend is a React application. To start the frontend:\n\n```bash\n# Install dependencies and start the development server\nmake ui-dev\n```\n\n### Prerequisites\n- Docker and Docker Compose\n- Python 3.9+ (for backend development)\n- Node.js (for frontend development)\n- OpenAI API Key (for LLM interactions)\n\n### Getting Started\nFollow the setup instructions in the README.md file to set up your development environment.\n\n## Code Standards\n\nWe value:\n- Clean, well-documented code\n- Thoughtful discussions about features and improvements\n- Respectful and constructive feedback\n- A welcoming environment for all contributors\n\n## Pull Request Process\n\n1. Ensure your code follows the project's coding standards\n2. Update documentation as needed\n3. Include tests for new features\n4. Make sure all tests pass before submitting\n\nJoin us in building the future of AI memory management! Your contributions help make OpenMemory better for everyone.\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_49","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,220p' embedchain/CONTRIBUTING.md\"","aggregated_output":"# Contributing to embedchain\n\nLet us make contribution easy, collaborative and fun.\n\n## Submit your Contribution through PR\n\nTo make a contribution, follow these steps:\n\n1. Fork and clone this repository\n2. Do the changes on your fork with dedicated feature branch `feature/f1`\n3. If you modified the code (new feature or bug-fix), please add tests for it\n4. Include proper documentation / docstring and examples to run the feature\n5. Check the linting\n6. Ensure that all tests pass\n7. Submit a pull request\n\nFor more details about pull requests, please read [GitHub's guides](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request).\n\n\n### üì¶ Package manager\n\nWe use `poetry` as our package manager. You can install poetry by following the instructions [here](https://python-poetry.org/docs/#installation).\n\nPlease DO NOT use pip or conda to install the dependencies. Instead, use poetry:\n\n```bash\nmake install_all\n\n#activate\n\npoetry shell\n```\n\n### üìå Pre-commit\n\nTo ensure our standards, make sure to install pre-commit before starting to contribute.\n\n```bash\npre-commit install\n```\n\n### üßπ Linting\n\nWe use `ruff` to lint our code. You can run the linter by running the following command:\n\n```bash\nmake lint\n```\n\nMake sure that the linter does not report any errors or warnings before submitting a pull request.\n\n### Code Formatting with `black`\n\nWe use `black` to reformat the code by running the following command:\n\n```bash\nmake format\n```\n\n### üß™ Testing\n\nWe use `pytest` to test our code. You can run the tests by running the following command:\n\n```bash\npoetry run pytest\n```\n\n\nSeveral packages have been removed from Poetry to make the package lighter. Therefore, it is recommended to run `make install_all` to install the remaining packages and ensure all tests pass.\n\n\nMake sure that all tests pass before submitting a pull request.\n\n## üöÄ Release Process\n\nAt the moment, the release process is manual. We try to make frequent releases. Usually, we release a new version when we have a new feature or bugfix. A developer with admin rights to the repository will create a new release on GitHub, and then publish the new version to PyPI.\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_50","type":"reasoning","text":"**Inspecting feature branch diff**"}}
{"type":"item.started","item":{"id":"item_51","type":"command_execution","command":"/bin/zsh -lc 'git show --stat --oneline --no-patch origin/feature/add-forge-llm-provider && git log --oneline -n 5 origin/feature/add-forge-llm-provider'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_51","type":"command_execution","command":"/bin/zsh -lc 'git show --stat --oneline --no-patch origin/feature/add-forge-llm-provider && git log --oneline -n 5 origin/feature/add-forge-llm-provider'","aggregated_output":"55b387b0 feat: Add Forge LLM router support\n55b387b0 feat: Add Forge LLM router support\n69a832dc chore: add update project options (#3947)\n70baa46c fix: add OpenClaw to docs navigation (#3965)\n3d3e875d Feature: Add OpenClaw plugin and documentation (#3964)\ndba7f045 (version-bump): update the project version to v1.0.2 (#3902)\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_52","type":"reasoning","text":"**Inspecting commit file list**"}}
{"type":"item.started","item":{"id":"item_53","type":"command_execution","command":"/bin/zsh -lc \"git show --name-only --pretty=format: 55b387b0 | sed '/\"'^$/d'\"'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_53","type":"command_execution","command":"/bin/zsh -lc \"git show --name-only --pretty=format: 55b387b0 | sed '/\"'^$/d'\"'\"","aggregated_output":"docs/components/llms/models/forge.mdx\ndocs/llms.txt\nmem0/llms/openai.py\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_54","type":"reasoning","text":"**Inspecting relevant commit details**"}}
{"type":"item.completed","item":{"id":"item_55","type":"agent_message","text":"I found an upstream `feat: Add Forge LLM router support` commit in this repo history; I‚Äôm inspecting it as the closest concrete precedent so the contract can target the same minimal surface."}}
{"type":"item.started","item":{"id":"item_56","type":"command_execution","command":"/bin/zsh -lc \"git show --stat 55b387b0 && git show -- mem0/llms/openai.py 55b387b0 | sed -n '1,260p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_56","type":"command_execution","command":"/bin/zsh -lc \"git show --stat 55b387b0 && git show -- mem0/llms/openai.py 55b387b0 | sed -n '1,260p'\"","aggregated_output":"commit 55b387b067e612a96d6305b4ee4a16cb2048ae62\nAuthor: Yiiii0 <yiqiaozhou@outlook.com>\nDate:   Thu Feb 5 00:25:04 2026 -0500\n\n    feat: Add Forge LLM router support\n    \n    - Add automatic Forge detection via FORGE_API_KEY environment variable\n    - Support custom base URL via FORGE_API_BASE (defaults to https://api.forge.tensorblock.co/v1)\n    - Forge uses standard OpenAI API format, no special parameters needed\n    - Add Forge documentation page with usage examples\n    - Add Forge to LLM documentation index\n\n docs/components/llms/models/forge.mdx | 44 +++++++++++++++++++++++++++++++++++\n docs/llms.txt                         |  1 +\n mem0/llms/openai.py                   | 28 +++++++++++++++-------\n 3 files changed, 64 insertions(+), 9 deletions(-)\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_57","type":"reasoning","text":"**Checking specific commit file content**"}}
{"type":"item.started","item":{"id":"item_58","type":"command_execution","command":"/bin/zsh -lc \"git show 55b387b0 -- mem0/llms/openai.py | sed -n '1,260p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_58","type":"command_execution","command":"/bin/zsh -lc \"git show 55b387b0 -- mem0/llms/openai.py | sed -n '1,260p'\"","aggregated_output":"commit 55b387b067e612a96d6305b4ee4a16cb2048ae62\nAuthor: Yiiii0 <yiqiaozhou@outlook.com>\nDate:   Thu Feb 5 00:25:04 2026 -0500\n\n    feat: Add Forge LLM router support\n    \n    - Add automatic Forge detection via FORGE_API_KEY environment variable\n    - Support custom base URL via FORGE_API_BASE (defaults to https://api.forge.tensorblock.co/v1)\n    - Forge uses standard OpenAI API format, no special parameters needed\n    - Add Forge documentation page with usage examples\n    - Add Forge to LLM documentation index\n\ndiff --git a/mem0/llms/openai.py b/mem0/llms/openai.py\nindex a486ff86..1c16bcb8 100644\n--- a/mem0/llms/openai.py\n+++ b/mem0/llms/openai.py\n@@ -37,7 +37,12 @@ class OpenAILLM(LLMBase):\n         if not self.config.model:\n             self.config.model = \"gpt-4.1-nano-2025-04-14\"\n \n-        if os.environ.get(\"OPENROUTER_API_KEY\"):  # Use OpenRouter\n+        if os.environ.get(\"FORGE_API_KEY\"):  # Use Forge\n+            self.client = OpenAI(\n+                api_key=os.environ.get(\"FORGE_API_KEY\"),\n+                base_url=os.getenv(\"FORGE_API_BASE\") or \"https://api.forge.tensorblock.co/v1\",\n+            )\n+        elif os.environ.get(\"OPENROUTER_API_KEY\"):  # Use OpenRouter\n             self.client = OpenAI(\n                 api_key=os.environ.get(\"OPENROUTER_API_KEY\"),\n                 base_url=self.config.openrouter_base_url\n@@ -102,13 +107,18 @@ class OpenAILLM(LLMBase):\n             json: The generated response.\n         \"\"\"\n         params = self._get_supported_params(messages=messages, **kwargs)\n-        \n-        params.update({\n-            \"model\": self.config.model,\n-            \"messages\": messages,\n-        })\n \n-        if os.getenv(\"OPENROUTER_API_KEY\"):\n+        params.update(\n+            {\n+                \"model\": self.config.model,\n+                \"messages\": messages,\n+            }\n+        )\n+\n+        if os.getenv(\"FORGE_API_KEY\"):\n+            # Forge uses standard OpenAI API format, no special parameters needed\n+            pass\n+        elif os.getenv(\"OPENROUTER_API_KEY\"):\n             openrouter_params = {}\n             if self.config.models:\n                 openrouter_params[\"models\"] = self.config.models\n@@ -123,13 +133,13 @@ class OpenAILLM(LLMBase):\n                 openrouter_params[\"extra_headers\"] = extra_headers\n \n             params.update(**openrouter_params)\n-        \n+\n         else:\n             openai_specific_generation_params = [\"store\"]\n             for param in openai_specific_generation_params:\n                 if hasattr(self.config, param):\n                     params[param] = getattr(self.config, param)\n-            \n+\n         if response_format:\n             params[\"response_format\"] = response_format\n         if tools:  # TODO: Remove tools if no issues found with new memory addition logic\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_59","type":"reasoning","text":"**Inspecting documentation for forge references**"}}
{"type":"item.started","item":{"id":"item_60","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' docs/llms.txt\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_60","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' docs/llms.txt\"","aggregated_output":"# Mem0\n\n> Mem0 is a self-improving memory layer for LLM applications, enabling personalized AI experiences that retain context across sessions, adapt over time, and reduce costs by intelligently storing and retrieving relevant information.\n\nMem0 provides both a managed platform and open-source solutions for adding persistent memory to AI agents and applications. Unlike traditional RAG systems that are stateless, Mem0 creates stateful agents that remember user preferences, learn from interactions, and evolve behavior over time.\n\nKey differentiators:\n- **Stateful vs Stateless**: Retains context across sessions rather than forgetting after each interaction\n- **Intelligent Memory Management**: Uses LLMs to extract, filter, and organize relevant information\n- **Dual Storage Architecture**: Combines vector embeddings with graph databases for comprehensive memory\n- **Sub-50ms Retrieval**: Lightning-fast memory lookups for real-time applications\n- **Multimodal Support**: Handles text, images, and documents seamlessly\n\n## Getting Started\n\n- [Introduction](https://docs.mem0.ai/introduction): Overview of Mem0's memory layer for AI agents, including stateless vs stateful agents and how memory fits in the agent stack\n- [Platform Quickstart](https://docs.mem0.ai/platform/quickstart): Get started with Mem0 Platform (managed) in minutes\n- [Open Source Python Quickstart](https://docs.mem0.ai/open-source/python-quickstart): Get started with Mem0 Open Source using Python\n- [Open Source Node.js Quickstart](https://docs.mem0.ai/open-source/node-quickstart): Get started with Mem0 Open Source using Node.js\n- [Platform Overview](https://docs.mem0.ai/platform/overview): Managed solution with 4-line integration, sub-50ms latency, and intuitive dashboard\n- [Open Source Overview](https://docs.mem0.ai/open-source/overview): Self-hosted solution with full infrastructure control and customization\n\n## Core Concepts\n\n- [Memory Types](https://docs.mem0.ai/core-concepts/memory-types): Working memory (short-term session awareness), factual memory (structured knowledge), episodic memory (past conversations), and semantic memory (general knowledge)\n- [Memory Operations - Add](https://docs.mem0.ai/core-concepts/memory-operations/add): How Mem0 processes conversations through information extraction, conflict resolution, and dual storage\n- [Memory Operations - Search](https://docs.mem0.ai/core-concepts/memory-operations/search): Retrieval of relevant memories using semantic search with query processing and result ranking\n- [Memory Operations - Update](https://docs.mem0.ai/core-concepts/memory-operations/update): Modifying existing memories when new information conflicts or supplements stored data\n- [Memory Operations - Delete](https://docs.mem0.ai/core-concepts/memory-operations/delete): Removing outdated or irrelevant memories to maintain memory quality\n\n## Platform (Managed Solution)\n\n- [Platform Quickstart](https://docs.mem0.ai/platform/quickstart): Complete guide to using Mem0 Platform with Python, JavaScript, and cURL examples\n- [Platform vs Open Source](https://docs.mem0.ai/platform/platform-vs-oss): Compare managed platform vs self-hosted options\n- [Advanced Memory Operations](https://docs.mem0.ai/platform/advanced-memory-operations): Sophisticated memory management techniques for complex applications\n\n### Essential Platform Features\n- [V2 Memory Filters](https://docs.mem0.ai/platform/features/v2-memory-filters): Advanced filtering and querying capabilities\n- [Async Client](https://docs.mem0.ai/platform/features/async-client): Non-blocking operations for high-concurrency applications\n- [Multimodal Support](https://docs.mem0.ai/platform/features/multimodal-support): Integration of images and documents (JPG, PNG, MDX, TXT, PDF) via URLs or Base64\n- [Custom Categories](https://docs.mem0.ai/platform/features/custom-categories): Define domain-specific categories to improve memory organization\n- [Async Mode Default Changes](https://docs.mem0.ai/platform/features/async-mode-default-change): Understanding new async behavior defaults\n\n### Advanced Platform Features\n- [Graph Memory](https://docs.mem0.ai/platform/features/graph-memory): Build and query relationships between entities for contextually relevant retrieval\n- [Graph Threshold](https://docs.mem0.ai/platform/features/graph-threshold): Configure graph relationship sensitivity and strength\n- [Advanced Retrieval](https://docs.mem0.ai/platform/features/advanced-retrieval): Enhanced search with keyword search, reranking, and filtering capabilities\n- [Criteria-Based Retrieval](https://docs.mem0.ai/platform/features/criteria-retrieval): Targeted memory retrieval using custom criteria\n- [Contextual Add](https://docs.mem0.ai/platform/features/contextual-add): Add memories with enhanced context awareness\n- [Custom Instructions](https://docs.mem0.ai/platform/features/custom-instructions): Customize how Mem0 processes and stores information\n\n### Data Management\n- [Direct Import](https://docs.mem0.ai/platform/features/direct-import): Bulk import existing data into Mem0 memory\n- [Memory Export](https://docs.mem0.ai/platform/features/memory-export): Export memories in structured formats using customizable Pydantic schemas\n- [Timestamp Support](https://docs.mem0.ai/platform/features/timestamp): Temporal memory management with time-based queries\n- [Expiration Dates](https://docs.mem0.ai/platform/features/expiration-date): Automatic memory cleanup with configurable expiration\n\n### Integration Features\n- [Webhooks](https://docs.mem0.ai/platform/features/webhooks): Real-time notifications for memory events\n- [Feedback Mechanism](https://docs.mem0.ai/platform/features/feedback-mechanism): Improve memory quality through user feedback\n- [Group Chat Support](https://docs.mem0.ai/platform/features/group-chat): Multi-conversation memory management\n\n### Platform Support\n- [FAQs](https://docs.mem0.ai/platform/faqs): Frequently asked questions about Mem0 Platform\n- [Contribute Guide](https://docs.mem0.ai/platform/contribute): Contributing to Mem0 Platform development\n\n## Open Source\n\n### Getting Started\n- [Python Quickstart](https://docs.mem0.ai/open-source/python-quickstart): Installation, configuration, and usage examples for Python SDK\n- [Node.js Quickstart](https://docs.mem0.ai/open-source/node-quickstart): Installation, configuration, and usage examples for Node.js SDK\n- [Configuration Guide](https://docs.mem0.ai/open-source/configuration): Complete configuration options for self-hosted deployment\n\n### Open Source Features\n- [OpenAI Compatibility](https://docs.mem0.ai/open-source/features/openai_compatibility): Seamless integration with OpenAI-compatible APIs\n- [REST API Server](https://docs.mem0.ai/open-source/features/rest-api): FastAPI-based server with core operations and OpenAPI documentation\n- [Graph Memory](https://docs.mem0.ai/open-source/features/graph-memory): Build and query entity relationships using graph stores like Neo4j\n- [Metadata Filtering](https://docs.mem0.ai/open-source/features/metadata-filtering): Advanced filtering using custom metadata fields\n- [Reranker Search](https://docs.mem0.ai/open-source/features/reranker-search): Enhanced search results with reranking models\n- [Async Memory](https://docs.mem0.ai/open-source/features/async-memory): Asynchronous memory operations for better performance\n- [Multimodal Support](https://docs.mem0.ai/open-source/features/multimodal-support): Handle text, images, and documents in self-hosted setup\n\n### Customization\n- [Custom Fact Extraction](https://docs.mem0.ai/open-source/features/custom-fact-extraction-prompt): Tailor information extraction for specific use cases\n- [Custom Memory Update Prompt](https://docs.mem0.ai/open-source/features/custom-update-memory-prompt): Customize how memories are updated and merged\n\n## Components\n\n- [LLM Overview](https://docs.mem0.ai/components/llms/overview): Comprehensive guide to Large Language Model integration and configuration options\n- [Vector Database Overview](https://docs.mem0.ai/components/vectordbs/overview): Guide to supported vector databases for semantic memory storage\n- [Embeddings Overview](https://docs.mem0.ai/components/embedders/overview): Embedding model configuration for semantic understanding\n\n### Supported LLMs\n\n- [OpenAI](https://docs.mem0.ai/components/llms/models/openai): Integration with OpenAI models including GPT-4 and structured outputs\n- [Anthropic](https://docs.mem0.ai/components/llms/models/anthropic): Claude model integration with advanced reasoning capabilities\n- [Google AI](https://docs.mem0.ai/components/llms/models/google_AI): Gemini model integration for multimodal applications\n- [Groq](https://docs.mem0.ai/components/llms/models/groq): High-performance LPU optimized models for fast inference\n- [AWS Bedrock](https://docs.mem0.ai/components/llms/models/aws_bedrock): Enterprise-grade AWS managed model integration\n- [Azure OpenAI](https://docs.mem0.ai/components/llms/models/azure_openai): Microsoft Azure hosted OpenAI models for enterprise environments\n- [Ollama](https://docs.mem0.ai/components/llms/models/ollama): Local model deployment for privacy-focused applications\n- [vLLM](https://docs.mem0.ai/components/llms/models/vllm): High-performance inference framework\n- [LM Studio](https://docs.mem0.ai/components/llms/models/lmstudio): Local model management and deployment\n- [Together](https://docs.mem0.ai/components/llms/models/together): Open-source model inference platform\n- [DeepSeek](https://docs.mem0.ai/components/llms/models/deepseek): Advanced reasoning models\n- [Sarvam](https://docs.mem0.ai/components/llms/models/sarvam): Indian language models\n- [XAI](https://docs.mem0.ai/components/llms/models/xai): xAI models integration\n- [LiteLLM](https://docs.mem0.ai/components/llms/models/litellm): Unified LLM interface and proxy\n- [LangChain](https://docs.mem0.ai/components/llms/models/langchain): LangChain LLM integration\n- [OpenAI Structured](https://docs.mem0.ai/components/llms/models/openai_structured): OpenAI with structured output support\n- [Azure OpenAI Structured](https://docs.mem0.ai/components/llms/models/azure_openai_structured): Azure OpenAI with structured outputs\n\n### Supported Vector Databases\n\n- [Qdrant](https://docs.mem0.ai/components/vectordbs/dbs/qdrant): High-performance vector similarity search engine\n- [Pinecone](https://docs.mem0.ai/components/vectordbs/dbs/pinecone): Managed vector database with serverless and pod deployment options\n- [Chroma](https://docs.mem0.ai/components/vectordbs/dbs/chroma): AI-native open-source vector database optimized for speed\n- [Weaviate](https://docs.mem0.ai/components/vectordbs/dbs/weaviate): Open-source vector search engine with built-in ML capabilities\n- [PGVector](https://docs.mem0.ai/components/vectordbs/dbs/pgvector): PostgreSQL extension for vector similarity search\n- [Milvus](https://docs.mem0.ai/components/vectordbs/dbs/milvus): Open-source vector database for AI applications at scale\n- [Redis](https://docs.mem0.ai/components/vectordbs/dbs/redis): Real-time vector storage and search with Redis Stack\n- [Supabase](https://docs.mem0.ai/components/vectordbs/dbs/supabase): Open-source Firebase alternative with vector support\n- [Upstash Vector](https://docs.mem0.ai/components/vectordbs/dbs/upstash_vector): Serverless vector database\n- [Elasticsearch](https://docs.mem0.ai/components/vectordbs/dbs/elasticsearch): Distributed search and analytics engine\n- [OpenSearch](https://docs.mem0.ai/components/vectordbs/dbs/opensearch): Open-source search and analytics platform\n- [FAISS](https://docs.mem0.ai/components/vectordbs/dbs/faiss): Facebook AI Similarity Search library\n- [MongoDB](https://docs.mem0.ai/components/vectordbs/dbs/mongodb): Document database with vector search capabilities\n- [Azure AI Search](https://docs.mem0.ai/components/vectordbs/dbs/azure): Microsoft's enterprise search service\n- [Vertex AI Vector Search](https://docs.mem0.ai/components/vectordbs/dbs/vertex_ai): Google Cloud's vector search service\n- [Databricks](https://docs.mem0.ai/components/vectordbs/dbs/databricks): Delta Lake integration for vector search\n- [Baidu](https://docs.mem0.ai/components/vectordbs/dbs/baidu): Baidu vector database integration\n- [LangChain](https://docs.mem0.ai/components/vectordbs/dbs/langchain): LangChain vector store integration\n- [S3 Vectors](https://docs.mem0.ai/components/vectordbs/dbs/s3_vectors): Amazon S3 Vectors integration\n\n### Supported Embeddings\n\n- [OpenAI Embeddings](https://docs.mem0.ai/components/embedders/models/openai): High-quality text embeddings with customizable dimensions\n- [Azure OpenAI Embeddings](https://docs.mem0.ai/components/embedders/models/azure_openai): Enterprise Azure-hosted embedding models\n- [Google AI](https://docs.mem0.ai/components/embedders/models/google_ai): Gemini embedding models\n- [AWS Bedrock](https://docs.mem0.ai/components/embedders/models/aws_bedrock): Amazon embedding models through Bedrock\n- [Hugging Face](https://docs.mem0.ai/components/embedders/models/hugging_face): Open-source embedding models for local deployment\n- [Vertex AI](https://docs.mem0.ai/components/embedders/models/vertexai): Google Cloud's enterprise embedding models\n- [Ollama](https://docs.mem0.ai/components/embedders/models/ollama): Local embedding models for privacy-focused applications\n- [Together](https://docs.mem0.ai/components/embedders/models/together): Open-source model embeddings\n- [LM Studio](https://docs.mem0.ai/components/embedders/models/lmstudio): Local model embeddings\n- [LangChain](https://docs.mem0.ai/components/embedders/models/langchain): LangChain embedder integration\n\n## Integrations\n\n- [LangChain](https://docs.mem0.ai/integrations/langchain): Seamless integration with LangChain framework for enhanced agent capabilities\n- [LangGraph](https://docs.mem0.ai/integrations/langgraph): Build stateful, multi-actor applications with persistent memory\n- [LlamaIndex](https://docs.mem0.ai/integrations/llama-index): Enhanced RAG applications with intelligent memory layer\n- [CrewAI](https://docs.mem0.ai/integrations/crewai): Multi-agent systems with shared and individual memory capabilities\n- [AutoGen](https://docs.mem0.ai/integrations/autogen): Microsoft's multi-agent conversation framework with memory\n- [Vercel AI SDK](https://docs.mem0.ai/integrations/vercel-ai-sdk): Build AI-powered web applications with persistent memory\n- [Flowise](https://docs.mem0.ai/integrations/flowise): No-code LLM workflow builder with memory capabilities\n- [Dify](https://docs.mem0.ai/integrations/dify): LLMOps platform integration for production AI applications\n\n## Cookbooks and Examples\n\n### Cookbooks Overview\n- [Cookbooks Overview](https://docs.mem0.ai/cookbooks/overview): Complete guide to Mem0 examples and implementation patterns\n\n### Essential Guides\n- [Building AI Companion](https://docs.mem0.ai/cookbooks/essentials/building-ai-companion): Core patterns for building AI agents with memory\n- [Partition Memories by Entity](https://docs.mem0.ai/cookbooks/essentials/entity-partitioning-playbook): Keep multi-tenant assistants isolated by tagging user, agent, app, and session identifiers\n- [Controlling Memory Ingestion](https://docs.mem0.ai/cookbooks/essentials/controlling-memory-ingestion): Fine-tune what gets stored in memory and when\n- [Memory Expiration](https://docs.mem0.ai/cookbooks/essentials/memory-expiration-short-and-long-term): Implement short-term and long-term memory strategies\n- [Tagging and Organizing Memories](https://docs.mem0.ai/cookbooks/essentials/tagging-and-organizing-memories): Advanced memory organization and categorization\n- [Exporting Memories](https://docs.mem0.ai/cookbooks/essentials/exporting-memories): Backup and transfer memory data between systems\n- [Choosing Memory Architecture](https://docs.mem0.ai/cookbooks/essentials/choosing-memory-architecture-vector-vs-graph): Vector vs Graph memory architectures comparison\n\n### AI Companion Examples\n- [AI Tutor](https://docs.mem0.ai/cookbooks/companions/ai-tutor): Educational AI that adapts to learning progress\n- [Travel Assistant](https://docs.mem0.ai/cookbooks/companions/travel-assistant): Travel planning agent that learns preferences\n- [Voice Companion](https://docs.mem0.ai/cookbooks/companions/voice-companion-openai): Voice-enabled AI with conversational memory\n- [Local Companion](https://docs.mem0.ai/cookbooks/companions/local-companion-ollama): Privacy-focused companion using local models\n- [Node.js Companion](https://docs.mem0.ai/cookbooks/companions/nodejs-companion): JavaScript-based AI companion applications\n- [YouTube Research Assistant](https://docs.mem0.ai/cookbooks/companions/youtube-research): AI that researches and learns from video content\n\n### Operations & Automation\n- [Support Inbox](https://docs.mem0.ai/cookbooks/operations/support-inbox): Customer service agents with conversation history\n- [Email Automation](https://docs.mem0.ai/cookbooks/operations/email-automation): Smart email processing with contextual memory\n- [Content Writing](https://docs.mem0.ai/cookbooks/operations/content-writing): AI writers that maintain brand voice and style\n- [Deep Research](https://docs.mem0.ai/cookbooks/operations/deep-research): Research assistants that build on previous findings\n- [Team Task Agent](https://docs.mem0.ai/cookbooks/operations/team-task-agent): Collaborative AI agents with shared project memory\n\n### Integration Examples\n- [OpenAI Tool Calls](https://docs.mem0.ai/cookbooks/integrations/openai-tool-calls): Mem0 integrated with OpenAI function calling\n- [AWS Bedrock](https://docs.mem0.ai/cookbooks/integrations/aws-bedrock): Enterprise memory with AWS managed services\n- [Tavily Search](https://docs.mem0.ai/cookbooks/integrations/tavily-search): Web search with persistent memory of results\n- [Healthcare Google ADK](https://docs.mem0.ai/cookbooks/integrations/healthcare-google-adk): Medical AI applications with memory\n- [Mastra Agent](https://docs.mem0.ai/cookbooks/integrations/mastra-agent): Mastra framework integration with memory\n\n### Framework Examples\n- [LlamaIndex React](https://docs.mem0.ai/cookbooks/frameworks/llamaindex-react): React applications with LlamaIndex and memory\n- [LlamaIndex Multiagent](https://docs.mem0.ai/cookbooks/frameworks/llamaindex-multiagent): Multi-agent systems with shared memory\n- [Eliza OS Character](https://docs.mem0.ai/cookbooks/frameworks/eliza-os-character): Character-based AI with persistent personality\n- [Chrome Extension](https://docs.mem0.ai/cookbooks/frameworks/chrome-extension): Browser extensions that remember user interactions\n- [Multimodal Retrieval](https://docs.mem0.ai/cookbooks/frameworks/multimodal-retrieval): Memory systems handling text, images, and documents\n\n## API Reference\n\n- [Memory APIs](https://docs.mem0.ai/api-reference/memory/add-memories): Comprehensive API documentation for memory operations\n- [Add Memories](https://docs.mem0.ai/api-reference/memory/add-memories): REST API for storing new memories with detailed request/response formats\n- [Search Memories](https://docs.mem0.ai/api-reference/memory/search-memories): Advanced search API with filtering and ranking capabilities\n- [Get All Memories](https://docs.mem0.ai/api-reference/memory/get-memories): Retrieve all memories with pagination and filtering options\n- [Update Memory](https://docs.mem0.ai/api-reference/memory/update-memory): Modify existing memories with conflict resolution\n- [Delete Memory](https://docs.mem0.ai/api-reference/memory/delete-memory): Remove memories individually or in batches\n\n## Optional\n\n- [FAQs](https://docs.mem0.ai/platform/faqs): Frequently asked questions about Mem0's Platform capabilities and implementation details\n- [Changelog](https://docs.mem0.ai/changelog): Detailed product updates and version history for tracking new features and improvements\n- [Contributing Guide](https://docs.mem0.ai/contributing/development): Guidelines for contributing to Mem0's open-source development\n- [OpenMemory](https://docs.mem0.ai/openmemory/overview): Open-source memory infrastructure for research and experimentation\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_61","type":"reasoning","text":"**Investigating documentation structure**"}}
{"type":"item.started","item":{"id":"item_62","type":"command_execution","command":"/bin/zsh -lc 'find docs/components/llms -maxdepth 3 -type f | sort'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_63","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' docs/components/llms/overview.mdx\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_64","type":"command_execution","command":"/bin/zsh -lc \"rg -n \\\"components/llms/models|forge|openai.mdx|llms.txt|navigation|tabs\\\" docs -g '*.json' -g '*.mdx' -g '*.md' | head -n 300\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_62","type":"command_execution","command":"/bin/zsh -lc 'find docs/components/llms -maxdepth 3 -type f | sort'","aggregated_output":"docs/components/llms/config.mdx\ndocs/components/llms/models/anthropic.mdx\ndocs/components/llms/models/aws_bedrock.mdx\ndocs/components/llms/models/azure_openai.mdx\ndocs/components/llms/models/deepseek.mdx\ndocs/components/llms/models/google_AI.mdx\ndocs/components/llms/models/groq.mdx\ndocs/components/llms/models/langchain.mdx\ndocs/components/llms/models/litellm.mdx\ndocs/components/llms/models/lmstudio.mdx\ndocs/components/llms/models/mistral_AI.mdx\ndocs/components/llms/models/ollama.mdx\ndocs/components/llms/models/openai.mdx\ndocs/components/llms/models/sarvam.mdx\ndocs/components/llms/models/together.mdx\ndocs/components/llms/models/vllm.mdx\ndocs/components/llms/models/xAI.mdx\ndocs/components/llms/overview.mdx\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_63","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,260p' docs/components/llms/overview.mdx\"","aggregated_output":"---\ntitle: Overview\n---\n\nMem0 includes built-in support for various popular large language models. Memory can utilize the LLM provided by the user, ensuring efficient use for specific needs.\n\n## Usage\n\nTo use a llm, you must provide a configuration to customize its usage. If no configuration is supplied, a default configuration will be applied, and `OpenAI` will be used as the llm.\n\nFor a comprehensive list of available parameters for llm configuration, please refer to [Config](./config).\n\n## Supported LLMs\n\nSee the list of supported LLMs below.\n\n<Note>\n  All LLMs are supported in Python. The following LLMs are also supported in TypeScript: **OpenAI**, **Anthropic**, and **Groq**.\n</Note>\n\n<CardGroup cols={4}>\n  <Card title=\"OpenAI\" href=\"/components/llms/models/openai\" />\n  <Card title=\"Ollama\" href=\"/components/llms/models/ollama\" />\n  <Card title=\"Azure OpenAI\" href=\"/components/llms/models/azure_openai\" />\n  <Card title=\"Anthropic\" href=\"/components/llms/models/anthropic\" />\n  <Card title=\"Together\" href=\"/components/llms/models/together\" />\n  <Card title=\"Groq\" href=\"/components/llms/models/groq\" />\n  <Card title=\"Litellm\" href=\"/components/llms/models/litellm\" />\n  <Card title=\"Mistral AI\" href=\"/components/llms/models/mistral_AI\" />\n  <Card title=\"Google AI\" href=\"/components/llms/models/google_AI\" />\n  <Card title=\"AWS bedrock\" href=\"/components/llms/models/aws_bedrock\" />\n  <Card title=\"DeepSeek\" href=\"/components/llms/models/deepseek\" />\n  <Card title=\"xAI\" href=\"/components/llms/models/xAI\" />\n  <Card title=\"Sarvam AI\" href=\"/components/llms/models/sarvam\" />\n  <Card title=\"LM Studio\" href=\"/components/llms/models/lmstudio\" />\n  <Card title=\"Langchain\" href=\"/components/llms/models/langchain\" />\n</CardGroup>\n\n## Structured vs Unstructured Outputs\n\nMem0 supports two types of OpenAI LLM formats, each with its own strengths and use cases:\n\n### Structured Outputs\n\nStructured outputs are LLMs that align with OpenAI's structured outputs model:\n\n- **Optimized for:** Returning structured responses (e.g., JSON objects)\n- **Benefits:** Precise, easily parseable data\n- **Ideal for:** Data extraction, form filling, API responses\n- **Learn more:** [OpenAI Structured Outputs Guide](https://platform.openai.com/docs/guides/structured-outputs/introduction)\n\n### Unstructured Outputs\n\nUnstructured outputs correspond to OpenAI's standard, free-form text model:\n\n- **Flexibility:** Returns open-ended, natural language responses\n- **Customization:** Use the `response_format` parameter to guide output\n- **Trade-off:** Less efficient than structured outputs for specific data needs\n- **Best for:** Creative writing, explanations, general conversation\n\nChoose the format that best suits your application's requirements for optimal performance and usability.\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_64","type":"command_execution","command":"/bin/zsh -lc \"rg -n \\\"components/llms/models|forge|openai.mdx|llms.txt|navigation|tabs\\\" docs -g '*.json' -g '*.mdx' -g '*.md' | head -n 300\"","aggregated_output":"docs/templates/quickstart_template.mdx:238:- [ ] Python **and** TypeScript tabs render correctly (or you added a `<Note>` explaining a missing language).\ndocs/platform/contribute.mdx:18:Check your team‚Äôs latest checklist or guidance so the update keeps the right navigation flow, CTA pattern, and language coverage.\ndocs/platform/contribute.mdx:128:    description=\"Jump straight into the quickstart skeleton and switch tabs for other types.\"\ndocs/templates/cookbook_template.mdx:20:- Always provide Python **and** TypeScript tabs when an SDK exists for both.\ndocs/templates/cookbook_template.mdx:21:- Every page must end with exactly two navigation cards (left = related/side quest, right = next cookbook in the journey).\ndocs/templates/feature_guide_template.mdx:25:- Add a `## Configure access` snippet (even if it‚Äôs ‚ÄúConfirm your Mem0 API key is already configured‚Äù) so contributors never forget to mention the baseline setup.\ndocs/open-source/features/multimodal-support.mdx:319:  <Card title=\"Connect Vision Models\" icon=\"circle-dot\" href=\"/components/llms/models/openai\">\ndocs/open-source/features/openai_compatibility.mdx:141:  <Card title=\"Connect Vision Models\" icon=\"circle-dot\" href=\"/components/llms/models/openai\">\ndocs/open-source/overview.mdx:58:  Need a managed alternative? Compare hosting models in the <Link href=\"/platform/platform-vs-oss\">Platform vs OSS guide</Link> or switch tabs to the Platform documentation.\ndocs/open-source/overview.mdx:107:  Need a managed alternative? Compare hosting models in the <Link href=\"/platform/platform-vs-oss\">Platform vs OSS guide</Link> or switch tabs to the Platform documentation.\ndocs/integrations/openclaw.mdx:5:Add long-term memory to [OpenClaw](https://github.com/openclaw/openclaw) agents with the `@mem0/openclaw-mem0` plugin. Your agent forgets everything between sessions ‚Äî this plugin fixes that by automatically watching conversations, extracting what matters, and bringing it back when relevant.\ndocs/integrations/openclaw.mdx:95:| `memory_forget` | Delete by ID or by query |\ndocs/components/llms/overview.mdx:22:  <Card title=\"OpenAI\" href=\"/components/llms/models/openai\" />\ndocs/components/llms/overview.mdx:23:  <Card title=\"Ollama\" href=\"/components/llms/models/ollama\" />\ndocs/components/llms/overview.mdx:24:  <Card title=\"Azure OpenAI\" href=\"/components/llms/models/azure_openai\" />\ndocs/components/llms/overview.mdx:25:  <Card title=\"Anthropic\" href=\"/components/llms/models/anthropic\" />\ndocs/components/llms/overview.mdx:26:  <Card title=\"Together\" href=\"/components/llms/models/together\" />\ndocs/components/llms/overview.mdx:27:  <Card title=\"Groq\" href=\"/components/llms/models/groq\" />\ndocs/components/llms/overview.mdx:28:  <Card title=\"Litellm\" href=\"/components/llms/models/litellm\" />\ndocs/components/llms/overview.mdx:29:  <Card title=\"Mistral AI\" href=\"/components/llms/models/mistral_AI\" />\ndocs/components/llms/overview.mdx:30:  <Card title=\"Google AI\" href=\"/components/llms/models/google_AI\" />\ndocs/components/llms/overview.mdx:31:  <Card title=\"AWS bedrock\" href=\"/components/llms/models/aws_bedrock\" />\ndocs/components/llms/overview.mdx:32:  <Card title=\"DeepSeek\" href=\"/components/llms/models/deepseek\" />\ndocs/components/llms/overview.mdx:33:  <Card title=\"xAI\" href=\"/components/llms/models/xAI\" />\ndocs/components/llms/overview.mdx:34:  <Card title=\"Sarvam AI\" href=\"/components/llms/models/sarvam\" />\ndocs/components/llms/overview.mdx:35:  <Card title=\"LM Studio\" href=\"/components/llms/models/lmstudio\" />\ndocs/components/llms/overview.mdx:36:  <Card title=\"Langchain\" href=\"/components/llms/models/langchain\" />\ndocs/cookbooks/integrations/aws-bedrock.mdx:38:- [AWS Bedrock for LLM](https://docs.mem0.ai/components/llms/models/aws_bedrock)\ndocs/cookbooks/integrations/neptune-analytics.mdx:38:- [AWS Bedrock for LLM](https://docs.mem0.ai/components/llms/models/aws_bedrock)\ndocs/docs.json:17:  \"navigation\": {\ndocs/docs.json:22:            \"tabs\": [\ndocs/docs.json:180:                              \"components/llms/models/openai\",\ndocs/docs.json:181:                              \"components/llms/models/anthropic\",\ndocs/docs.json:182:                              \"components/llms/models/azure_openai\",\ndocs/docs.json:183:                              \"components/llms/models/ollama\",\ndocs/docs.json:184:                              \"components/llms/models/together\",\ndocs/docs.json:185:                              \"components/llms/models/groq\",\ndocs/docs.json:186:                              \"components/llms/models/litellm\",\ndocs/docs.json:187:                              \"components/llms/models/mistral_AI\",\ndocs/docs.json:188:                              \"components/llms/models/google_AI\",\ndocs/docs.json:189:                              \"components/llms/models/aws_bedrock\",\ndocs/docs.json:190:                              \"components/llms/models/deepseek\",\ndocs/docs.json:191:                              \"components/llms/models/xAI\",\ndocs/docs.json:192:                              \"components/llms/models/sarvam\",\ndocs/docs.json:193:                              \"components/llms/models/lmstudio\",\ndocs/docs.json:194:                              \"components/llms/models/langchain\",\ndocs/docs.json:195:                              \"components/llms/models/vllm\"\ndocs/docs.json:715:      \"source\": \"/components/llms/models/azure_openai_structured\",\ndocs/docs.json:716:      \"destination\": \"/components/llms/models/azure_openai\"\ndocs/docs.json:719:      \"source\": \"/components/llms/models/openai_structured\",\ndocs/docs.json:720:      \"destination\": \"/components/llms/models/openai\"\ndocs/core-concepts/memory-operations/delete.mdx:167:| `delete_all(...)` | You need to forget a user/agent/run | ‚úñÔ∏è | ‚úîÔ∏è |\ndocs/changelog.mdx:582:  - Updated openai.mdx\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_65","type":"reasoning","text":"**Inspecting docs JSON for updates**"}}
{"type":"item.started","item":{"id":"item_66","type":"command_execution","command":"/bin/zsh -lc \"sed -n '150,240p' docs/docs.json\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_66","type":"command_execution","command":"/bin/zsh -lc \"sed -n '150,240p' docs/docs.json\"","aggregated_output":"                    \"group\": \"Self-Hosting Features\",\n                    \"icon\": \"server\",\n                    \"pages\": [\n                      \"open-source/features/overview\",\n                      \"open-source/features/graph-memory\",\n                      \"open-source/features/metadata-filtering\",\n                      \"open-source/features/reranker-search\",\n                      \"open-source/features/async-memory\",\n                      \"open-source/features/multimodal-support\",\n                      \"open-source/features/custom-fact-extraction-prompt\",\n                      \"open-source/features/custom-update-memory-prompt\",\n                      \"open-source/features/rest-api\",\n                      \"open-source/features/openai_compatibility\"\n                    ]\n                  },\n                  {\n                    \"group\": \"Configuration\",\n                    \"icon\": \"sliders\",\n                    \"pages\": [\n                      \"open-source/configuration\",\n                      {\n                        \"group\": \"LLMs\",\n                        \"icon\": \"message-bot\",\n                        \"pages\": [\n                          \"components/llms/overview\",\n                          \"components/llms/config\",\n                          {\n                            \"group\": \"Supported LLMs\",\n                            \"icon\": \"list\",\n                            \"pages\": [\n                              \"components/llms/models/openai\",\n                              \"components/llms/models/anthropic\",\n                              \"components/llms/models/azure_openai\",\n                              \"components/llms/models/ollama\",\n                              \"components/llms/models/together\",\n                              \"components/llms/models/groq\",\n                              \"components/llms/models/litellm\",\n                              \"components/llms/models/mistral_AI\",\n                              \"components/llms/models/google_AI\",\n                              \"components/llms/models/aws_bedrock\",\n                              \"components/llms/models/deepseek\",\n                              \"components/llms/models/xAI\",\n                              \"components/llms/models/sarvam\",\n                              \"components/llms/models/lmstudio\",\n                              \"components/llms/models/langchain\",\n                              \"components/llms/models/vllm\"\n                            ]\n                          }\n                        ]\n                      },\n                      {\n                        \"group\": \"Vector Databases\",\n                        \"icon\": \"hard-drive\",\n                        \"pages\": [\n                          \"components/vectordbs/overview\",\n                          \"components/vectordbs/config\",\n                          {\n                            \"group\": \"Supported Vector Databases\",\n                            \"icon\": \"list\",\n                            \"pages\": [\n                              \"components/vectordbs/dbs/qdrant\",\n                              \"components/vectordbs/dbs/chroma\",\n                              \"components/vectordbs/dbs/pgvector\",\n                              \"components/vectordbs/dbs/milvus\",\n                              \"components/vectordbs/dbs/pinecone\",\n                              \"components/vectordbs/dbs/mongodb\",\n                              \"components/vectordbs/dbs/azure\",\n                              \"components/vectordbs/dbs/azure_mysql\",\n                              \"components/vectordbs/dbs/redis\",\n                              \"components/vectordbs/dbs/valkey\",\n                              \"components/vectordbs/dbs/elasticsearch\",\n                              \"components/vectordbs/dbs/opensearch\",\n                              \"components/vectordbs/dbs/supabase\",\n                              \"components/vectordbs/dbs/upstash-vector\",\n                              \"components/vectordbs/dbs/vectorize\",\n                              \"components/vectordbs/dbs/vertex_ai\",\n                              \"components/vectordbs/dbs/weaviate\",\n                              \"components/vectordbs/dbs/faiss\",\n                              \"components/vectordbs/dbs/langchain\",\n                              \"components/vectordbs/dbs/baidu\",\n                              \"components/vectordbs/dbs/cassandra\",\n                              \"components/vectordbs/dbs/s3_vectors\",\n                              \"components/vectordbs/dbs/databricks\",\n                              \"components/vectordbs/dbs/neptune_analytics\"\n                            ]\n                          }\n                        ]\n                      },\n                      {\n                        \"group\": \"Embedding Models\",\n                        \"icon\": \"cube\",\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_67","type":"reasoning","text":"**Assessing docs and tests requirements**"}}
{"type":"item.started","item":{"id":"item_68","type":"command_execution","command":"/bin/zsh -lc \"test -f CODEOWNERS && echo CODEOWNERS || echo 'NO_CODEOWNERS'; test -f .github/CODEOWNERS && echo .github/CODEOWNERS || true\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_68","type":"command_execution","command":"/bin/zsh -lc \"test -f CODEOWNERS && echo CODEOWNERS || echo 'NO_CODEOWNERS'; test -f .github/CODEOWNERS && echo .github/CODEOWNERS || true\"","aggregated_output":"NO_CODEOWNERS\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_69","type":"reasoning","text":"**Checking pre-commit config for style rules**"}}
{"type":"item.started","item":{"id":"item_70","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,220p' .pre-commit-config.yaml\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_70","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,220p' .pre-commit-config.yaml\"","aggregated_output":"repos:\n  - repo: local\n    hooks:\n      - id: ruff\n        name: Ruff\n        entry: ruff check\n        language: system\n        types: [python]\n        args: [--fix] \n\n      - id: isort\n        name: isort\n        entry: isort\n        language: system\n        types: [python]\n        args: [\"--profile\", \"black\"]\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_71","type":"reasoning","text":"**Assessing Forge integration approach**"}}
{"type":"item.started","item":{"id":"item_72","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,220p' docs/components/llms/models/together.mdx\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_73","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,220p' tests/llms/test_deepseek.py\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_74","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,220p' docs/components/llms/models/deepseek.mdx\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_74","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,220p' docs/components/llms/models/deepseek.mdx\"","aggregated_output":"---\ntitle: DeepSeek\n---\n\nTo use DeepSeek LLM models, you have to set the `DEEPSEEK_API_KEY` environment variable. You can also optionally set `DEEPSEEK_API_BASE` if you need to use a different API endpoint (defaults to \"https://api.deepseek.com\").\n\n## Usage\n\n```python\nimport os\nfrom mem0 import Memory\n\nos.environ[\"DEEPSEEK_API_KEY\"] = \"your-api-key\"\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\" # for embedder model\n\nconfig = {\n    \"llm\": {\n        \"provider\": \"deepseek\",\n        \"config\": {\n            \"model\": \"deepseek-chat\",  # default model\n            \"temperature\": 0.2,\n            \"max_tokens\": 2000,\n            \"top_p\": 1.0\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I‚Äôm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\nYou can also configure the API base URL in the config:\n\n```python\nconfig = {\n    \"llm\": {\n        \"provider\": \"deepseek\",\n        \"config\": {\n            \"model\": \"deepseek-chat\",\n            \"deepseek_base_url\": \"https://your-custom-endpoint.com\",\n            \"api_key\": \"your-api-key\"  # alternatively to using environment variable\n        }\n    }\n}\n```\n\n## Config\n\nAll available parameters for the `deepseek` config are present in [Master List of All Params in Config](../config).","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_72","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,220p' docs/components/llms/models/together.mdx\"","aggregated_output":"---\ntitle: Together\n---\n\nTo use Together LLM models, you have to set the `TOGETHER_API_KEY` environment variable. You can obtain the Together API key from their [Account settings page](https://api.together.xyz/settings/api-keys).\n\n## Usage\n\n```python\nimport os\nfrom mem0 import Memory\n\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\" # used for embedding model\nos.environ[\"TOGETHER_API_KEY\"] = \"your-api-key\"\n\nconfig = {\n    \"llm\": {\n        \"provider\": \"together\",\n        \"config\": {\n            \"model\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n            \"temperature\": 0.2,\n            \"max_tokens\": 2000,\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I‚Äôm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\n## Config\n\nAll available parameters for the `together` config are present in [Master List of All Params in Config](../config).","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_73","type":"command_execution","command":"/bin/zsh -lc \"sed -n '1,220p' tests/llms/test_deepseek.py\"","aggregated_output":"import os\nfrom unittest.mock import Mock, patch\n\nimport pytest\n\nfrom mem0.configs.llms.base import BaseLlmConfig\nfrom mem0.configs.llms.deepseek import DeepSeekConfig\nfrom mem0.llms.deepseek import DeepSeekLLM\n\n\n@pytest.fixture\ndef mock_deepseek_client():\n    with patch(\"mem0.llms.deepseek.OpenAI\") as mock_openai:\n        mock_client = Mock()\n        mock_openai.return_value = mock_client\n        yield mock_client\n\n\ndef test_deepseek_llm_base_url():\n    # case1: default config with deepseek official base url\n    config = BaseLlmConfig(model=\"deepseek-chat\", temperature=0.7, max_tokens=100, top_p=1.0, api_key=\"api_key\")\n    llm = DeepSeekLLM(config)\n    assert str(llm.client.base_url) == \"https://api.deepseek.com\"\n\n    # case2: with env variable DEEPSEEK_API_BASE\n    provider_base_url = \"https://api.provider.com/v1/\"\n    os.environ[\"DEEPSEEK_API_BASE\"] = provider_base_url\n    config = DeepSeekConfig(model=\"deepseek-chat\", temperature=0.7, max_tokens=100, top_p=1.0, api_key=\"api_key\")\n    llm = DeepSeekLLM(config)\n    assert str(llm.client.base_url) == provider_base_url\n\n    # case3: with config.deepseek_base_url\n    config_base_url = \"https://api.config.com/v1/\"\n    config = DeepSeekConfig(\n        model=\"deepseek-chat\",\n        temperature=0.7,\n        max_tokens=100,\n        top_p=1.0,\n        api_key=\"api_key\",\n        deepseek_base_url=config_base_url,\n    )\n    llm = DeepSeekLLM(config)\n    assert str(llm.client.base_url) == config_base_url\n\n\ndef test_generate_response_without_tools(mock_deepseek_client):\n    config = BaseLlmConfig(model=\"deepseek-chat\", temperature=0.7, max_tokens=100, top_p=1.0)\n    llm = DeepSeekLLM(config)\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n    ]\n\n    mock_response = Mock()\n    mock_response.choices = [Mock(message=Mock(content=\"I'm doing well, thank you for asking!\"))]\n    mock_deepseek_client.chat.completions.create.return_value = mock_response\n\n    response = llm.generate_response(messages)\n\n    mock_deepseek_client.chat.completions.create.assert_called_once_with(\n        model=\"deepseek-chat\", messages=messages, temperature=0.7, max_tokens=100, top_p=1.0\n    )\n    assert response == \"I'm doing well, thank you for asking!\"\n\n\ndef test_generate_response_with_tools(mock_deepseek_client):\n    config = BaseLlmConfig(model=\"deepseek-chat\", temperature=0.7, max_tokens=100, top_p=1.0)\n    llm = DeepSeekLLM(config)\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Add a new memory: Today is a sunny day.\"},\n    ]\n    tools = [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"add_memory\",\n                \"description\": \"Add a memory\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\"data\": {\"type\": \"string\", \"description\": \"Data to add to memory\"}},\n                    \"required\": [\"data\"],\n                },\n            },\n        }\n    ]\n\n    mock_response = Mock()\n    mock_message = Mock()\n    mock_message.content = \"I've added the memory for you.\"\n\n    mock_tool_call = Mock()\n    mock_tool_call.function.name = \"add_memory\"\n    mock_tool_call.function.arguments = '{\"data\": \"Today is a sunny day.\"}'\n\n    mock_message.tool_calls = [mock_tool_call]\n    mock_response.choices = [Mock(message=mock_message)]\n    mock_deepseek_client.chat.completions.create.return_value = mock_response\n\n    response = llm.generate_response(messages, tools=tools)\n\n    mock_deepseek_client.chat.completions.create.assert_called_once_with(\n        model=\"deepseek-chat\",\n        messages=messages,\n        temperature=0.7,\n        max_tokens=100,\n        top_p=1.0,\n        tools=tools,\n        tool_choice=\"auto\",\n    )\n\n    assert response[\"content\"] == \"I've added the memory for you.\"\n    assert len(response[\"tool_calls\"]) == 1\n    assert response[\"tool_calls\"][0][\"name\"] == \"add_memory\"\n    assert response[\"tool_calls\"][0][\"arguments\"] == {\"data\": \"Today is a sunny day.\"}\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_75","type":"reasoning","text":"**Analyzing OpenAI test environment handling**"}}
{"type":"item.started","item":{"id":"item_76","type":"command_execution","command":"/bin/zsh -lc \"nl -ba tests/llms/test_openai.py | sed -n '1,280p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_77","type":"command_execution","command":"/bin/zsh -lc \"nl -ba mem0/llms/openai.py | sed -n '1,260p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_78","type":"command_execution","command":"/bin/zsh -lc \"nl -ba mem0/utils/factory.py | sed -n '1,220p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_77","type":"command_execution","command":"/bin/zsh -lc \"nl -ba mem0/llms/openai.py | sed -n '1,260p'\"","aggregated_output":"     1\timport json\n     2\timport logging\n     3\timport os\n     4\tfrom typing import Dict, List, Optional, Union\n     5\t\n     6\tfrom openai import OpenAI\n     7\t\n     8\tfrom mem0.configs.llms.base import BaseLlmConfig\n     9\tfrom mem0.configs.llms.openai import OpenAIConfig\n    10\tfrom mem0.llms.base import LLMBase\n    11\tfrom mem0.memory.utils import extract_json\n    12\t\n    13\t\n    14\tclass OpenAILLM(LLMBase):\n    15\t    def __init__(self, config: Optional[Union[BaseLlmConfig, OpenAIConfig, Dict]] = None):\n    16\t        # Convert to OpenAIConfig if needed\n    17\t        if config is None:\n    18\t            config = OpenAIConfig()\n    19\t        elif isinstance(config, dict):\n    20\t            config = OpenAIConfig(**config)\n    21\t        elif isinstance(config, BaseLlmConfig) and not isinstance(config, OpenAIConfig):\n    22\t            # Convert BaseLlmConfig to OpenAIConfig\n    23\t            config = OpenAIConfig(\n    24\t                model=config.model,\n    25\t                temperature=config.temperature,\n    26\t                api_key=config.api_key,\n    27\t                max_tokens=config.max_tokens,\n    28\t                top_p=config.top_p,\n    29\t                top_k=config.top_k,\n    30\t                enable_vision=config.enable_vision,\n    31\t                vision_details=config.vision_details,\n    32\t                http_client_proxies=config.http_client,\n    33\t            )\n    34\t\n    35\t        super().__init__(config)\n    36\t\n    37\t        if not self.config.model:\n    38\t            self.config.model = \"gpt-4.1-nano-2025-04-14\"\n    39\t\n    40\t        if os.environ.get(\"OPENROUTER_API_KEY\"):  # Use OpenRouter\n    41\t            self.client = OpenAI(\n    42\t                api_key=os.environ.get(\"OPENROUTER_API_KEY\"),\n    43\t                base_url=self.config.openrouter_base_url\n    44\t                or os.getenv(\"OPENROUTER_API_BASE\")\n    45\t                or \"https://openrouter.ai/api/v1\",\n    46\t            )\n    47\t        else:\n    48\t            api_key = self.config.api_key or os.getenv(\"OPENAI_API_KEY\")\n    49\t            base_url = self.config.openai_base_url or os.getenv(\"OPENAI_BASE_URL\") or \"https://api.openai.com/v1\"\n    50\t\n    51\t            self.client = OpenAI(api_key=api_key, base_url=base_url)\n    52\t\n    53\t    def _parse_response(self, response, tools):\n    54\t        \"\"\"\n    55\t        Process the response based on whether tools are used or not.\n    56\t\n    57\t        Args:\n    58\t            response: The raw response from API.\n    59\t            tools: The list of tools provided in the request.\n    60\t\n    61\t        Returns:\n    62\t            str or dict: The processed response.\n    63\t        \"\"\"\n    64\t        if tools:\n    65\t            processed_response = {\n    66\t                \"content\": response.choices[0].message.content,\n    67\t                \"tool_calls\": [],\n    68\t            }\n    69\t\n    70\t            if response.choices[0].message.tool_calls:\n    71\t                for tool_call in response.choices[0].message.tool_calls:\n    72\t                    processed_response[\"tool_calls\"].append(\n    73\t                        {\n    74\t                            \"name\": tool_call.function.name,\n    75\t                            \"arguments\": json.loads(extract_json(tool_call.function.arguments)),\n    76\t                        }\n    77\t                    )\n    78\t\n    79\t            return processed_response\n    80\t        else:\n    81\t            return response.choices[0].message.content\n    82\t\n    83\t    def generate_response(\n    84\t        self,\n    85\t        messages: List[Dict[str, str]],\n    86\t        response_format=None,\n    87\t        tools: Optional[List[Dict]] = None,\n    88\t        tool_choice: str = \"auto\",\n    89\t        **kwargs,\n    90\t    ):\n    91\t        \"\"\"\n    92\t        Generate a JSON response based on the given messages using OpenAI.\n    93\t\n    94\t        Args:\n    95\t            messages (list): List of message dicts containing 'role' and 'content'.\n    96\t            response_format (str or object, optional): Format of the response. Defaults to \"text\".\n    97\t            tools (list, optional): List of tools that the model can call. Defaults to None.\n    98\t            tool_choice (str, optional): Tool choice method. Defaults to \"auto\".\n    99\t            **kwargs: Additional OpenAI-specific parameters.\n   100\t\n   101\t        Returns:\n   102\t            json: The generated response.\n   103\t        \"\"\"\n   104\t        params = self._get_supported_params(messages=messages, **kwargs)\n   105\t        \n   106\t        params.update({\n   107\t            \"model\": self.config.model,\n   108\t            \"messages\": messages,\n   109\t        })\n   110\t\n   111\t        if os.getenv(\"OPENROUTER_API_KEY\"):\n   112\t            openrouter_params = {}\n   113\t            if self.config.models:\n   114\t                openrouter_params[\"models\"] = self.config.models\n   115\t                openrouter_params[\"route\"] = self.config.route\n   116\t                params.pop(\"model\")\n   117\t\n   118\t            if self.config.site_url and self.config.app_name:\n   119\t                extra_headers = {\n   120\t                    \"HTTP-Referer\": self.config.site_url,\n   121\t                    \"X-Title\": self.config.app_name,\n   122\t                }\n   123\t                openrouter_params[\"extra_headers\"] = extra_headers\n   124\t\n   125\t            params.update(**openrouter_params)\n   126\t        \n   127\t        else:\n   128\t            openai_specific_generation_params = [\"store\"]\n   129\t            for param in openai_specific_generation_params:\n   130\t                if hasattr(self.config, param):\n   131\t                    params[param] = getattr(self.config, param)\n   132\t            \n   133\t        if response_format:\n   134\t            params[\"response_format\"] = response_format\n   135\t        if tools:  # TODO: Remove tools if no issues found with new memory addition logic\n   136\t            params[\"tools\"] = tools\n   137\t            params[\"tool_choice\"] = tool_choice\n   138\t        response = self.client.chat.completions.create(**params)\n   139\t        parsed_response = self._parse_response(response, tools)\n   140\t        if self.config.response_callback:\n   141\t            try:\n   142\t                self.config.response_callback(self, response, params)\n   143\t            except Exception as e:\n   144\t                # Log error but don't propagate\n   145\t                logging.error(f\"Error due to callback: {e}\")\n   146\t                pass\n   147\t        return parsed_response\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_78","type":"command_execution","command":"/bin/zsh -lc \"nl -ba mem0/utils/factory.py | sed -n '1,220p'\"","aggregated_output":"     1\timport importlib\n     2\tfrom typing import Dict, Optional, Union\n     3\t\n     4\tfrom mem0.configs.embeddings.base import BaseEmbedderConfig\n     5\tfrom mem0.configs.llms.anthropic import AnthropicConfig\n     6\tfrom mem0.configs.llms.azure import AzureOpenAIConfig\n     7\tfrom mem0.configs.llms.base import BaseLlmConfig\n     8\tfrom mem0.configs.llms.deepseek import DeepSeekConfig\n     9\tfrom mem0.configs.llms.lmstudio import LMStudioConfig\n    10\tfrom mem0.configs.llms.ollama import OllamaConfig\n    11\tfrom mem0.configs.llms.openai import OpenAIConfig\n    12\tfrom mem0.configs.llms.vllm import VllmConfig\n    13\tfrom mem0.configs.rerankers.base import BaseRerankerConfig\n    14\tfrom mem0.configs.rerankers.cohere import CohereRerankerConfig\n    15\tfrom mem0.configs.rerankers.sentence_transformer import SentenceTransformerRerankerConfig\n    16\tfrom mem0.configs.rerankers.zero_entropy import ZeroEntropyRerankerConfig\n    17\tfrom mem0.configs.rerankers.llm import LLMRerankerConfig\n    18\tfrom mem0.configs.rerankers.huggingface import HuggingFaceRerankerConfig\n    19\tfrom mem0.embeddings.mock import MockEmbeddings\n    20\t\n    21\t\n    22\tdef load_class(class_type):\n    23\t    module_path, class_name = class_type.rsplit(\".\", 1)\n    24\t    module = importlib.import_module(module_path)\n    25\t    return getattr(module, class_name)\n    26\t\n    27\t\n    28\tclass LlmFactory:\n    29\t    \"\"\"\n    30\t    Factory for creating LLM instances with appropriate configurations.\n    31\t    Supports both old-style BaseLlmConfig and new provider-specific configs.\n    32\t    \"\"\"\n    33\t\n    34\t    # Provider mappings with their config classes\n    35\t    provider_to_class = {\n    36\t        \"ollama\": (\"mem0.llms.ollama.OllamaLLM\", OllamaConfig),\n    37\t        \"openai\": (\"mem0.llms.openai.OpenAILLM\", OpenAIConfig),\n    38\t        \"groq\": (\"mem0.llms.groq.GroqLLM\", BaseLlmConfig),\n    39\t        \"together\": (\"mem0.llms.together.TogetherLLM\", BaseLlmConfig),\n    40\t        \"aws_bedrock\": (\"mem0.llms.aws_bedrock.AWSBedrockLLM\", BaseLlmConfig),\n    41\t        \"litellm\": (\"mem0.llms.litellm.LiteLLM\", BaseLlmConfig),\n    42\t        \"azure_openai\": (\"mem0.llms.azure_openai.AzureOpenAILLM\", AzureOpenAIConfig),\n    43\t        \"openai_structured\": (\"mem0.llms.openai_structured.OpenAIStructuredLLM\", OpenAIConfig),\n    44\t        \"anthropic\": (\"mem0.llms.anthropic.AnthropicLLM\", AnthropicConfig),\n    45\t        \"azure_openai_structured\": (\"mem0.llms.azure_openai_structured.AzureOpenAIStructuredLLM\", AzureOpenAIConfig),\n    46\t        \"gemini\": (\"mem0.llms.gemini.GeminiLLM\", BaseLlmConfig),\n    47\t        \"deepseek\": (\"mem0.llms.deepseek.DeepSeekLLM\", DeepSeekConfig),\n    48\t        \"xai\": (\"mem0.llms.xai.XAILLM\", BaseLlmConfig),\n    49\t        \"sarvam\": (\"mem0.llms.sarvam.SarvamLLM\", BaseLlmConfig),\n    50\t        \"lmstudio\": (\"mem0.llms.lmstudio.LMStudioLLM\", LMStudioConfig),\n    51\t        \"vllm\": (\"mem0.llms.vllm.VllmLLM\", VllmConfig),\n    52\t        \"langchain\": (\"mem0.llms.langchain.LangchainLLM\", BaseLlmConfig),\n    53\t    }\n    54\t\n    55\t    @classmethod\n    56\t    def create(cls, provider_name: str, config: Optional[Union[BaseLlmConfig, Dict]] = None, **kwargs):\n    57\t        \"\"\"\n    58\t        Create an LLM instance with the appropriate configuration.\n    59\t\n    60\t        Args:\n    61\t            provider_name (str): The provider name (e.g., 'openai', 'anthropic')\n    62\t            config: Configuration object or dict. If None, will create default config\n    63\t            **kwargs: Additional configuration parameters\n    64\t\n    65\t        Returns:\n    66\t            Configured LLM instance\n    67\t\n    68\t        Raises:\n    69\t            ValueError: If provider is not supported\n    70\t        \"\"\"\n    71\t        if provider_name not in cls.provider_to_class:\n    72\t            raise ValueError(f\"Unsupported Llm provider: {provider_name}\")\n    73\t\n    74\t        class_type, config_class = cls.provider_to_class[provider_name]\n    75\t        llm_class = load_class(class_type)\n    76\t\n    77\t        # Handle configuration\n    78\t        if config is None:\n    79\t            # Create default config with kwargs\n    80\t            config = config_class(**kwargs)\n    81\t        elif isinstance(config, dict):\n    82\t            # Merge dict config with kwargs\n    83\t            config.update(kwargs)\n    84\t            config = config_class(**config)\n    85\t        elif isinstance(config, BaseLlmConfig):\n    86\t            # Convert base config to provider-specific config if needed\n    87\t            if config_class != BaseLlmConfig:\n    88\t                # Convert to provider-specific config\n    89\t                config_dict = {\n    90\t                    \"model\": config.model,\n    91\t                    \"temperature\": config.temperature,\n    92\t                    \"api_key\": config.api_key,\n    93\t                    \"max_tokens\": config.max_tokens,\n    94\t                    \"top_p\": config.top_p,\n    95\t                    \"top_k\": config.top_k,\n    96\t                    \"enable_vision\": config.enable_vision,\n    97\t                    \"vision_details\": config.vision_details,\n    98\t                    \"http_client_proxies\": config.http_client,\n    99\t                }\n   100\t                config_dict.update(kwargs)\n   101\t                config = config_class(**config_dict)\n   102\t            else:\n   103\t                # Use base config as-is\n   104\t                pass\n   105\t        else:\n   106\t            # Assume it's already the correct config type\n   107\t            pass\n   108\t\n   109\t        return llm_class(config)\n   110\t\n   111\t    @classmethod\n   112\t    def register_provider(cls, name: str, class_path: str, config_class=None):\n   113\t        \"\"\"\n   114\t        Register a new provider.\n   115\t\n   116\t        Args:\n   117\t            name (str): Provider name\n   118\t            class_path (str): Full path to LLM class\n   119\t            config_class: Configuration class for the provider (defaults to BaseLlmConfig)\n   120\t        \"\"\"\n   121\t        if config_class is None:\n   122\t            config_class = BaseLlmConfig\n   123\t        cls.provider_to_class[name] = (class_path, config_class)\n   124\t\n   125\t    @classmethod\n   126\t    def get_supported_providers(cls) -> list:\n   127\t        \"\"\"\n   128\t        Get list of supported providers.\n   129\t\n   130\t        Returns:\n   131\t            list: List of supported provider names\n   132\t        \"\"\"\n   133\t        return list(cls.provider_to_class.keys())\n   134\t\n   135\t\n   136\tclass EmbedderFactory:\n   137\t    provider_to_class = {\n   138\t        \"openai\": \"mem0.embeddings.openai.OpenAIEmbedding\",\n   139\t        \"ollama\": \"mem0.embeddings.ollama.OllamaEmbedding\",\n   140\t        \"huggingface\": \"mem0.embeddings.huggingface.HuggingFaceEmbedding\",\n   141\t        \"azure_openai\": \"mem0.embeddings.azure_openai.AzureOpenAIEmbedding\",\n   142\t        \"gemini\": \"mem0.embeddings.gemini.GoogleGenAIEmbedding\",\n   143\t        \"vertexai\": \"mem0.embeddings.vertexai.VertexAIEmbedding\",\n   144\t        \"together\": \"mem0.embeddings.together.TogetherEmbedding\",\n   145\t        \"lmstudio\": \"mem0.embeddings.lmstudio.LMStudioEmbedding\",\n   146\t        \"langchain\": \"mem0.embeddings.langchain.LangchainEmbedding\",\n   147\t        \"aws_bedrock\": \"mem0.embeddings.aws_bedrock.AWSBedrockEmbedding\",\n   148\t        \"fastembed\": \"mem0.embeddings.fastembed.FastEmbedEmbedding\",\n   149\t    }\n   150\t\n   151\t    @classmethod\n   152\t    def create(cls, provider_name, config, vector_config: Optional[dict]):\n   153\t        if provider_name == \"upstash_vector\" and vector_config and vector_config.enable_embeddings:\n   154\t            return MockEmbeddings()\n   155\t        class_type = cls.provider_to_class.get(provider_name)\n   156\t        if class_type:\n   157\t            embedder_instance = load_class(class_type)\n   158\t            base_config = BaseEmbedderConfig(**config)\n   159\t            return embedder_instance(base_config)\n   160\t        else:\n   161\t            raise ValueError(f\"Unsupported Embedder provider: {provider_name}\")\n   162\t\n   163\t\n   164\tclass VectorStoreFactory:\n   165\t    provider_to_class = {\n   166\t        \"qdrant\": \"mem0.vector_stores.qdrant.Qdrant\",\n   167\t        \"chroma\": \"mem0.vector_stores.chroma.ChromaDB\",\n   168\t        \"pgvector\": \"mem0.vector_stores.pgvector.PGVector\",\n   169\t        \"milvus\": \"mem0.vector_stores.milvus.MilvusDB\",\n   170\t        \"upstash_vector\": \"mem0.vector_stores.upstash_vector.UpstashVector\",\n   171\t        \"azure_ai_search\": \"mem0.vector_stores.azure_ai_search.AzureAISearch\",\n   172\t        \"azure_mysql\": \"mem0.vector_stores.azure_mysql.AzureMySQL\",\n   173\t        \"pinecone\": \"mem0.vector_stores.pinecone.PineconeDB\",\n   174\t        \"mongodb\": \"mem0.vector_stores.mongodb.MongoDB\",\n   175\t        \"redis\": \"mem0.vector_stores.redis.RedisDB\",\n   176\t        \"valkey\": \"mem0.vector_stores.valkey.ValkeyDB\",\n   177\t        \"databricks\": \"mem0.vector_stores.databricks.Databricks\",\n   178\t        \"elasticsearch\": \"mem0.vector_stores.elasticsearch.ElasticsearchDB\",\n   179\t        \"vertex_ai_vector_search\": \"mem0.vector_stores.vertex_ai_vector_search.GoogleMatchingEngine\",\n   180\t        \"opensearch\": \"mem0.vector_stores.opensearch.OpenSearchDB\",\n   181\t        \"supabase\": \"mem0.vector_stores.supabase.Supabase\",\n   182\t        \"weaviate\": \"mem0.vector_stores.weaviate.Weaviate\",\n   183\t        \"faiss\": \"mem0.vector_stores.faiss.FAISS\",\n   184\t        \"langchain\": \"mem0.vector_stores.langchain.Langchain\",\n   185\t        \"s3_vectors\": \"mem0.vector_stores.s3_vectors.S3Vectors\",\n   186\t        \"baidu\": \"mem0.vector_stores.baidu.BaiduDB\",\n   187\t        \"cassandra\": \"mem0.vector_stores.cassandra.CassandraDB\",\n   188\t        \"neptune\": \"mem0.vector_stores.neptune_analytics.NeptuneAnalyticsVector\",\n   189\t    }\n   190\t\n   191\t    @classmethod\n   192\t    def create(cls, provider_name, config):\n   193\t        class_type = cls.provider_to_class.get(provider_name)\n   194\t        if class_type:\n   195\t            if not isinstance(config, dict):\n   196\t                config = config.model_dump()\n   197\t            vector_store_instance = load_class(class_type)\n   198\t            return vector_store_instance(**config)\n   199\t        else:\n   200\t            raise ValueError(f\"Unsupported VectorStore provider: {provider_name}\")\n   201\t\n   202\t    @classmethod\n   203\t    def reset(cls, instance):\n   204\t        instance.reset()\n   205\t        return instance\n   206\t\n   207\t\n   208\tclass GraphStoreFactory:\n   209\t    \"\"\"\n   210\t    Factory for creating MemoryGraph instances for different graph store providers.\n   211\t    Usage: GraphStoreFactory.create(provider_name, config)\n   212\t    \"\"\"\n   213\t\n   214\t    provider_to_class = {\n   215\t        \"memgraph\": \"mem0.memory.memgraph_memory.MemoryGraph\",\n   216\t        \"neptune\": \"mem0.graphs.neptune.neptunegraph.MemoryGraph\",\n   217\t        \"neptunedb\": \"mem0.graphs.neptune.neptunedb.MemoryGraph\",\n   218\t        \"kuzu\": \"mem0.memory.kuzu_memory.MemoryGraph\",\n   219\t        \"default\": \"mem0.memory.graph_memory.MemoryGraph\",\n   220\t    }\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_76","type":"command_execution","command":"/bin/zsh -lc \"nl -ba tests/llms/test_openai.py | sed -n '1,280p'\"","aggregated_output":"     1\timport os\n     2\tfrom unittest.mock import Mock, patch\n     3\t\n     4\timport pytest\n     5\t\n     6\tfrom mem0.configs.llms.openai import OpenAIConfig\n     7\tfrom mem0.llms.openai import OpenAILLM\n     8\t\n     9\t\n    10\t@pytest.fixture\n    11\tdef mock_openai_client():\n    12\t    with patch(\"mem0.llms.openai.OpenAI\") as mock_openai:\n    13\t        mock_client = Mock()\n    14\t        mock_openai.return_value = mock_client\n    15\t        yield mock_client\n    16\t\n    17\t\n    18\tdef test_openai_llm_base_url():\n    19\t    # case1: default config: with openai official base url\n    20\t    config = OpenAIConfig(model=\"gpt-4.1-nano-2025-04-14\", temperature=0.7, max_tokens=100, top_p=1.0, api_key=\"api_key\")\n    21\t    llm = OpenAILLM(config)\n    22\t    # Note: openai client will parse the raw base_url into a URL object, which will have a trailing slash\n    23\t    assert str(llm.client.base_url) == \"https://api.openai.com/v1/\"\n    24\t\n    25\t    # case2: with env variable OPENAI_API_BASE\n    26\t    provider_base_url = \"https://api.provider.com/v1\"\n    27\t    os.environ[\"OPENAI_BASE_URL\"] = provider_base_url\n    28\t    config = OpenAIConfig(model=\"gpt-4.1-nano-2025-04-14\", temperature=0.7, max_tokens=100, top_p=1.0, api_key=\"api_key\")\n    29\t    llm = OpenAILLM(config)\n    30\t    # Note: openai client will parse the raw base_url into a URL object, which will have a trailing slash\n    31\t    assert str(llm.client.base_url) == provider_base_url + \"/\"\n    32\t\n    33\t    # case3: with config.openai_base_url\n    34\t    config_base_url = \"https://api.config.com/v1\"\n    35\t    config = OpenAIConfig(\n    36\t        model=\"gpt-4.1-nano-2025-04-14\", temperature=0.7, max_tokens=100, top_p=1.0, api_key=\"api_key\", openai_base_url=config_base_url\n    37\t    )\n    38\t    llm = OpenAILLM(config)\n    39\t    # Note: openai client will parse the raw base_url into a URL object, which will have a trailing slash\n    40\t    assert str(llm.client.base_url) == config_base_url + \"/\"\n    41\t\n    42\t\n    43\tdef test_generate_response_without_tools(mock_openai_client):\n    44\t    config = OpenAIConfig(model=\"gpt-4.1-nano-2025-04-14\", temperature=0.7, max_tokens=100, top_p=1.0)\n    45\t    llm = OpenAILLM(config)\n    46\t    messages = [\n    47\t        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    48\t        {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n    49\t    ]\n    50\t\n    51\t    mock_response = Mock()\n    52\t    mock_response.choices = [Mock(message=Mock(content=\"I'm doing well, thank you for asking!\"))]\n    53\t    mock_openai_client.chat.completions.create.return_value = mock_response\n    54\t\n    55\t    response = llm.generate_response(messages)\n    56\t\n    57\t    mock_openai_client.chat.completions.create.assert_called_once_with(\n    58\t        model=\"gpt-4.1-nano-2025-04-14\", messages=messages, temperature=0.7, max_tokens=100, top_p=1.0, store=False\n    59\t    )\n    60\t    assert response == \"I'm doing well, thank you for asking!\"\n    61\t\n    62\t\n    63\tdef test_generate_response_with_tools(mock_openai_client):\n    64\t    config = OpenAIConfig(model=\"gpt-4.1-nano-2025-04-14\", temperature=0.7, max_tokens=100, top_p=1.0)\n    65\t    llm = OpenAILLM(config)\n    66\t    messages = [\n    67\t        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    68\t        {\"role\": \"user\", \"content\": \"Add a new memory: Today is a sunny day.\"},\n    69\t    ]\n    70\t    tools = [\n    71\t        {\n    72\t            \"type\": \"function\",\n    73\t            \"function\": {\n    74\t                \"name\": \"add_memory\",\n    75\t                \"description\": \"Add a memory\",\n    76\t                \"parameters\": {\n    77\t                    \"type\": \"object\",\n    78\t                    \"properties\": {\"data\": {\"type\": \"string\", \"description\": \"Data to add to memory\"}},\n    79\t                    \"required\": [\"data\"],\n    80\t                },\n    81\t            },\n    82\t        }\n    83\t    ]\n    84\t\n    85\t    mock_response = Mock()\n    86\t    mock_message = Mock()\n    87\t    mock_message.content = \"I've added the memory for you.\"\n    88\t\n    89\t    mock_tool_call = Mock()\n    90\t    mock_tool_call.function.name = \"add_memory\"\n    91\t    mock_tool_call.function.arguments = '{\"data\": \"Today is a sunny day.\"}'\n    92\t\n    93\t    mock_message.tool_calls = [mock_tool_call]\n    94\t    mock_response.choices = [Mock(message=mock_message)]\n    95\t    mock_openai_client.chat.completions.create.return_value = mock_response\n    96\t\n    97\t    response = llm.generate_response(messages, tools=tools)\n    98\t\n    99\t    mock_openai_client.chat.completions.create.assert_called_once_with(\n   100\t        model=\"gpt-4.1-nano-2025-04-14\", messages=messages, temperature=0.7, max_tokens=100, top_p=1.0, tools=tools, tool_choice=\"auto\", store=False\n   101\t    )\n   102\t\n   103\t    assert response[\"content\"] == \"I've added the memory for you.\"\n   104\t    assert len(response[\"tool_calls\"]) == 1\n   105\t    assert response[\"tool_calls\"][0][\"name\"] == \"add_memory\"\n   106\t    assert response[\"tool_calls\"][0][\"arguments\"] == {\"data\": \"Today is a sunny day.\"}\n   107\t\n   108\t\n   109\tdef test_response_callback_invocation(mock_openai_client):\n   110\t    # Setup mock callback\n   111\t    mock_callback = Mock()\n   112\t    \n   113\t    config = OpenAIConfig(model=\"gpt-4.1-nano-2025-04-14\", response_callback=mock_callback)\n   114\t    llm = OpenAILLM(config)\n   115\t    messages = [{\"role\": \"user\", \"content\": \"Test callback\"}]\n   116\t    \n   117\t    # Mock response\n   118\t    mock_response = Mock()\n   119\t    mock_response.choices = [Mock(message=Mock(content=\"Response\"))]\n   120\t    mock_openai_client.chat.completions.create.return_value = mock_response\n   121\t    \n   122\t    # Call method\n   123\t    llm.generate_response(messages)\n   124\t    \n   125\t    # Verify callback called with correct arguments\n   126\t    mock_callback.assert_called_once()\n   127\t    args = mock_callback.call_args[0]\n   128\t    assert args[0] is llm  # llm_instance\n   129\t    assert args[1] == mock_response  # raw_response\n   130\t    assert \"messages\" in args[2]  # params\n   131\t\n   132\t\n   133\tdef test_no_response_callback(mock_openai_client):\n   134\t    config = OpenAIConfig(model=\"gpt-4.1-nano-2025-04-14\")\n   135\t    llm = OpenAILLM(config)\n   136\t    messages = [{\"role\": \"user\", \"content\": \"Test no callback\"}]\n   137\t    \n   138\t    # Mock response\n   139\t    mock_response = Mock()\n   140\t    mock_response.choices = [Mock(message=Mock(content=\"Response\"))]\n   141\t    mock_openai_client.chat.completions.create.return_value = mock_response\n   142\t    \n   143\t    # Should complete without calling any callback\n   144\t    response = llm.generate_response(messages)\n   145\t    assert response == \"Response\"\n   146\t    \n   147\t    # Verify no callback is set\n   148\t    assert llm.config.response_callback is None\n   149\t\n   150\t\n   151\tdef test_callback_exception_handling(mock_openai_client):\n   152\t    # Callback that raises exception\n   153\t    def faulty_callback(*args):\n   154\t        raise ValueError(\"Callback error\")\n   155\t    \n   156\t    config = OpenAIConfig(model=\"gpt-4.1-nano-2025-04-14\", response_callback=faulty_callback)\n   157\t    llm = OpenAILLM(config)\n   158\t    messages = [{\"role\": \"user\", \"content\": \"Test exception\"}]\n   159\t    \n   160\t    # Mock response\n   161\t    mock_response = Mock()\n   162\t    mock_response.choices = [Mock(message=Mock(content=\"Expected response\"))]\n   163\t    mock_openai_client.chat.completions.create.return_value = mock_response\n   164\t    \n   165\t    # Should complete without raising\n   166\t    response = llm.generate_response(messages)\n   167\t    assert response == \"Expected response\"\n   168\t    \n   169\t    # Verify callback was called (even though it raised an exception)\n   170\t    assert llm.config.response_callback is faulty_callback\n   171\t\n   172\t\n   173\tdef test_callback_with_tools(mock_openai_client):\n   174\t    mock_callback = Mock()\n   175\t    config = OpenAIConfig(model=\"gpt-4.1-nano-2025-04-14\", response_callback=mock_callback)\n   176\t    llm = OpenAILLM(config)\n   177\t    messages = [{\"role\": \"user\", \"content\": \"Test tools\"}]\n   178\t    tools = [\n   179\t        {\n   180\t            \"type\": \"function\",\n   181\t            \"function\": {\n   182\t                \"name\": \"test_tool\",\n   183\t                \"description\": \"A test tool\",\n   184\t                \"parameters\": {\n   185\t                    \"type\": \"object\",\n   186\t                    \"properties\": {\"param1\": {\"type\": \"string\"}},\n   187\t                    \"required\": [\"param1\"],\n   188\t                },\n   189\t            }\n   190\t        }\n   191\t    ]\n   192\t    \n   193\t    # Mock tool response\n   194\t    mock_response = Mock()\n   195\t    mock_message = Mock()\n   196\t    mock_message.content = \"Tool response\"\n   197\t    mock_tool_call = Mock()\n   198\t    mock_tool_call.function.name = \"test_tool\"\n   199\t    mock_tool_call.function.arguments = '{\"param1\": \"value1\"}'\n   200\t    mock_message.tool_calls = [mock_tool_call]\n   201\t    mock_response.choices = [Mock(message=mock_message)]\n   202\t    mock_openai_client.chat.completions.create.return_value = mock_response\n   203\t    \n   204\t    llm.generate_response(messages, tools=tools)\n   205\t    \n   206\t    # Verify callback called with tool response\n   207\t    mock_callback.assert_called_once()\n   208\t    # Check that tool_calls exists in the message\n   209\t    assert hasattr(mock_callback.call_args[0][1].choices[0].message, 'tool_calls')\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_79","type":"reasoning","text":"**Assessing CI and local environment gaps**"}}
{"type":"item.completed","item":{"id":"item_80","type":"agent_message","text":"I now have enough repo context; I‚Äôm validating mandatory blockers against your safety contract and then I‚Äôll output a single machine-readable preflight contract JSON for this discovery stage."}}
{"type":"item.started","item":{"id":"item_81","type":"command_execution","command":"/bin/zsh -lc \"nl -ba .github/workflows/ci.yml | sed -n '1,260p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_82","type":"command_execution","command":"/bin/zsh -lc \"git show 55b387b0:docs/components/llms/models/forge.mdx | sed -n '1,260p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_83","type":"command_execution","command":"/bin/zsh -lc 'git show 55b387b0:docs/llms.txt | rg -n \"Forge|components/llms/models/forge|Supported LLMs\" -n --context 3'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_81","type":"command_execution","command":"/bin/zsh -lc \"nl -ba .github/workflows/ci.yml | sed -n '1,260p'\"","aggregated_output":"     1\tname: ci\n     2\t\n     3\ton:\n     4\t  push:\n     5\t    branches: [main]\n     6\t    paths:\n     7\t      - 'mem0/**'\n     8\t      - 'tests/**'\n     9\t      - 'embedchain/**'\n    10\t      - '.github/workflows/**'\n    11\t      - 'pyproject.toml'\n    12\t  pull_request:\n    13\t    paths:\n    14\t      - 'mem0/**'\n    15\t      - 'tests/**'\n    16\t      - 'embedchain/**'\n    17\t\n    18\tjobs:\n    19\t  check_changes:\n    20\t    runs-on: ubuntu-latest\n    21\t    outputs:\n    22\t      mem0_changed: ${{ steps.filter.outputs.mem0 }}\n    23\t      embedchain_changed: ${{ steps.filter.outputs.embedchain }}\n    24\t    steps:\n    25\t    - uses: actions/checkout@v3\n    26\t    - uses: dorny/paths-filter@v2\n    27\t      id: filter\n    28\t      with:\n    29\t        filters: |\n    30\t          mem0:\n    31\t            - 'mem0/**'\n    32\t            - 'tests/**'\n    33\t            - '.github/workflows/**'\n    34\t            - 'pyproject.toml'\n    35\t          embedchain:\n    36\t            - 'embedchain/**'\n    37\t\n    38\t  build_mem0:\n    39\t    needs: check_changes\n    40\t    if: needs.check_changes.outputs.mem0_changed == 'true'\n    41\t    runs-on: ubuntu-latest\n    42\t    strategy:\n    43\t      matrix:\n    44\t        python-version: [\"3.10\", \"3.11\", \"3.12\"]\n    45\t    steps:\n    46\t      - uses: actions/checkout@v3\n    47\t      - name: Set up Python ${{ matrix.python-version }}\n    48\t        uses: actions/setup-python@v4\n    49\t        with:\n    50\t          python-version: ${{ matrix.python-version }}\n    51\t      - name: Clean up disk space\n    52\t        run: |\n    53\t          df -h\n    54\t          sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /opt/hostedtoolcache/CodeQL\n    55\t          sudo docker image prune --all --force\n    56\t          sudo docker builder prune -a\n    57\t          df -h\n    58\t      - name: Install Hatch\n    59\t        run: pip install hatch\n    60\t      - name: Load cached venv\n    61\t        id: cached-hatch-dependencies\n    62\t        uses: actions/cache@v3\n    63\t        with:\n    64\t          path: .venv\n    65\t          key: venv-mem0-${{ runner.os }}-${{ hashFiles('**/pyproject.toml') }}\n    66\t      - name: Install GEOS Libraries\n    67\t        run: sudo apt-get update && sudo apt-get install -y libgeos-dev\n    68\t      - name: Install dependencies\n    69\t        run: |\n    70\t          pip install --upgrade pip\n    71\t          pip install -e \".[test,graph,vector_stores,llms,extras]\"\n    72\t          pip install ruff\n    73\t        if: steps.cached-hatch-dependencies.outputs.cache-hit != 'true'\n    74\t      - name: Run Linting\n    75\t        run: make lint\n    76\t      - name: Run tests and generate coverage report\n    77\t        run: make test\n    78\t\n    79\t  build_embedchain:\n    80\t    needs: check_changes\n    81\t    if: needs.check_changes.outputs.embedchain_changed == 'true'\n    82\t    runs-on: ubuntu-latest\n    83\t    strategy:\n    84\t      matrix:\n    85\t        python-version: [\"3.9\", \"3.10\", \"3.11\", \"3.12\"]\n    86\t    steps:\n    87\t      - uses: actions/checkout@v3\n    88\t      - name: Set up Python ${{ matrix.python-version }}\n    89\t        uses: actions/setup-python@v4\n    90\t        with:\n    91\t          python-version: ${{ matrix.python-version }}\n    92\t      - name: Install Hatch\n    93\t        run: pip install hatch\n    94\t      - name: Load cached venv\n    95\t        id: cached-hatch-dependencies\n    96\t        uses: actions/cache@v3\n    97\t        with:\n    98\t          path: .venv\n    99\t          key: venv-embedchain-${{ runner.os }}-${{ hashFiles('**/pyproject.toml') }}\n   100\t      - name: Install dependencies\n   101\t        run: cd embedchain && make install_all\n   102\t        if: steps.cached-hatch-dependencies.outputs.cache-hit != 'true'\n   103\t      - name: Run Formatting\n   104\t        run: |\n   105\t          mkdir -p embedchain/.ruff_cache && chmod -R 777 embedchain/.ruff_cache\n   106\t          cd embedchain && hatch run format\n   107\t      - name: Lint with ruff\n   108\t        run: cd embedchain && make lint\n   109\t      - name: Run tests and generate coverage report\n   110\t        run: cd embedchain && make coverage\n   111\t      - name: Upload coverage reports to Codecov\n   112\t        uses: codecov/codecov-action@v3\n   113\t        with:\n   114\t          file: coverage.xml\n   115\t        env:\n   116\t          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_82","type":"command_execution","command":"/bin/zsh -lc \"git show 55b387b0:docs/components/llms/models/forge.mdx | sed -n '1,260p'\"","aggregated_output":"---\ntitle: Forge\n---\n\n[Forge](https://forge.tensorblock.co) is an LLM router that provides unified access to multiple AI providers (OpenAI, Anthropic, Google, and more) through a single API. It's compatible with the OpenAI API format, making it easy to integrate with existing applications.\n\n## Usage\n\n```python\nimport os\nfrom mem0 import Memory\n\nos.environ[\"FORGE_API_KEY\"] = \"your-api-key\"\n\nconfig = {\n    \"llm\": {\n        \"provider\": \"openai\",\n        \"config\": {\n            \"model\": \"OpenAI/gpt-4o-mini\",\n            \"temperature\": 0.2,\n            \"max_tokens\": 2000,\n        }\n    }\n}\n\nm = Memory.from_config(config)\nmessages = [\n    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    {\"role\": \"assistant\", \"content\": \"How about thriller movies? They can be quite engaging.\"},\n    {\"role\": \"user\", \"content\": \"I'm not a big fan of thriller movies but I love sci-fi movies.\"},\n    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n]\nm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n```\n\nYou can optionally set a custom base URL:\n\n```python\nos.environ[\"FORGE_API_BASE\"] = \"https://api.forge.tensorblock.co/v1\"\n```\n\n## Config\n\nAll available parameters for the `openai` config are present in [Master List of All Params in Config](../config).\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_83","type":"command_execution","command":"/bin/zsh -lc 'git show 55b387b0:docs/llms.txt | rg -n \"Forge|components/llms/models/forge|Supported LLMs\" -n --context 3'","aggregated_output":"90-- [Vector Database Overview](https://docs.mem0.ai/components/vectordbs/overview): Guide to supported vector databases for semantic memory storage\n91-- [Embeddings Overview](https://docs.mem0.ai/components/embedders/overview): Embedding model configuration for semantic understanding\n92-\n93:### Supported LLMs\n94-\n95-- [OpenAI](https://docs.mem0.ai/components/llms/models/openai): Integration with OpenAI models including GPT-4 and structured outputs\n96-- [Anthropic](https://docs.mem0.ai/components/llms/models/anthropic): Claude model integration with advanced reasoning capabilities\n--\n106-- [Sarvam](https://docs.mem0.ai/components/llms/models/sarvam): Indian language models\n107-- [XAI](https://docs.mem0.ai/components/llms/models/xai): xAI models integration\n108-- [LiteLLM](https://docs.mem0.ai/components/llms/models/litellm): Unified LLM interface and proxy\n109:- [Forge](https://docs.mem0.ai/components/llms/models/forge): LLM router with unified access to multiple AI providers\n110-- [LangChain](https://docs.mem0.ai/components/llms/models/langchain): LangChain LLM integration\n111-- [OpenAI Structured](https://docs.mem0.ai/components/llms/models/openai_structured): OpenAI with structured output support\n112-- [Azure OpenAI Structured](https://docs.mem0.ai/components/llms/models/azure_openai_structured): Azure OpenAI with structured outputs\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_84","type":"reasoning","text":"**Preparing line-numbered file views**"}}
{"type":"item.started","item":{"id":"item_85","type":"command_execution","command":"/bin/zsh -lc \"nl -ba CONTRIBUTING.md | sed -n '1,260p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_86","type":"command_execution","command":"/bin/zsh -lc \"nl -ba .github/PULL_REQUEST_TEMPLATE.md | sed -n '1,260p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_87","type":"command_execution","command":"/bin/zsh -lc \"nl -ba Makefile | sed -n '1,220p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.started","item":{"id":"item_88","type":"command_execution","command":"/bin/zsh -lc \"nl -ba pyproject.toml | sed -n '1,260p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_87","type":"command_execution","command":"/bin/zsh -lc \"nl -ba Makefile | sed -n '1,220p'\"","aggregated_output":"     1\t.PHONY: format sort lint\n     2\t\n     3\t# Variables\n     4\tISORT_OPTIONS = --profile black\n     5\tPROJECT_NAME := mem0ai\n     6\t\n     7\t# Default target\n     8\tall: format sort lint\n     9\t\n    10\tinstall:\n    11\t\thatch env create\n    12\t\n    13\tinstall_all:\n    14\t\tpip install ruff==0.6.9 groq together boto3 litellm ollama chromadb weaviate weaviate-client sentence_transformers vertexai \\\n    15\t\t            google-generativeai elasticsearch opensearch-py vecs \"pinecone<7.0.0\" pinecone-text faiss-cpu langchain-community \\\n    16\t\t\t\t\t\t\t\tupstash-vector azure-search-documents langchain-memgraph langchain-neo4j langchain-aws rank-bm25 pymochow pymongo psycopg kuzu databricks-sdk valkey\n    17\t\n    18\t# Format code with ruff\n    19\tformat:\n    20\t\thatch run format\n    21\t\n    22\t# Sort imports with isort\n    23\tsort:\n    24\t\thatch run isort mem0/\n    25\t\n    26\t# Lint code with ruff\n    27\tlint:\n    28\t\thatch run lint\n    29\t\n    30\tdocs:\n    31\t\tcd docs && mintlify dev\n    32\t\n    33\tbuild:\n    34\t\thatch build\n    35\t\n    36\tpublish:\n    37\t\thatch publish\n    38\t\n    39\tclean:\n    40\t\trm -rf dist\n    41\t\n    42\ttest:\n    43\t\thatch run test\n    44\t\n    45\ttest-py-3.9:\n    46\t\thatch run dev_py_3_9:test\n    47\t\n    48\ttest-py-3.10:\n    49\t\thatch run dev_py_3_10:test\n    50\t\n    51\ttest-py-3.11:\n    52\t\thatch run dev_py_3_11:test\n    53\t\n    54\ttest-py-3.12:\n    55\t\thatch run dev_py_3_12:test\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_86","type":"command_execution","command":"/bin/zsh -lc \"nl -ba .github/PULL_REQUEST_TEMPLATE.md | sed -n '1,260p'\"","aggregated_output":"     1\t## Description\n     2\t\n     3\tPlease include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change.\n     4\t\n     5\tFixes # (issue)\n     6\t\n     7\t## Type of change\n     8\t\n     9\tPlease delete options that are not relevant.\n    10\t\n    11\t- [ ] Bug fix (non-breaking change which fixes an issue)\n    12\t- [ ] New feature (non-breaking change which adds functionality)\n    13\t- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\n    14\t- [ ] Refactor (does not change functionality, e.g. code style improvements, linting)\n    15\t- [ ] Documentation update\n    16\t\n    17\t## How Has This Been Tested?\n    18\t\n    19\tPlease describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration\n    20\t\n    21\tPlease delete options that are not relevant.\n    22\t\n    23\t- [ ] Unit Test\n    24\t- [ ] Test Script (please provide)\n    25\t\n    26\t## Checklist:\n    27\t\n    28\t- [ ] My code follows the style guidelines of this project\n    29\t- [ ] I have performed a self-review of my own code\n    30\t- [ ] I have commented my code, particularly in hard-to-understand areas\n    31\t- [ ] I have made corresponding changes to the documentation\n    32\t- [ ] My changes generate no new warnings\n    33\t- [ ] I have added tests that prove my fix is effective or that my feature works\n    34\t- [ ] New and existing unit tests pass locally with my changes\n    35\t- [ ] Any dependent changes have been merged and published in downstream modules\n    36\t- [ ] I have checked my code and corrected any misspellings\n    37\t\n    38\t## Maintainer Checklist\n    39\t\n    40\t- [ ] closes #xxxx (Replace xxxx with the GitHub issue number)\n    41\t- [ ] Made sure Checks passed\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_85","type":"command_execution","command":"/bin/zsh -lc \"nl -ba CONTRIBUTING.md | sed -n '1,260p'\"","aggregated_output":"     1\t# Contributing to mem0\n     2\t\n     3\tLet us make contribution easy, collaborative and fun.\n     4\t\n     5\t## Submit your Contribution through PR\n     6\t\n     7\tTo make a contribution, follow these steps:\n     8\t\n     9\t1. Fork and clone this repository\n    10\t2. Do the changes on your fork with dedicated feature branch `feature/f1`\n    11\t3. If you modified the code (new feature or bug-fix), please add tests for it\n    12\t4. Include proper documentation / docstring and examples to run the feature\n    13\t5. Ensure that all tests pass\n    14\t6. Submit a pull request\n    15\t\n    16\tFor more details about pull requests, please read [GitHub's guides](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request).\n    17\t\n    18\t\n    19\t### üì¶ Development Environment\n    20\t\n    21\tWe use `hatch` for managing development environments. To set up:\n    22\t\n    23\t```bash\n    24\t# Activate environment for specific Python version:\n    25\thatch shell dev_py_3_9   # Python 3.9\n    26\thatch shell dev_py_3_10  # Python 3.10  \n    27\thatch shell dev_py_3_11  # Python 3.11\n    28\thatch shell dev_py_3_12  # Python 3.12\n    29\t\n    30\t# The environment will automatically install all dev dependencies\n    31\t# Run tests within the activated shell:\n    32\tmake test\n    33\t```\n    34\t\n    35\t### üìå Pre-commit\n    36\t\n    37\tTo ensure our standards, make sure to install pre-commit before starting to contribute.\n    38\t\n    39\t```bash\n    40\tpre-commit install\n    41\t```\n    42\t\n    43\t### üß™ Testing\n    44\t\n    45\tWe use `pytest` to test our code across multiple Python versions. You can run tests using:\n    46\t\n    47\t```bash\n    48\t# Run tests with default Python version\n    49\tmake test\n    50\t\n    51\t# Test specific Python versions:\n    52\tmake test-py-3.9   # Python 3.9 environment\n    53\tmake test-py-3.10  # Python 3.10 environment\n    54\tmake test-py-3.11  # Python 3.11 environment\n    55\tmake test-py-3.12  # Python 3.12 environment\n    56\t\n    57\t# When using hatch shells, run tests with:\n    58\tmake test  # After activating a shell with hatch shell test_XX\n    59\t```\n    60\t\n    61\tMake sure that all tests pass across all supported Python versions before submitting a pull request.\n    62\t\n    63\tWe look forward to your pull requests and can't wait to see your contributions!\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_88","type":"command_execution","command":"/bin/zsh -lc \"nl -ba pyproject.toml | sed -n '1,260p'\"","aggregated_output":"     1\t[build-system]\n     2\trequires = [\"hatchling\"]\n     3\tbuild-backend = \"hatchling.build\"\n     4\t\n     5\t[project]\n     6\tname = \"mem0ai\"\n     7\tversion = \"1.0.4\"\n     8\tdescription = \"Long-term memory for AI Agents\"\n     9\tauthors = [\n    10\t    { name = \"Mem0\", email = \"founders@mem0.ai\" }\n    11\t]\n    12\treadme = \"README.md\"\n    13\tlicense = \"Apache-2.0\"\n    14\tlicense-files = [\"LICENSE\"]\n    15\trequires-python = \">=3.9,<4.0\"\n    16\tdependencies = [\n    17\t    \"qdrant-client>=1.9.1\",\n    18\t    \"pydantic>=2.7.3\",\n    19\t    \"openai>=1.90.0\",\n    20\t    \"posthog>=3.5.0\",\n    21\t    \"pytz>=2024.1\",\n    22\t    \"sqlalchemy>=2.0.31\",\n    23\t    \"protobuf>=5.29.0,<6.0.0\",\n    24\t]\n    25\t\n    26\t[project.optional-dependencies]\n    27\tgraph = [\n    28\t    \"langchain-neo4j>=0.4.0\",\n    29\t    \"langchain-aws>=0.2.23\",\n    30\t    \"langchain-memgraph>=0.1.0\",\n    31\t    \"neo4j>=5.23.1\",\n    32\t    \"rank-bm25>=0.2.2\",\n    33\t    \"kuzu>=0.11.0\",\n    34\t]\n    35\tvector_stores = [\n    36\t    \"vecs>=0.4.0\",\n    37\t    \"chromadb>=0.4.24\",\n    38\t    \"cassandra-driver>=3.29.0\",\n    39\t    \"weaviate-client>=4.4.0,<4.15.0\",\n    40\t    \"pinecone<=7.3.0\",\n    41\t    \"pinecone-text>=0.10.0\",\n    42\t    \"faiss-cpu>=1.7.4\",\n    43\t    \"upstash-vector>=0.1.0\",\n    44\t    \"azure-search-documents>=11.4.0b8\",\n    45\t    \"psycopg>=3.2.8\",\n    46\t    \"psycopg-pool>=3.2.6,<4.0.0\",\n    47\t    \"pymongo>=4.13.2\",\n    48\t    \"pymochow>=2.2.9\",\n    49\t    \"pymysql>=1.1.0\",\n    50\t    \"dbutils>=3.0.3\",\n    51\t    \"valkey>=6.0.0\",\n    52\t    \"databricks-sdk>=0.63.0\",\n    53\t    \"azure-identity>=1.24.0\",\n    54\t    \"redis>=5.0.0,<6.0.0\",\n    55\t    \"redisvl>=0.1.0,<1.0.0\",\n    56\t    \"elasticsearch>=8.0.0,<9.0.0\",\n    57\t    \"pymilvus>=2.4.0,<2.6.0\",\n    58\t    \"langchain-aws>=0.2.23\",\n    59\t]\n    60\tllms = [\n    61\t    \"groq>=0.3.0\",\n    62\t    \"together>=0.2.10\",\n    63\t    \"litellm>=1.74.0\",\n    64\t    \"openai>=1.90.0\",\n    65\t    \"ollama>=0.1.0\",\n    66\t    \"vertexai>=0.1.0\",\n    67\t    \"google-generativeai>=0.3.0\",\n    68\t    \"google-genai>=1.0.0\",\n    69\t]\n    70\textras = [\n    71\t    \"boto3>=1.34.0\",\n    72\t    \"langchain-community>=0.0.0\",\n    73\t    \"sentence-transformers>=5.0.0\",\n    74\t    \"elasticsearch>=8.0.0,<9.0.0\",\n    75\t    \"opensearch-py>=2.0.0\",\n    76\t    \"fastembed>=0.3.1\",\n    77\t]\n    78\ttest = [\n    79\t    \"pytest>=8.2.2\",\n    80\t    \"pytest-mock>=3.14.0\",\n    81\t    \"pytest-asyncio>=0.23.7\",\n    82\t]\n    83\tdev = [\n    84\t    \"ruff>=0.6.5\",\n    85\t    \"isort>=5.13.2\",\n    86\t    \"pytest>=8.2.2\",\n    87\t]\n    88\t\n    89\t[tool.pytest.ini_options]\n    90\tpythonpath = [\".\"]\n    91\t\n    92\t[tool.hatch.build]\n    93\tinclude = [\n    94\t    \"mem0/**/*.py\",\n    95\t]\n    96\texclude = [\n    97\t    \"**/*\",\n    98\t    \"!mem0/**/*.py\",\n    99\t]\n   100\t\n   101\t[tool.hatch.build.targets.wheel]\n   102\tpackages = [\"mem0\"]\n   103\tonly-include = [\"mem0\"]\n   104\t\n   105\t[tool.hatch.build.targets.wheel.shared-data]\n   106\t\"README.md\" = \"README.md\"\n   107\t\n   108\t[tool.hatch.envs.dev_py_3_9]\n   109\tpython = \"3.9\"\n   110\tfeatures = [\n   111\t  \"test\",\n   112\t  \"graph\",\n   113\t  \"vector_stores\",\n   114\t  \"llms\",\n   115\t  \"extras\",\n   116\t]\n   117\t\n   118\t[tool.hatch.envs.dev_py_3_10]\n   119\tpython = \"3.10\"\n   120\tfeatures = [\n   121\t  \"test\",\n   122\t  \"graph\",\n   123\t  \"vector_stores\",\n   124\t  \"llms\",\n   125\t  \"extras\",\n   126\t]\n   127\t\n   128\t[tool.hatch.envs.dev_py_3_11]\n   129\tpython = \"3.11\"\n   130\tfeatures = [\n   131\t  \"test\",\n   132\t  \"graph\",\n   133\t  \"vector_stores\",\n   134\t  \"llms\",\n   135\t  \"extras\",\n   136\t]\n   137\t\n   138\t[tool.hatch.envs.dev_py_3_12]\n   139\tpython = \"3.12\"\n   140\tfeatures = [\n   141\t  \"test\",\n   142\t  \"graph\",\n   143\t  \"vector_stores\",\n   144\t  \"llms\",\n   145\t  \"extras\",\n   146\t]\n   147\t\n   148\t[tool.hatch.envs.default.scripts]\n   149\tformat = [\n   150\t    \"ruff format\",\n   151\t]\n   152\tformat-check = [\n   153\t    \"ruff format --check\",\n   154\t]\n   155\tlint = [\n   156\t    \"ruff check\",\n   157\t]\n   158\tlint-fix = [\n   159\t    \"ruff check --fix\",\n   160\t]\n   161\ttest = [\n   162\t    \"pytest tests/ {args}\",\n   163\t]\n   164\t\n   165\t[tool.ruff]\n   166\tline-length = 120\n   167\texclude = [\"embedchain/\", \"openmemory/\"]\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_89","type":"reasoning","text":"**Checking docs line references**"}}
{"type":"item.started","item":{"id":"item_90","type":"command_execution","command":"/bin/zsh -lc \"nl -ba docs/docs.json | sed -n '160,230p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_90","type":"command_execution","command":"/bin/zsh -lc \"nl -ba docs/docs.json | sed -n '160,230p'\"","aggregated_output":"   160\t                      \"open-source/features/custom-update-memory-prompt\",\n   161\t                      \"open-source/features/rest-api\",\n   162\t                      \"open-source/features/openai_compatibility\"\n   163\t                    ]\n   164\t                  },\n   165\t                  {\n   166\t                    \"group\": \"Configuration\",\n   167\t                    \"icon\": \"sliders\",\n   168\t                    \"pages\": [\n   169\t                      \"open-source/configuration\",\n   170\t                      {\n   171\t                        \"group\": \"LLMs\",\n   172\t                        \"icon\": \"message-bot\",\n   173\t                        \"pages\": [\n   174\t                          \"components/llms/overview\",\n   175\t                          \"components/llms/config\",\n   176\t                          {\n   177\t                            \"group\": \"Supported LLMs\",\n   178\t                            \"icon\": \"list\",\n   179\t                            \"pages\": [\n   180\t                              \"components/llms/models/openai\",\n   181\t                              \"components/llms/models/anthropic\",\n   182\t                              \"components/llms/models/azure_openai\",\n   183\t                              \"components/llms/models/ollama\",\n   184\t                              \"components/llms/models/together\",\n   185\t                              \"components/llms/models/groq\",\n   186\t                              \"components/llms/models/litellm\",\n   187\t                              \"components/llms/models/mistral_AI\",\n   188\t                              \"components/llms/models/google_AI\",\n   189\t                              \"components/llms/models/aws_bedrock\",\n   190\t                              \"components/llms/models/deepseek\",\n   191\t                              \"components/llms/models/xAI\",\n   192\t                              \"components/llms/models/sarvam\",\n   193\t                              \"components/llms/models/lmstudio\",\n   194\t                              \"components/llms/models/langchain\",\n   195\t                              \"components/llms/models/vllm\"\n   196\t                            ]\n   197\t                          }\n   198\t                        ]\n   199\t                      },\n   200\t                      {\n   201\t                        \"group\": \"Vector Databases\",\n   202\t                        \"icon\": \"hard-drive\",\n   203\t                        \"pages\": [\n   204\t                          \"components/vectordbs/overview\",\n   205\t                          \"components/vectordbs/config\",\n   206\t                          {\n   207\t                            \"group\": \"Supported Vector Databases\",\n   208\t                            \"icon\": \"list\",\n   209\t                            \"pages\": [\n   210\t                              \"components/vectordbs/dbs/qdrant\",\n   211\t                              \"components/vectordbs/dbs/chroma\",\n   212\t                              \"components/vectordbs/dbs/pgvector\",\n   213\t                              \"components/vectordbs/dbs/milvus\",\n   214\t                              \"components/vectordbs/dbs/pinecone\",\n   215\t                              \"components/vectordbs/dbs/mongodb\",\n   216\t                              \"components/vectordbs/dbs/azure\",\n   217\t                              \"components/vectordbs/dbs/azure_mysql\",\n   218\t                              \"components/vectordbs/dbs/redis\",\n   219\t                              \"components/vectordbs/dbs/valkey\",\n   220\t                              \"components/vectordbs/dbs/elasticsearch\",\n   221\t                              \"components/vectordbs/dbs/opensearch\",\n   222\t                              \"components/vectordbs/dbs/supabase\",\n   223\t                              \"components/vectordbs/dbs/upstash-vector\",\n   224\t                              \"components/vectordbs/dbs/vectorize\",\n   225\t                              \"components/vectordbs/dbs/vertex_ai\",\n   226\t                              \"components/vectordbs/dbs/weaviate\",\n   227\t                              \"components/vectordbs/dbs/faiss\",\n   228\t                              \"components/vectordbs/dbs/langchain\",\n   229\t                              \"components/vectordbs/dbs/baidu\",\n   230\t                              \"components/vectordbs/dbs/cassandra\",\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_91","type":"reasoning","text":"**Planning line numbers for docs**"}}
{"type":"item.started","item":{"id":"item_92","type":"command_execution","command":"/bin/zsh -lc \"nl -ba docs/llms.txt | sed -n '130,220p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_92","type":"command_execution","command":"/bin/zsh -lc \"nl -ba docs/llms.txt | sed -n '130,220p'\"","aggregated_output":"   130\t- [Databricks](https://docs.mem0.ai/components/vectordbs/dbs/databricks): Delta Lake integration for vector search\n   131\t- [Baidu](https://docs.mem0.ai/components/vectordbs/dbs/baidu): Baidu vector database integration\n   132\t- [LangChain](https://docs.mem0.ai/components/vectordbs/dbs/langchain): LangChain vector store integration\n   133\t- [S3 Vectors](https://docs.mem0.ai/components/vectordbs/dbs/s3_vectors): Amazon S3 Vectors integration\n   134\t\n   135\t### Supported Embeddings\n   136\t\n   137\t- [OpenAI Embeddings](https://docs.mem0.ai/components/embedders/models/openai): High-quality text embeddings with customizable dimensions\n   138\t- [Azure OpenAI Embeddings](https://docs.mem0.ai/components/embedders/models/azure_openai): Enterprise Azure-hosted embedding models\n   139\t- [Google AI](https://docs.mem0.ai/components/embedders/models/google_ai): Gemini embedding models\n   140\t- [AWS Bedrock](https://docs.mem0.ai/components/embedders/models/aws_bedrock): Amazon embedding models through Bedrock\n   141\t- [Hugging Face](https://docs.mem0.ai/components/embedders/models/hugging_face): Open-source embedding models for local deployment\n   142\t- [Vertex AI](https://docs.mem0.ai/components/embedders/models/vertexai): Google Cloud's enterprise embedding models\n   143\t- [Ollama](https://docs.mem0.ai/components/embedders/models/ollama): Local embedding models for privacy-focused applications\n   144\t- [Together](https://docs.mem0.ai/components/embedders/models/together): Open-source model embeddings\n   145\t- [LM Studio](https://docs.mem0.ai/components/embedders/models/lmstudio): Local model embeddings\n   146\t- [LangChain](https://docs.mem0.ai/components/embedders/models/langchain): LangChain embedder integration\n   147\t\n   148\t## Integrations\n   149\t\n   150\t- [LangChain](https://docs.mem0.ai/integrations/langchain): Seamless integration with LangChain framework for enhanced agent capabilities\n   151\t- [LangGraph](https://docs.mem0.ai/integrations/langgraph): Build stateful, multi-actor applications with persistent memory\n   152\t- [LlamaIndex](https://docs.mem0.ai/integrations/llama-index): Enhanced RAG applications with intelligent memory layer\n   153\t- [CrewAI](https://docs.mem0.ai/integrations/crewai): Multi-agent systems with shared and individual memory capabilities\n   154\t- [AutoGen](https://docs.mem0.ai/integrations/autogen): Microsoft's multi-agent conversation framework with memory\n   155\t- [Vercel AI SDK](https://docs.mem0.ai/integrations/vercel-ai-sdk): Build AI-powered web applications with persistent memory\n   156\t- [Flowise](https://docs.mem0.ai/integrations/flowise): No-code LLM workflow builder with memory capabilities\n   157\t- [Dify](https://docs.mem0.ai/integrations/dify): LLMOps platform integration for production AI applications\n   158\t\n   159\t## Cookbooks and Examples\n   160\t\n   161\t### Cookbooks Overview\n   162\t- [Cookbooks Overview](https://docs.mem0.ai/cookbooks/overview): Complete guide to Mem0 examples and implementation patterns\n   163\t\n   164\t### Essential Guides\n   165\t- [Building AI Companion](https://docs.mem0.ai/cookbooks/essentials/building-ai-companion): Core patterns for building AI agents with memory\n   166\t- [Partition Memories by Entity](https://docs.mem0.ai/cookbooks/essentials/entity-partitioning-playbook): Keep multi-tenant assistants isolated by tagging user, agent, app, and session identifiers\n   167\t- [Controlling Memory Ingestion](https://docs.mem0.ai/cookbooks/essentials/controlling-memory-ingestion): Fine-tune what gets stored in memory and when\n   168\t- [Memory Expiration](https://docs.mem0.ai/cookbooks/essentials/memory-expiration-short-and-long-term): Implement short-term and long-term memory strategies\n   169\t- [Tagging and Organizing Memories](https://docs.mem0.ai/cookbooks/essentials/tagging-and-organizing-memories): Advanced memory organization and categorization\n   170\t- [Exporting Memories](https://docs.mem0.ai/cookbooks/essentials/exporting-memories): Backup and transfer memory data between systems\n   171\t- [Choosing Memory Architecture](https://docs.mem0.ai/cookbooks/essentials/choosing-memory-architecture-vector-vs-graph): Vector vs Graph memory architectures comparison\n   172\t\n   173\t### AI Companion Examples\n   174\t- [AI Tutor](https://docs.mem0.ai/cookbooks/companions/ai-tutor): Educational AI that adapts to learning progress\n   175\t- [Travel Assistant](https://docs.mem0.ai/cookbooks/companions/travel-assistant): Travel planning agent that learns preferences\n   176\t- [Voice Companion](https://docs.mem0.ai/cookbooks/companions/voice-companion-openai): Voice-enabled AI with conversational memory\n   177\t- [Local Companion](https://docs.mem0.ai/cookbooks/companions/local-companion-ollama): Privacy-focused companion using local models\n   178\t- [Node.js Companion](https://docs.mem0.ai/cookbooks/companions/nodejs-companion): JavaScript-based AI companion applications\n   179\t- [YouTube Research Assistant](https://docs.mem0.ai/cookbooks/companions/youtube-research): AI that researches and learns from video content\n   180\t\n   181\t### Operations & Automation\n   182\t- [Support Inbox](https://docs.mem0.ai/cookbooks/operations/support-inbox): Customer service agents with conversation history\n   183\t- [Email Automation](https://docs.mem0.ai/cookbooks/operations/email-automation): Smart email processing with contextual memory\n   184\t- [Content Writing](https://docs.mem0.ai/cookbooks/operations/content-writing): AI writers that maintain brand voice and style\n   185\t- [Deep Research](https://docs.mem0.ai/cookbooks/operations/deep-research): Research assistants that build on previous findings\n   186\t- [Team Task Agent](https://docs.mem0.ai/cookbooks/operations/team-task-agent): Collaborative AI agents with shared project memory\n   187\t\n   188\t### Integration Examples\n   189\t- [OpenAI Tool Calls](https://docs.mem0.ai/cookbooks/integrations/openai-tool-calls): Mem0 integrated with OpenAI function calling\n   190\t- [AWS Bedrock](https://docs.mem0.ai/cookbooks/integrations/aws-bedrock): Enterprise memory with AWS managed services\n   191\t- [Tavily Search](https://docs.mem0.ai/cookbooks/integrations/tavily-search): Web search with persistent memory of results\n   192\t- [Healthcare Google ADK](https://docs.mem0.ai/cookbooks/integrations/healthcare-google-adk): Medical AI applications with memory\n   193\t- [Mastra Agent](https://docs.mem0.ai/cookbooks/integrations/mastra-agent): Mastra framework integration with memory\n   194\t\n   195\t### Framework Examples\n   196\t- [LlamaIndex React](https://docs.mem0.ai/cookbooks/frameworks/llamaindex-react): React applications with LlamaIndex and memory\n   197\t- [LlamaIndex Multiagent](https://docs.mem0.ai/cookbooks/frameworks/llamaindex-multiagent): Multi-agent systems with shared memory\n   198\t- [Eliza OS Character](https://docs.mem0.ai/cookbooks/frameworks/eliza-os-character): Character-based AI with persistent personality\n   199\t- [Chrome Extension](https://docs.mem0.ai/cookbooks/frameworks/chrome-extension): Browser extensions that remember user interactions\n   200\t- [Multimodal Retrieval](https://docs.mem0.ai/cookbooks/frameworks/multimodal-retrieval): Memory systems handling text, images, and documents\n   201\t\n   202\t## API Reference\n   203\t\n   204\t- [Memory APIs](https://docs.mem0.ai/api-reference/memory/add-memories): Comprehensive API documentation for memory operations\n   205\t- [Add Memories](https://docs.mem0.ai/api-reference/memory/add-memories): REST API for storing new memories with detailed request/response formats\n   206\t- [Search Memories](https://docs.mem0.ai/api-reference/memory/search-memories): Advanced search API with filtering and ranking capabilities\n   207\t- [Get All Memories](https://docs.mem0.ai/api-reference/memory/get-memories): Retrieve all memories with pagination and filtering options\n   208\t- [Update Memory](https://docs.mem0.ai/api-reference/memory/update-memory): Modify existing memories with conflict resolution\n   209\t- [Delete Memory](https://docs.mem0.ai/api-reference/memory/delete-memory): Remove memories individually or in batches\n   210\t\n   211\t## Optional\n   212\t\n   213\t- [FAQs](https://docs.mem0.ai/platform/faqs): Frequently asked questions about Mem0's Platform capabilities and implementation details\n   214\t- [Changelog](https://docs.mem0.ai/changelog): Detailed product updates and version history for tracking new features and improvements\n   215\t- [Contributing Guide](https://docs.mem0.ai/contributing/development): Guidelines for contributing to Mem0's open-source development\n   216\t- [OpenMemory](https://docs.mem0.ai/openmemory/overview): Open-source memory infrastructure for research and experimentation\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_93","type":"reasoning","text":"**Inspecting supported LLMs code section**"}}
{"type":"item.started","item":{"id":"item_94","type":"command_execution","command":"/bin/zsh -lc \"nl -ba docs/llms.txt | sed -n '88,124p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_94","type":"command_execution","command":"/bin/zsh -lc \"nl -ba docs/llms.txt | sed -n '88,124p'\"","aggregated_output":"    88\t\n    89\t- [LLM Overview](https://docs.mem0.ai/components/llms/overview): Comprehensive guide to Large Language Model integration and configuration options\n    90\t- [Vector Database Overview](https://docs.mem0.ai/components/vectordbs/overview): Guide to supported vector databases for semantic memory storage\n    91\t- [Embeddings Overview](https://docs.mem0.ai/components/embedders/overview): Embedding model configuration for semantic understanding\n    92\t\n    93\t### Supported LLMs\n    94\t\n    95\t- [OpenAI](https://docs.mem0.ai/components/llms/models/openai): Integration with OpenAI models including GPT-4 and structured outputs\n    96\t- [Anthropic](https://docs.mem0.ai/components/llms/models/anthropic): Claude model integration with advanced reasoning capabilities\n    97\t- [Google AI](https://docs.mem0.ai/components/llms/models/google_AI): Gemini model integration for multimodal applications\n    98\t- [Groq](https://docs.mem0.ai/components/llms/models/groq): High-performance LPU optimized models for fast inference\n    99\t- [AWS Bedrock](https://docs.mem0.ai/components/llms/models/aws_bedrock): Enterprise-grade AWS managed model integration\n   100\t- [Azure OpenAI](https://docs.mem0.ai/components/llms/models/azure_openai): Microsoft Azure hosted OpenAI models for enterprise environments\n   101\t- [Ollama](https://docs.mem0.ai/components/llms/models/ollama): Local model deployment for privacy-focused applications\n   102\t- [vLLM](https://docs.mem0.ai/components/llms/models/vllm): High-performance inference framework\n   103\t- [LM Studio](https://docs.mem0.ai/components/llms/models/lmstudio): Local model management and deployment\n   104\t- [Together](https://docs.mem0.ai/components/llms/models/together): Open-source model inference platform\n   105\t- [DeepSeek](https://docs.mem0.ai/components/llms/models/deepseek): Advanced reasoning models\n   106\t- [Sarvam](https://docs.mem0.ai/components/llms/models/sarvam): Indian language models\n   107\t- [XAI](https://docs.mem0.ai/components/llms/models/xai): xAI models integration\n   108\t- [LiteLLM](https://docs.mem0.ai/components/llms/models/litellm): Unified LLM interface and proxy\n   109\t- [LangChain](https://docs.mem0.ai/components/llms/models/langchain): LangChain LLM integration\n   110\t- [OpenAI Structured](https://docs.mem0.ai/components/llms/models/openai_structured): OpenAI with structured output support\n   111\t- [Azure OpenAI Structured](https://docs.mem0.ai/components/llms/models/azure_openai_structured): Azure OpenAI with structured outputs\n   112\t\n   113\t### Supported Vector Databases\n   114\t\n   115\t- [Qdrant](https://docs.mem0.ai/components/vectordbs/dbs/qdrant): High-performance vector similarity search engine\n   116\t- [Pinecone](https://docs.mem0.ai/components/vectordbs/dbs/pinecone): Managed vector database with serverless and pod deployment options\n   117\t- [Chroma](https://docs.mem0.ai/components/vectordbs/dbs/chroma): AI-native open-source vector database optimized for speed\n   118\t- [Weaviate](https://docs.mem0.ai/components/vectordbs/dbs/weaviate): Open-source vector search engine with built-in ML capabilities\n   119\t- [PGVector](https://docs.mem0.ai/components/vectordbs/dbs/pgvector): PostgreSQL extension for vector similarity search\n   120\t- [Milvus](https://docs.mem0.ai/components/vectordbs/dbs/milvus): Open-source vector database for AI applications at scale\n   121\t- [Redis](https://docs.mem0.ai/components/vectordbs/dbs/redis): Real-time vector storage and search with Redis Stack\n   122\t- [Supabase](https://docs.mem0.ai/components/vectordbs/dbs/supabase): Open-source Firebase alternative with vector support\n   123\t- [Upstash Vector](https://docs.mem0.ai/components/vectordbs/dbs/upstash_vector): Serverless vector database\n   124\t- [Elasticsearch](https://docs.mem0.ai/components/vectordbs/dbs/elasticsearch): Distributed search and analytics engine\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_95","type":"reasoning","text":"**Identifying line numbers for component**"}}
{"type":"item.started","item":{"id":"item_96","type":"command_execution","command":"/bin/zsh -lc \"nl -ba docs/components/llms/overview.mdx | sed -n '1,200p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_96","type":"command_execution","command":"/bin/zsh -lc \"nl -ba docs/components/llms/overview.mdx | sed -n '1,200p'\"","aggregated_output":"     1\t---\n     2\ttitle: Overview\n     3\t---\n     4\t\n     5\tMem0 includes built-in support for various popular large language models. Memory can utilize the LLM provided by the user, ensuring efficient use for specific needs.\n     6\t\n     7\t## Usage\n     8\t\n     9\tTo use a llm, you must provide a configuration to customize its usage. If no configuration is supplied, a default configuration will be applied, and `OpenAI` will be used as the llm.\n    10\t\n    11\tFor a comprehensive list of available parameters for llm configuration, please refer to [Config](./config).\n    12\t\n    13\t## Supported LLMs\n    14\t\n    15\tSee the list of supported LLMs below.\n    16\t\n    17\t<Note>\n    18\t  All LLMs are supported in Python. The following LLMs are also supported in TypeScript: **OpenAI**, **Anthropic**, and **Groq**.\n    19\t</Note>\n    20\t\n    21\t<CardGroup cols={4}>\n    22\t  <Card title=\"OpenAI\" href=\"/components/llms/models/openai\" />\n    23\t  <Card title=\"Ollama\" href=\"/components/llms/models/ollama\" />\n    24\t  <Card title=\"Azure OpenAI\" href=\"/components/llms/models/azure_openai\" />\n    25\t  <Card title=\"Anthropic\" href=\"/components/llms/models/anthropic\" />\n    26\t  <Card title=\"Together\" href=\"/components/llms/models/together\" />\n    27\t  <Card title=\"Groq\" href=\"/components/llms/models/groq\" />\n    28\t  <Card title=\"Litellm\" href=\"/components/llms/models/litellm\" />\n    29\t  <Card title=\"Mistral AI\" href=\"/components/llms/models/mistral_AI\" />\n    30\t  <Card title=\"Google AI\" href=\"/components/llms/models/google_AI\" />\n    31\t  <Card title=\"AWS bedrock\" href=\"/components/llms/models/aws_bedrock\" />\n    32\t  <Card title=\"DeepSeek\" href=\"/components/llms/models/deepseek\" />\n    33\t  <Card title=\"xAI\" href=\"/components/llms/models/xAI\" />\n    34\t  <Card title=\"Sarvam AI\" href=\"/components/llms/models/sarvam\" />\n    35\t  <Card title=\"LM Studio\" href=\"/components/llms/models/lmstudio\" />\n    36\t  <Card title=\"Langchain\" href=\"/components/llms/models/langchain\" />\n    37\t</CardGroup>\n    38\t\n    39\t## Structured vs Unstructured Outputs\n    40\t\n    41\tMem0 supports two types of OpenAI LLM formats, each with its own strengths and use cases:\n    42\t\n    43\t### Structured Outputs\n    44\t\n    45\tStructured outputs are LLMs that align with OpenAI's structured outputs model:\n    46\t\n    47\t- **Optimized for:** Returning structured responses (e.g., JSON objects)\n    48\t- **Benefits:** Precise, easily parseable data\n    49\t- **Ideal for:** Data extraction, form filling, API responses\n    50\t- **Learn more:** [OpenAI Structured Outputs Guide](https://platform.openai.com/docs/guides/structured-outputs/introduction)\n    51\t\n    52\t### Unstructured Outputs\n    53\t\n    54\tUnstructured outputs correspond to OpenAI's standard, free-form text model:\n    55\t\n    56\t- **Flexibility:** Returns open-ended, natural language responses\n    57\t- **Customization:** Use the `response_format` parameter to guide output\n    58\t- **Trade-off:** Less efficient than structured outputs for specific data needs\n    59\t- **Best for:** Creative writing, explanations, general conversation\n    60\t\n    61\tChoose the format that best suits your application's requirements for optimal performance and usability.\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_97","type":"reasoning","text":"**Considering line numbers for docs**"}}
{"type":"item.started","item":{"id":"item_98","type":"command_execution","command":"/bin/zsh -lc \"nl -ba docs/components/llms/models/openai.mdx | sed -n '1,240p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_98","type":"command_execution","command":"/bin/zsh -lc \"nl -ba docs/components/llms/models/openai.mdx | sed -n '1,240p'\"","aggregated_output":"     1\t---\n     2\ttitle: OpenAI\n     3\t---\n     4\t\n     5\tTo use OpenAI LLM models, you have to set the `OPENAI_API_KEY` environment variable. You can obtain the OpenAI API key from the [OpenAI Platform](https://platform.openai.com/account/api-keys).\n     6\t\n     7\t> **Note**: The following are currently unsupported with reasoning models `Parallel tool calling`,`temperature`, `top_p`, `presence_penalty`, `frequency_penalty`, `logprobs`, `top_logprobs`, `logit_bias`, `max_tokens`\n     8\t\n     9\t## Usage\n    10\t\n    11\t<CodeGroup>\n    12\t```python Python\n    13\timport os\n    14\tfrom mem0 import Memory\n    15\t\n    16\tos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n    17\t\n    18\tconfig = {\n    19\t    \"llm\": {\n    20\t        \"provider\": \"openai\",\n    21\t        \"config\": {\n    22\t            \"model\": \"gpt-4.1-nano-2025-04-14\",\n    23\t            \"temperature\": 0.2,\n    24\t            \"max_tokens\": 2000,\n    25\t        }\n    26\t    }\n    27\t}\n    28\t\n    29\t# Use Openrouter by passing it's api key\n    30\t# os.environ[\"OPENROUTER_API_KEY\"] = \"your-api-key\"\n    31\t# config = {\n    32\t#    \"llm\": {\n    33\t#        \"provider\": \"openai\",\n    34\t#        \"config\": {\n    35\t#            \"model\": \"meta-llama/llama-3.1-70b-instruct\",\n    36\t#        }\n    37\t#    }\n    38\t# }\n    39\t\n    40\tm = Memory.from_config(config)\n    41\tmessages = [\n    42\t    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    43\t    {\"role\": \"assistant\", \"content\": \"How about thriller movies? They can be quite engaging.\"},\n    44\t    {\"role\": \"user\", \"content\": \"I‚Äôm not a big fan of thriller movies but I love sci-fi movies.\"},\n    45\t    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n    46\t]\n    47\tm.add(messages, user_id=\"alice\", metadata={\"category\": \"movies\"})\n    48\t```\n    49\t\n    50\t```typescript TypeScript\n    51\timport { Memory } from 'mem0ai/oss';\n    52\t\n    53\tconst config = {\n    54\t  llm: {\n    55\t    provider: 'openai',\n    56\t    config: {\n    57\t      apiKey: process.env.OPENAI_API_KEY || '',\n    58\t      model: 'gpt-4-turbo-preview',\n    59\t      temperature: 0.2,\n    60\t      maxTokens: 1500,\n    61\t    },\n    62\t  },\n    63\t};\n    64\t\n    65\tconst memory = new Memory(config);\n    66\tconst messages = [\n    67\t    {\"role\": \"user\", \"content\": \"I'm planning to watch a movie tonight. Any recommendations?\"},\n    68\t    {\"role\": \"assistant\", \"content\": \"How about thriller movies? They can be quite engaging.\"},\n    69\t    {\"role\": \"user\", \"content\": \"I‚Äôm not a big fan of thriller movies but I love sci-fi movies.\"},\n    70\t    {\"role\": \"assistant\", \"content\": \"Got it! I'll avoid thriller recommendations and suggest sci-fi movies in the future.\"}\n    71\t]\n    72\tawait memory.add(messages, { userId: \"alice\", metadata: { category: \"movies\" } });\n    73\t```\n    74\t</CodeGroup>\n    75\t\n    76\tWe also support the new [OpenAI structured-outputs](https://platform.openai.com/docs/guides/structured-outputs/introduction) model.\n    77\t\n    78\t```python\n    79\timport os\n    80\tfrom mem0 import Memory\n    81\t\n    82\tos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n    83\t\n    84\tconfig = {\n    85\t    \"llm\": {\n    86\t        \"provider\": \"openai_structured\",\n    87\t        \"config\": {\n    88\t            \"model\": \"gpt-4.1-nano-2025-04-14\",\n    89\t            \"temperature\": 0.0,\n    90\t        }\n    91\t    }\n    92\t}\n    93\t\n    94\tm = Memory.from_config(config)\n    95\t```\n    96\t\n    97\t## Config\n    98\t\n    99\tAll available parameters for the `openai` config are present in [Master List of All Params in Config](../config).\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_99","type":"reasoning","text":"**Summarizing discovery and blockers**"}}
{"type":"item.completed","item":{"id":"item_100","type":"agent_message","text":"Preflight scan is complete and I‚Äôm assembling the contract now. I‚Äôll mark this stage as `NEEDS REVIEW` because two mandatory inputs are outside the repository and blocked by your safety contract (`forge_integration/*` and the skill‚Äôs external schema references), then include a repo-only minimal-diff plan (4 files, <=90 added lines) plus exact CI-derived validation commands."}}
{"type":"item.completed","item":{"id":"item_101","type":"reasoning","text":"**Crafting comprehensive final JSON**"}}
{"type":"item.completed","item":{"id":"item_102","type":"agent_message","text":"{\n  \"status\": \"NEEDS REVIEW\",\n  \"files_changed\": [],\n  \"test_results\": {\n    \"executed\": [],\n    \"required_commands\": [\n      {\n        \"command\": \"pip install --upgrade pip && pip install -e \\\".[test,graph,vector_stores,llms,extras]\\\" && pip install ruff\",\n        \"source\": \"/Users/yi/Documents/Career/TensorBlcok/agentpr/workspaces/mem0/.github/workflows/ci.yml:68-73\",\n        \"outcome\": \"not run (DISCOVERY stage)\"\n      },\n      {\n        \"command\": \"make lint\",\n        \"source\": \"/Users/yi/Documents/Career/TensorBlcok/agentpr/workspaces/mem0/.github/workflows/ci.yml:74-75\",\n        \"outcome\": \"not run (DISCOVERY stage)\"\n      },\n      {\n        \"command\": \"make test\",\n        \"source\": \"/Users/yi/Documents/Career/TensorBlcok/agentpr/workspaces/mem0/.github/workflows/ci.yml:76-77\",\n        \"outcome\": \"not run (DISCOVERY stage)\"\n      }\n    ],\n    \"outcome\": \"No lint/test executed in discovery.\"\n  },\n  \"notes\": \"Preflight contract completed from in-repo evidence only. NEEDS REVIEW because mandatory non-repo inputs were blocked by policy (cannot read /Users/yi/Documents/Career/TensorBlcok/agentpr/forge_integration/* and external skill reference schema files). Next action: approve repo-only contract or provide in-repo copies of those files, then proceed to implementation.\",\n  \"run_context\": {\n    \"run_id\": \"calib_mem0_20260225_r3\",\n    \"stage\": \"DISCOVERY\",\n    \"required_skill\": \"agentpr-repo-preflight-contract\",\n    \"base_branch\": {\n      \"value\": \"main\",\n      \"evidence\": [\n        \"/Users/yi/Documents/Career/TensorBlcok/agentpr/workspaces/mem0/.github/workflows/ci.yml:5\",\n        \"git symbolic-ref refs/remotes/origin/HEAD -> refs/remotes/origin/main\"\n      ]\n    }\n  },\n  \"governance\": {\n    \"agents_md\": {\n      \"found_in_repo\": false\n    },\n    \"contributing_files\": [\n      \"/Users/yi/Documents/Career/TensorBlcok/agentpr/workspaces/mem0/CONTRIBUTING.md\",\n      \"/Users/yi/Documents/Career/TensorBlcok/agentpr/workspaces/mem0/embedchain/CONTRIBUTING.md\",\n      \"/Users/yi/Documents/Career/TensorBlcok/agentpr/workspaces/mem0/openmemory/CONTRIBUTING.md\"\n    ],\n    \"pr_template_files\": [\n      \"/Users/yi/Documents/Career/TensorBlcok/agentpr/workspaces/mem0/.github/PULL_REQUEST_TEMPLATE.md\"\n    ],\n    \"codeowners_files\": [],\n    \"rules\": {\n      \"branch_convention\": \"Dedicated feature branch suggested (feature/f1) in CONTRIBUTING.\",\n      \"tests_required_for_code_changes\": true,\n      \"docs_required_for_feature_or_bugfix\": true,\n      \"pr_checklist_requires_docs_and_tests\": true,\n      \"commit_message_convention_found\": false\n    },\n    \"evidence\": [\n      \"/Users/yi/Documents/Career/TensorBlcok/agentpr/workspaces/mem0/CONTRIBUTING.md:10-14\",\n      \"/Users/yi/Documents/Career/TensorBlcok/agentpr/workspaces/mem0/.github/PULL_REQUEST_TEMPLATE.md:26-35\"\n    ]\n  },\n  \"ci_contract\": {\n    \"workflows_scanned\": [\n      \"/Users/yi/Documents/Career/TensorBlcok/agentpr/workspaces/mem0/.github/workflows/ci.yml\",\n      \"/Users/yi/Documents/Career/TensorBlcok/agentpr/workspaces/mem0/.github/workflows/cd.yml\"\n    ],\n    \"mem0_job_trigger_paths\": [\n      \"mem0/**\",\n      \"tests/**\",\n      \".github/workflows/**\",\n      \"pyproject.toml\"\n    ],\n    \"embedchain_job_trigger_paths\": [\n      \"embedchain/**\"\n    ],\n    \"toolchain\": {\n      \"python_matrix_mem0\": [\n        \"3.10\",\n        \"3.11\",\n        \"3.12\"\n      ],\n      \"package_manager\": \"pip + hatch\",\n      \"lint_command\": \"make lint\",\n      \"test_command\": \"make test\"\n    },\n    \"required_env_and_system\": {\n      \"system_package_step_in_ci\": \"sudo apt-get install -y libgeos-dev\",\n      \"repo_python_constraint\": \">=3.9,<4.0\",\n      \"llm_runtime_deps_include_openai\": true\n    },\n    \"evidence\": [\n      \"/Users/yi/Documents/Career/TensorBlcok/agentpr/workspaces/mem0/.github/workflows/ci.yml:43-77\",\n      \"/Users/yi/Documents/Career/TensorBlcok/agentpr/workspaces/mem0/pyproject.toml:15\",\n      \"/Users/yi/Documents/Career/TensorBlcok/agentpr/workspaces/mem0/Makefile:27-43\"\n    ]\n  },\n  \"integration_surface\": {\n    \"provider_registry_entrypoint\": \"/Users/yi/Documents/Career/TensorBlcok/agentpr/workspaces/mem0/mem0/utils/factory.py:35-53\",\n    \"openai_provider_impl\": \"/Users/yi/Documents/Career/TensorBlcok/agentpr/workspaces/mem0/mem0/llms/openai.py:40-51\",\n    \"openai_request_param_branching\": \"/Users/yi/Documents/Career/TensorBlcok/agentpr/workspaces/mem0/mem0/llms/openai.py:111-132\",\n    \"openai_config_surface\": \"/Users/yi/Documents/Career/TensorBlcok/agentpr/workspaces/mem0/mem0/configs/llms/openai.py:1-78\",\n    \"current_router_pattern\": \"OPENROUTER_API_KEY switches OpenAI client/base_url and request params.\",\n    \"closest_provider_pattern\": \"OpenRouter branch in OpenAILLM (same OpenAI-compatible routing style).\"\n  },\n  \"closest_precedent\": {\n    \"commit\": \"55b387b067e612a96d6305b4ee4a16cb2048ae62\",\n    \"title\": \"feat: Add Forge LLM router support\",\n    \"files\": [\n      \"mem0/llms/openai.py\",\n      \"docs/components/llms/models/forge.mdx\",\n      \"docs/llms.txt\"\n    ],\n    \"fit\": \"High; same Forge routing objective with minimal OpenAI-path wiring.\"\n  },\n  \"minimal_diff_plan\": {\n    \"constraints\": {\n      \"max_changed_files\": 5,\n      \"max_added_lines\": 90,\n      \"target_changed_files\": 4,\n      \"target_added_lines_estimate\": 88\n    },\n    \"planned_files\": [\n      {\n        \"path\": \"/Users/yi/Documents/Career/TensorBlcok/agentpr/workspaces/mem0/mem0/llms/openai.py\",\n        \"change\": \"Add Forge env-based routing (FORGE_API_KEY/FORGE_API_BASE) ahead of OpenRouter, and keep standard OpenAI request params for Forge.\"\n      },\n      {\n        \"path\": \"/Users/yi/Documents/Career/TensorBlcok/agentpr/workspaces/mem0/tests/llms/test_openai.py\",\n        \"change\": \"Add Forge path tests for base_url/client selection and generate_response parameter behavior.\"\n      },\n      {\n        \"path\": \"/Users/yi/Documents/Career/TensorBlcok/agentpr/workspaces/mem0/docs/components/llms/models/forge.mdx\",\n        \"change\": \"Add Forge usage docs using provider=openai and FORGE_* env vars.\"\n      },\n      {\n        \"path\": \"/Users/yi/Documents/Career/TensorBlcok/agentpr/workspaces/mem0/docs/llms.txt\",\n        \"change\": \"Add Forge entry to supported LLMs list.\"\n      }\n    ],\n    \"explicit_non_goals\": [\n      \"Do not add a new LLM provider module/class.\",\n      \"Do not change LlmFactory provider map unless requirements change.\",\n      \"Do not touch unrelated docs/navigation beyond minimal Forge mention.\"\n    ]\n  },\n  \"doc_test_requirements\": {\n    \"docs_required\": true,\n    \"tests_required\": true,\n    \"required_local_validation_sequence\": [\n      \"make lint\",\n      \"make test\"\n    ],\n    \"fast_precheck_recommended_before_full\": [\n      \"pytest tests/llms/test_openai.py -q\"\n    ]\n  },\n  \"env_vars_relevant_to_integration\": {\n    \"existing\": [\n      \"OPENAI_API_KEY\",\n      \"OPENAI_BASE_URL\",\n      \"OPENROUTER_API_KEY\",\n      \"OPENROUTER_API_BASE\"\n    ],\n    \"planned_for_forge\": [\n      \"FORGE_API_KEY\",\n      \"FORGE_API_BASE\"\n    ]\n  },\n  \"blockers\": [\n    {\n      \"id\": \"external_instructions_blocked\",\n      \"severity\": \"high\",\n      \"type\": \"policy\",\n      \"details\": \"Base integration prompt requires reading /Users/yi/Documents/Career/TensorBlcok/agentpr/forge_integration/* and /Users/yi/Documents/Career/TensorBlcok/forge/README.md, but safety contract forbids reading outside repository.\"\n    },\n    {\n      \"id\": \"skill_schema_external_blocked\",\n      \"severity\": \"high\",\n      \"type\": \"policy\",\n      \"details\": \"Skill references external schema files under /Users/yi/.codex/skills/.../references/*.md; safety contract forbids reading outside repository.\"\n    },\n    {\n      \"id\": \"ci_system_dependency_not_reproducible_as_is\",\n      \"severity\": \"medium\",\n      \"type\": \"environment\",\n      \"details\": \"CI requires sudo apt-get libgeos-dev; local run policy forbids sudo.\"\n    }\n  ]\n}"}}
{"type":"turn.completed","usage":{"input_tokens":1892559,"cached_input_tokens":1807360,"output_tokens":17127}}
